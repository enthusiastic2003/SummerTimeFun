{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/enthusiastic2003/SummerTimeFun/blob/main/MachineTranslator/venus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SIkzbYo2yXS"
      },
      "outputs": [],
      "source": [
        "import unicodedata\n",
        "\n",
        "import re\n",
        "\n",
        "\n",
        "# Convert the unicode sequence to ascii\n",
        "def unicode_to_ascii(s):\n",
        "\n",
        "  # Normalize the unicode string and remove the non-spacking mark\n",
        "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "      if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "# Preprocess the sequence\n",
        "def preprocess_sentence(w):\n",
        "\n",
        "  # Clean the sequence\n",
        "  w = unicode_to_ascii(w.lower().strip())\n",
        "\n",
        "  # Create a space between a word and the punctuation following it also place a space between the punctuation and the following word. Note that punctuation also includes |\n",
        "\n",
        "  w = re.sub(r\"([?.!ред])\", r\" \\1 \", w)\n",
        "\n",
        "  # Add a start and stop token to detect the start and end of the sequence\n",
        "  w = '<start> ' + w + ' <end>'\n",
        "  return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2ypdvMk2yXT"
      },
      "outputs": [],
      "source": [
        "import io\n",
        "\n",
        "# Create the Dataset\n",
        "def create_dataset(path, num_examples):\n",
        "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "\n",
        "  lines = lines[:num_examples]\n",
        "  # Loop through lines (sequences) and extract the English and French sequences. Store them as a word-pair\n",
        "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t', 2)[:-1]]  for l in lines]\n",
        "  return zip(*word_pairs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8-hAeouE2yXU"
      },
      "outputs": [],
      "source": [
        "path_to_file='ben.txt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8TbXlDt2yXU",
        "outputId": "d519c006-d6a2-460e-f152-0863d94ee4e9"
      },
      "outputs": [],
      "source": [
        "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
        "print(lines[0])\n",
        "print(preprocess_sentence(lines[0].split('\\t', 2)[0]))\n",
        "print(preprocess_sentence(lines[0].split('\\t', 2)[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Gsle0iC2yXV",
        "outputId": "fec65c27-821a-4851-9b53-939a0e679bd2"
      },
      "outputs": [],
      "source": [
        "en, fra = create_dataset(path_to_file,6508)\n",
        "print(en[0])\n",
        "print(fra[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ORM3DMJU2yXV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Convert sequences to tokenizers\n",
        "def tokenize(lang):\n",
        "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
        "      filters='')\n",
        "\n",
        "  # Convert sequences into internal vocab\n",
        "  lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "  # Convert internal vocab to numbers\n",
        "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "\n",
        "  # Pad the tensors to assign equal length to all the sequences\n",
        "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
        "                                                         padding='post')\n",
        "\n",
        "  return tensor, lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CAR7FUDi2yXV"
      },
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "def load_dataset(path, num_examples=5000):\n",
        "\n",
        "  # Create dataset (targ_lan = English, inp_lang = French)\n",
        "  inp_lang,targ_lang = create_dataset(path,num_examples)\n",
        "\n",
        "  # Tokenize the sequences\n",
        "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-T8w-fqK2yXW"
      },
      "outputs": [],
      "source": [
        "# Consider 50k examples\n",
        "num_examples = 6508\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file, num_examples)\n",
        "\n",
        "# Calculate max_length of the target tensors\n",
        "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PkSO4-G2yXW",
        "outputId": "7450cb0e-015d-4bc1-917a-ca6b4d000cff"
      },
      "outputs": [],
      "source": [
        "print(max_length_targ, max_length_inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iC2DKHe02yXX",
        "outputId": "68814fe6-4ea2-40cb-b9b3-61b29aa180fd"
      },
      "outputs": [],
      "source": [
        "print(input_tensor[0], target_tensor.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Aq9zieVX2yXX",
        "outputId": "d2533eed-9dae-456f-f3f6-53892d098780"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Create training and validation sets using an 80/20 split\n",
        "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
        "\n",
        "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_75cHIfT2yXY",
        "outputId": "9f9575fc-f987-44b4-b11f-887c07e2c652"
      },
      "outputs": [],
      "source": [
        "print(input_tensor_train)\n",
        "print(inp_lang)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ri1UPva2yXY",
        "outputId": "aa217e99-d8f2-43a3-93c2-37cba1445636"
      },
      "outputs": [],
      "source": [
        "# Show the mapping b/w word index and language tokenizer\n",
        "def convert(lang, tensor):\n",
        "  for t in tensor:\n",
        "    if t != 0:\n",
        "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
        "\n",
        "print (\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print ()\n",
        "print (\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bEYujF42yXY"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HySLG0GVIPAI"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ICnUoOnn2yXY",
        "outputId": "86dc5037-2bc1-4ebf-8adf-915c3e87c3e8"
      },
      "outputs": [],
      "source": [
        "# Size of input and target batches\n",
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kj3iFl0R2yXZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class Encoder(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "\n",
        "        # Embedding layer\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        # GRU-bidirectional Layer\n",
        "        self.gru = tf.keras.layers.GRU(self.enc_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "\n",
        "\n",
        "    # Encoder network comprises an Embedding layer followed by an LSTM layer\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output_all_cells, state = self.gru(x, initial_state=hidden)\n",
        "        return output_all_cells, state\n",
        "\n",
        "    # To initialize the hidden state\n",
        "    def initialize_hidden_state(self):\n",
        "      return tf.zeros((self.batch_sz, self.enc_units))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-pFI9h82yXZ",
        "outputId": "8b60766b-f826-4d37-eae3-b9caa6783d54"
      },
      "outputs": [],
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, enc_states = encoder(example_input_batch, sample_hidden)\n",
        "\n",
        "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
        "print('Encoder state shape: {}'.format(enc_states.shape))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IjBPIAem2yXZ"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "class PayAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units_enc, units_dec,enc_length):\n",
        "        super(PayAttention, self).__init__() #Call initializer of the superclass\n",
        "        self.units_enc=units_enc\n",
        "        self.units_dec=units_dec\n",
        "        self.enc_length=enc_length\n",
        "        self.W1 = tf.keras.layers.Dense(units_dec)\n",
        "        self.W2 = tf.keras.layers.Dense(units_dec)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, enc_output, dec_states):\n",
        "\n",
        "\n",
        "        # enc_output= (64, 23, 1024)\n",
        "        # x=(1, units_dec)\n",
        "        # x=(1,1)\n",
        "        dec_states=tf.expand_dims(dec_states,1)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(dec_states) + self.W2(enc_output)))\n",
        "        #weights=(64,23,scalr)\n",
        "        weights=score\n",
        "\n",
        "        # softmaxed_weights = (64,1,23)\n",
        "        softmaxed_weights = tf.nn.softmax(weights, axis=1)\n",
        "        context_vector = tf.reduce_sum(softmaxed_weights*enc_output, axis=1)\n",
        "        softmaxed_weights = tf.squeeze(softmaxed_weights)\n",
        "        return context_vector, softmaxed_weights\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWFgux-L2yXZ"
      },
      "outputs": [],
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz, enc_units,inp_length):\n",
        "        super().__init__()\n",
        "        self.batch_sz=batch_sz\n",
        "        self.dec_units=dec_units\n",
        "        self.enc_units=enc_units\n",
        "        self.embedding_dim=embedding_dim\n",
        "        self.vocab_size=vocab_size\n",
        "        self.inp_length=inp_length\n",
        "\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "        self.attention = PayAttention(self.enc_units , self.dec_units, self.inp_length)\n",
        "\n",
        "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True,\n",
        "                                   recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    def call(self, enc_output, dec_input, dec_internal_state=None):\n",
        "        # hidden is the hidden states of all the units in the encoder\n",
        "        # context_vector is the context vector from the attention layer\n",
        "        # dec_input is the input to the decoder\n",
        "\n",
        "        # Now, first embed the decoder input\n",
        "        if dec_internal_state==None:\n",
        "          dec_internal_state = self.gru.get_initial_state(inputs=dec_input)\n",
        "          dec_internal_state = tf.cast(dec_internal_state, tf.float32)\n",
        "\n",
        "        # Get Context for each target token generation\n",
        "        context_vector, softmaxed_weights = self.attention(enc_output, dec_internal_state)\n",
        "\n",
        "        # Embed the decoder input\n",
        "        embed_dec_input=self.embedding(dec_input)\n",
        "\n",
        "        # Context + input = input to the GRU\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), embed_dec_input], axis=-1)\n",
        "\n",
        "        # Pass through a GRU layer\n",
        "        output, state = self.gru(x)\n",
        "\n",
        "        # Pass through a dense layer to get the probabilities distribution over the target vocabulary\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        x = self.fc(output)\n",
        "        return x, state , softmaxed_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aAHtyNtqosAm"
      },
      "outputs": [],
      "source": [
        "decoder = DecoderLayer(vocab_tar_size ,embedding_dim, units, BATCH_SIZE, units, max_length_inp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tl2o0yoTpzfG"
      },
      "outputs": [],
      "source": [
        "dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "prob_dist, states_decode, weights = decoder(sample_output, dec_input, enc_states )\n",
        "#decoder(sample_output, dec_input, None )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GginEoNrPNZW",
        "outputId": "b979ff25-46e9-4eb0-d86c-1a6e55f371de"
      },
      "outputs": [],
      "source": [
        "print(states_decode.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4oSREMN-2ve4",
        "outputId": "ff5bdac6-7db6-4d6a-ea54-794545029c52"
      },
      "outputs": [],
      "source": [
        "print(weights[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x1cmJyXK4P4r"
      },
      "outputs": [],
      "source": [
        "# Now we will define the Model using the layers we have already defined.\n",
        "#class BenLish(tf.keras.Model):\n",
        "#  def __init__(self, vocab_size_inp, vocab_size_tar, embedding_dim, dec_units, batch_sz, enc_units,inp_length):\n",
        "#    super().__init__()\n",
        "#    self.encoder = Encoder(vocab_size_inp, embedding_dim, enc_units, batch_sz)\n",
        "#    self.decoder = DecoderLayer(vocab_size_tar ,embedding_dim, dec_units, batch_sz, dec_units, inp_length)\n",
        "\n",
        "#  def call(self, sentence):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZVYgi9Eq2yXZ"
      },
      "outputs": [],
      "source": [
        "#Now we define the loss function. The loss function is the cross entropy loss function. The cross entropy loss function is defined as follows:\n",
        "#-sum(y_true * log(y_pred), axis=-1)\n",
        "import numpy as np\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  optimizer = tf.optimizers.Adam()\n",
        "\n",
        "  def loss_function(real, pred):\n",
        "      mask = 1 - np.equal(real, 0)\n",
        "      loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
        "      return tf.reduce_mean(loss_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "at3GVNjC2yXZ"
      },
      "outputs": [],
      "source": [
        "\n",
        "#Now we run the training loop\n",
        "with tf.device('/gpu:0'):\n",
        "  optimizer = tf.keras.optimizers.Adam()\n",
        "  loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
        "\n",
        "  decoder = DecoderLayer(vocab_tar_size ,embedding_dim, units, BATCH_SIZE, units, max_length_inp)\n",
        "  encoder=Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "  import os\n",
        "  checkpoint_dir = './training_checkpoints'\n",
        "  checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "  checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                  encoder=encoder,\n",
        "                                  decoder=decoder)\n",
        "\n",
        "  def train_step(inp, targ):\n",
        "      loss=0\n",
        "\n",
        "      with tf.GradientTape() as tape:\n",
        "          # Initialize the hidden state of the encoder and pass the input to the encoder\n",
        "          hidden_initialize = encoder.initialize_hidden_state()\n",
        "          enc_output, enc_hidden = encoder(inp, hidden_initialize)\n",
        "          dec_hidd=enc_hidden\n",
        "          # Initialize the hidden state of the decoder\n",
        "          dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "          # Teacher forcing - feeding the target as the next input\n",
        "          for t in range(1, targ.shape[1]):\n",
        "              # passing enc_output to the decoder\n",
        "              prob_dist, dec_hidd, weights = decoder(enc_output, dec_input, dec_hidd )\n",
        "              loss += loss_function(targ[:, t], prob_dist)\n",
        "              # using teacher forcing\n",
        "              dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "\n",
        "\n",
        "      batch_loss = (loss / int(targ.shape[1]))\n",
        "      variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "      gradients = tape.gradient(loss, variables)\n",
        "      optimizer.apply_gradients(zip(gradients, variables))\n",
        "      return batch_loss\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7jMeLwq2yXZ",
        "outputId": "7fe8f8c3-f28c-427c-e057-8501729288d2"
      },
      "outputs": [],
      "source": [
        "print(steps_per_epoch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFi7nXthmUrZ",
        "outputId": "3b3e2685-d970-4983-b6b1-10f6c55aed46"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qL9jG-j82yXa",
        "outputId": "b45b293a-065c-4d20-9327-ab3170b793d6"
      },
      "outputs": [],
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  EPOCHS=100\n",
        "  import time\n",
        "  from tqdm import tqdm\n",
        "  for epoch in range(EPOCHS):\n",
        "      start = time.time()\n",
        "      total_loss = 0\n",
        "      pbar = tqdm(dataset.take(steps_per_epoch), ascii=True, total=steps_per_epoch)\n",
        "      print(\"Epoch : {} / {}\".format(epoch, EPOCHS))\n",
        "      for (batch, (inp, targ)) in enumerate(pbar):\n",
        "          batch_loss = train_step(inp, targ)\n",
        "          total_loss += batch_loss\n",
        "          pbar.set_description(\n",
        "              \"Step - {} / {} - batch loss - {:.4f} \"\n",
        "                  .format(batch+1, steps_per_epoch, batch_loss.numpy()))\n",
        "\n",
        "      # saving (checkpoint) the model every 2 epochs\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix+str(epoch))\n",
        "\n",
        "      print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
        "                                        total_loss / steps_per_epoch))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dxF0-yA2yXa"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "nXpHQcV72yXa",
        "outputId": "3a51e55b-a1a0-4774-c6bd-6d2bd45fff6d"
      },
      "outputs": [],
      "source": [
        "checkpoint.save(file_prefix = \"finish_line\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_nCNddw02yXb"
      },
      "outputs": [],
      "source": [
        "from translator import utils\n",
        "from translator import models\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "\n",
        "from matplotlib import ticker\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib import font_manager as fm\n",
        "\n",
        "FONT_NAME = 'assets/banglafonts/Siyamrupali.ttf'\n",
        "\n",
        "class Infer():\n",
        "    def __init__(self, input_language_tokenizer, target_language_tokenizer,\n",
        "                max_length_input, max_length_target, encoder, decoder, units):\n",
        "        self.input_language_tokenizer = input_language_tokenizer\n",
        "        self.target_language_tokenizer = target_language_tokenizer\n",
        "        self.max_length_input = max_length_input\n",
        "        self.max_length_target = max_length_target\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.units = units\n",
        "\n",
        "    def preprocess(self, sentence):\n",
        "        # clean and pad sequece\n",
        "        sentence = utils.clean_seq(sentence)\n",
        "        sentence = utils.add_start_and_end_token_to_seq(sentence)\n",
        "\n",
        "        inputs = [\n",
        "            self.input_language_tokenizer.word_index[i] for i in sentence.split(' ')]\n",
        "        inputs = sequence.pad_sequences(\n",
        "            [inputs], maxlen=self.max_length_input,padding='post')\n",
        "        tensor = tf.convert_to_tensor(inputs)\n",
        "\n",
        "        return tensor\n",
        "\n",
        "    def predict(self, sentence):\n",
        "        tensor = self.preprocess(sentence)\n",
        "\n",
        "        # init encoder\n",
        "        encoder_initial_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_out, encoder_hidden = self.encoder(tensor, encoder_initial_hidden)\n",
        "\n",
        "        # init decoder\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = tf.expand_dims(\n",
        "            [self.target_language_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "        result = ''\n",
        "        for _ in range(self.max_length_target):\n",
        "            predictions, decoder_hidden, _ = self.decoder(encoder_out, decoder_input, decoder_hidden)\n",
        "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "            result += self.target_language_tokenizer.index_word[predicted_id] + ' '\n",
        "            if self.target_language_tokenizer.index_word[predicted_id] == '<end>':\n",
        "                return result\n",
        "            # the predicted ID is fed back into the model insteqad of using\n",
        "            # teacher forcing that we use in training time\n",
        "            decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        return result\n",
        "\n",
        "    def predict_with_attention_weights(self, sentence):\n",
        "        tensor = self.preprocess(sentence)\n",
        "\n",
        "        # init encoder\n",
        "        encoder_initial_hidden = [tf.zeros((1, self.units))]\n",
        "        encoder_out, encoder_hidden = self.encoder(tensor, encoder_initial_hidden)\n",
        "\n",
        "        # init decoder\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoder_input = tf.expand_dims(\n",
        "            [self.target_language_tokenizer.word_index['<start>']], 0)\n",
        "\n",
        "        result = ''\n",
        "        attention_plot = np.zeros((self.max_length_target, self.max_length_input))\n",
        "        for t in range(self.max_length_target):\n",
        "            predictions, decoder_hidden, attention_weights = \\\n",
        "                self.decoder(decoder_input, decoder_hidden, encoder_out)\n",
        "\n",
        "            # storing the attention weights to plot later on\n",
        "            attention_weights = tf.reshape(attention_weights, (-1, ))\n",
        "            attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "            predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "            result += self.target_language_tokenizer.index_word[predicted_id] + ' '\n",
        "            if self.target_language_tokenizer.index_word[predicted_id] == '<end>':\n",
        "                return result, sentence, attention_plot\n",
        "\n",
        "            # the predicted ID is fed back into the model insteqad of using\n",
        "            # teacher forcing that we use in training time\n",
        "            decoder_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "        return result, sentence, attention_plot\n",
        "\n",
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    prop = fm.FontProperties(fname=FONT_NAME)\n",
        "    fig = plt.figure(figsize=(10, 10))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "\n",
        "    ax.set_xticklabels([''] + sentence, rotation=90, fontproperties=prop)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontproperties=prop)\n",
        "\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.rcParams.update({'font.size': 14})\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCr_Y8yqu4pJ"
      },
      "outputs": [],
      "source": [
        "decoder_infer = DecoderLayer(vocab_tar_size ,embedding_dim, units, 64, units, max_length_inp)\n",
        "encoder_infer=Encoder(vocab_inp_size, embedding_dim, units, 64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xl4WFF6vCej"
      },
      "outputs": [],
      "source": [
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                  encoder=encoder_infer,\n",
        "                                  decoder=decoder_infer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ux1Z-JndvJVD",
        "outputId": "a46abd19-0069-42ce-dc90-cfb7b98b02cf"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore('./training_checkpoints/ckpt30-31')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sxsLflhBvNgS"
      },
      "outputs": [],
      "source": [
        "pred=Infer(inp_lang,targ_lang, max_length_inp, max_length_targ, encoder_infer, decoder_infer,1024)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "HzWg4C7rvhEC",
        "outputId": "ab4ed543-3412-4b5f-cd2d-1eb989b9f943"
      },
      "outputs": [],
      "source": [
        "pred.predict(\"I did what i had to do.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
