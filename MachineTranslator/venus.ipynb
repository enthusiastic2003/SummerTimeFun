{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# Convert the unicode sequence to ascii\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "  # Normalize the unicode string and remove the non-spacking mark\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Preprocess the sequence\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "  # Clean the sequence\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  \n",
    "  # Create a space between a word and the punctuation following it also place a space between the punctuation and the following word. Note that punctuation also includes | \n",
    "\n",
    "  w = re.sub(r\"([?.!।])\", r\" \\1 \", w)\n",
    "  \n",
    "  # Add a start and stop token to detect the start and end of the sequence\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create the Dataset\n",
    "def create_dataset(path):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  # Loop through lines (sequences) and extract the English and French sequences. Store them as a word-pair\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t', 2)[:-1]]  for l in lines]\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file='ben.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tযাও।\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5545004 (tanay)\n",
      "<start> go .  <end>\n",
      "<start> যাও ।  <end>\n"
     ]
    }
   ],
   "source": [
    "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(lines[0])\n",
    "print(preprocess_sentence(lines[0].split('\\t', 2)[0]))\n",
    "print(preprocess_sentence(lines[0].split('\\t', 2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go .  <end>\n",
      "<start> যাও ।  <end>\n"
     ]
    }
   ],
   "source": [
    "en, fra = create_dataset(path_to_file)\n",
    "print(en[0])\n",
    "print(fra[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  \n",
    "  # Convert sequences into internal vocab\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  # Convert internal vocab to numbers\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  # Pad the tensors to assign equal length to all the sequences\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(path, num_examples=None):\n",
    " \n",
    "  # Create dataset (targ_lan = English, inp_lang = French)\n",
    "  inp_lang,targ_lang = create_dataset(path)\n",
    "\n",
    "  # Tokenize the sequences\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 50k examples\n",
    "num_examples = 50000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 23\n"
     ]
    }
   ],
   "source": [
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 33  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] (6509, 21)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor[0], target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207 5207 1302 1302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and validation sets using an 80/20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  1 237  27 ...   0   0   0]\n",
      " [  1  22  10 ...   0   0   0]\n",
      " [  1 119  20 ...   0   0   0]\n",
      " ...\n",
      " [  1   4  68 ...   0   0   0]\n",
      " [  1  23 388 ...   0   0   0]\n",
      " [  1   7 817 ...   0   0   0]]\n",
      "<keras.src.legacy.preprocessing.text.Tokenizer object at 0x1699bb7d0>\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor_train)\n",
    "print(inp_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "237 ----> who's\n",
      "27 ----> your\n",
      "1214 ----> aunt\n",
      "5 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "49 ----> কে\n",
      "14 ----> আপনার\n",
      "1605 ----> কাকি\n",
      "5 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping b/w word index and language tokenizer\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "      \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 23]), TensorShape([64, 21]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # Encoder network comprises an Embedding layer followed by an LSTM layer\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
    "        state = [state_h, state_c]\n",
    "        return output, state\n",
    "\n",
    "    # To initialize the hidden state\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 23, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
      "Encoder Cell state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
    "print ('Encoder Cell state shape: (batch size, units) {}'.format(sample_hidden[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PayAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, length):\n",
    "        self.units=units\n",
    "        self.length=length\n",
    "        super(PayAttention, self).__init__() #Call initializer of the superclass\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(self.units,1), initializer='normal')\n",
    "        self.b = self.add_weight(shape=(self.length,1), initializer='zeros')\n",
    "        super(PayAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, sentences):\n",
    "        # Sentences = (batch_size, length of sentence, units)\n",
    "        E = tf.nn.tanh(tf.keras.backend.dot(sentences,self.w)+self.b)\n",
    "        # E = (batch_size, length, 1)\n",
    "        A = tf.nn.softmax(E, axis=1)\n",
    "        # A = softmax(E) = (batch_size, length, 1)\n",
    "        out= A*sentences\n",
    "        # out = (batch_size, length, units)\n",
    "        return tf.keras.backend.sum(out, axis=1), A # (batch_size, units), (batch_size, length, 1)\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,inp_units, enc_length):\n",
    "        super().__init__()\n",
    "        self.batch_sz=batch_sz\n",
    "        self.dec_units=dec_units\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.vocab_size=vocab_size \n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) \n",
    "\n",
    "        self.attention = PayAttention(inp_units, enc_length)\n",
    "        \n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)        \n",
    "\n",
    "    def call(self, enc_output, dec_input):\n",
    "        # hidden is the hidden states of all the units in the encoder\n",
    "        # context_vector is the context vector from the attention layer\n",
    "        # dec_input is the input to the decoder\n",
    "\n",
    "        # Now, first embed the decoder input\n",
    "        x=self.embedding(dec_input)\n",
    "        \n",
    "        context_vector, attention_weights = self.attention(enc_output)\n",
    "        \n",
    "        # Now we will concat the encoder input and the context vector\n",
    "            #first expand the context vector\n",
    "        context_vector=tf.expand_dims(context_vector,1)\n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "\n",
    "        # Pass through a GRU layer\n",
    "        output,state = self.gru(x)\n",
    "\n",
    "        # Pass through a dense layer to get the probabilities distribution over the target vocabulary\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state , attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the loss function. The loss function is the cross entropy loss function. The cross entropy loss function is defined as follows:\n",
    "#-sum(y_true * log(y_pred), axis=-1)\n",
    "import numpy as np\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we run the training loop\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "decoder=DecoderLayer(vocab_tar_size, embedding_dim, units, BATCH_SIZE, units, max_length_inp)\n",
    "encoder=Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "import os\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder)\n",
    "\n",
    "def train_step(inp, targ):\n",
    "    loss=0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Initialize the hidden state of the encoder and pass the input to the encoder\n",
    "        hidden_initialize = encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = encoder(inp, hidden_initialize)\n",
    "\n",
    "        # Initialize the hidden state of the decoder\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)   \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden, _ = decoder(enc_output, dec_input)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 1 Batch 0 Loss 2.6109\n",
      "(64, 21)\n",
      "Epoch 1 Batch 1 Loss 2.5136\n",
      "(64, 21)\n",
      "Epoch 1 Batch 2 Loss 2.5645\n",
      "(64, 21)\n",
      "Epoch 1 Batch 3 Loss 2.2918\n",
      "(64, 21)\n",
      "Epoch 1 Batch 4 Loss 2.1355\n",
      "(64, 21)\n",
      "Epoch 1 Batch 5 Loss 1.8477\n",
      "(64, 21)\n",
      "Epoch 1 Batch 6 Loss 1.9221\n",
      "(64, 21)\n",
      "Epoch 1 Batch 7 Loss 1.8126\n",
      "(64, 21)\n",
      "Epoch 1 Batch 8 Loss 1.6980\n",
      "(64, 21)\n",
      "Epoch 1 Batch 9 Loss 1.8674\n",
      "(64, 21)\n",
      "Epoch 1 Batch 10 Loss 2.1525\n",
      "(64, 21)\n",
      "Epoch 1 Batch 11 Loss 1.8862\n",
      "(64, 21)\n",
      "Epoch 1 Batch 12 Loss 1.8881\n",
      "(64, 21)\n",
      "Epoch 1 Batch 13 Loss 1.9709\n",
      "(64, 21)\n",
      "Epoch 1 Batch 14 Loss 1.8169\n",
      "(64, 21)\n",
      "Epoch 1 Batch 15 Loss 1.7177\n",
      "(64, 21)\n",
      "Epoch 1 Batch 16 Loss 1.6843\n",
      "(64, 21)\n",
      "Epoch 1 Batch 17 Loss 1.8598\n",
      "(64, 21)\n",
      "Epoch 1 Batch 18 Loss 1.9808\n",
      "(64, 21)\n",
      "Epoch 1 Batch 19 Loss 1.7288\n",
      "(64, 21)\n",
      "Epoch 1 Batch 20 Loss 1.6829\n",
      "(64, 21)\n",
      "Epoch 1 Batch 21 Loss 1.7677\n",
      "(64, 21)\n",
      "Epoch 1 Batch 22 Loss 1.6307\n",
      "(64, 21)\n",
      "Epoch 1 Batch 23 Loss 1.8804\n",
      "(64, 21)\n",
      "Epoch 1 Batch 24 Loss 1.6944\n",
      "(64, 21)\n",
      "Epoch 1 Batch 25 Loss 1.6233\n",
      "(64, 21)\n",
      "Epoch 1 Batch 26 Loss 1.6615\n",
      "(64, 21)\n",
      "Epoch 1 Batch 27 Loss 1.6571\n",
      "(64, 21)\n",
      "Epoch 1 Batch 28 Loss 1.6494\n",
      "(64, 21)\n",
      "Epoch 1 Batch 29 Loss 1.6557\n",
      "(64, 21)\n",
      "Epoch 1 Batch 30 Loss 1.8280\n",
      "(64, 21)\n",
      "Epoch 1 Batch 31 Loss 1.6759\n",
      "(64, 21)\n",
      "Epoch 1 Batch 32 Loss 1.5803\n",
      "(64, 21)\n",
      "Epoch 1 Batch 33 Loss 1.5615\n",
      "(64, 21)\n",
      "Epoch 1 Batch 34 Loss 1.6776\n",
      "(64, 21)\n",
      "Epoch 1 Batch 35 Loss 1.5533\n",
      "(64, 21)\n",
      "Epoch 1 Batch 36 Loss 1.6963\n",
      "(64, 21)\n",
      "Epoch 1 Batch 37 Loss 1.5192\n",
      "(64, 21)\n",
      "Epoch 1 Batch 38 Loss 1.6051\n",
      "(64, 21)\n",
      "Epoch 1 Batch 39 Loss 1.6851\n",
      "(64, 21)\n",
      "Epoch 1 Batch 40 Loss 1.4791\n",
      "(64, 21)\n",
      "Epoch 1 Batch 41 Loss 1.6426\n",
      "(64, 21)\n",
      "Epoch 1 Batch 42 Loss 1.8493\n",
      "(64, 21)\n",
      "Epoch 1 Batch 43 Loss 1.8820\n",
      "(64, 21)\n",
      "Epoch 1 Batch 44 Loss 1.6151\n",
      "(64, 21)\n",
      "Epoch 1 Batch 45 Loss 1.7685\n",
      "(64, 21)\n",
      "Epoch 1 Batch 46 Loss 1.6021\n",
      "(64, 21)\n",
      "Epoch 1 Batch 47 Loss 1.6203\n",
      "(64, 21)\n",
      "Epoch 1 Batch 48 Loss 1.6786\n",
      "(64, 21)\n",
      "Epoch 1 Batch 49 Loss 1.6608\n",
      "(64, 21)\n",
      "Epoch 1 Batch 50 Loss 1.5539\n",
      "(64, 21)\n",
      "Epoch 1 Batch 51 Loss 1.5794\n",
      "(64, 21)\n",
      "Epoch 1 Batch 52 Loss 1.7012\n",
      "(64, 21)\n",
      "Epoch 1 Batch 53 Loss 1.8592\n",
      "(64, 21)\n",
      "Epoch 1 Batch 54 Loss 1.5808\n",
      "(64, 21)\n",
      "Epoch 1 Batch 55 Loss 1.6927\n",
      "(64, 21)\n",
      "Epoch 1 Batch 56 Loss 1.4244\n",
      "(64, 21)\n",
      "Epoch 1 Batch 57 Loss 1.5674\n",
      "(64, 21)\n",
      "Epoch 1 Batch 58 Loss 1.5300\n",
      "(64, 21)\n",
      "Epoch 1 Batch 59 Loss 1.5424\n",
      "(64, 21)\n",
      "Epoch 1 Batch 60 Loss 1.6436\n",
      "(64, 21)\n",
      "Epoch 1 Batch 61 Loss 1.7572\n",
      "(64, 21)\n",
      "Epoch 1 Batch 62 Loss 1.5143\n",
      "(64, 21)\n",
      "Epoch 1 Batch 63 Loss 1.5657\n",
      "(64, 21)\n",
      "Epoch 1 Batch 64 Loss 1.5710\n",
      "(64, 21)\n",
      "Epoch 1 Batch 65 Loss 1.5067\n",
      "(64, 21)\n",
      "Epoch 1 Batch 66 Loss 1.5807\n",
      "(64, 21)\n",
      "Epoch 1 Batch 67 Loss 1.4815\n",
      "(64, 21)\n",
      "Epoch 1 Batch 68 Loss 1.4093\n",
      "(64, 21)\n",
      "Epoch 1 Batch 69 Loss 1.5755\n",
      "(64, 21)\n",
      "Epoch 1 Batch 70 Loss 1.5548\n",
      "(64, 21)\n",
      "Epoch 1 Batch 71 Loss 1.6556\n",
      "(64, 21)\n",
      "Epoch 1 Batch 72 Loss 1.4387\n",
      "(64, 21)\n",
      "Epoch 1 Batch 73 Loss 1.5664\n",
      "(64, 21)\n",
      "Epoch 1 Batch 74 Loss 1.6628\n",
      "(64, 21)\n",
      "Epoch 1 Batch 75 Loss 1.5880\n",
      "(64, 21)\n",
      "Epoch 1 Batch 76 Loss 1.6309\n",
      "(64, 21)\n",
      "Epoch 1 Batch 77 Loss 1.6386\n",
      "(64, 21)\n",
      "Epoch 1 Batch 78 Loss 1.4507\n",
      "(64, 21)\n",
      "Epoch 1 Batch 79 Loss 1.4723\n",
      "(64, 21)\n",
      "Epoch 1 Batch 80 Loss 1.5740\n",
      "Epoch 1 Loss 1.7149\n",
      "Time taken for 1 epoch 239.46204710006714 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:30:31.692620: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 2 Batch 0 Loss 1.4677\n",
      "(64, 21)\n",
      "Epoch 2 Batch 1 Loss 1.4001\n",
      "(64, 21)\n",
      "Epoch 2 Batch 2 Loss 1.4374\n",
      "(64, 21)\n",
      "Epoch 2 Batch 3 Loss 1.5652\n",
      "(64, 21)\n",
      "Epoch 2 Batch 4 Loss 1.2603\n",
      "(64, 21)\n",
      "Epoch 2 Batch 5 Loss 1.5318\n",
      "(64, 21)\n",
      "Epoch 2 Batch 6 Loss 1.4232\n",
      "(64, 21)\n",
      "Epoch 2 Batch 7 Loss 1.3554\n",
      "(64, 21)\n",
      "Epoch 2 Batch 8 Loss 1.4330\n",
      "(64, 21)\n",
      "Epoch 2 Batch 9 Loss 1.4405\n",
      "(64, 21)\n",
      "Epoch 2 Batch 10 Loss 1.4820\n",
      "(64, 21)\n",
      "Epoch 2 Batch 11 Loss 1.5569\n",
      "(64, 21)\n",
      "Epoch 2 Batch 12 Loss 1.4050\n",
      "(64, 21)\n",
      "Epoch 2 Batch 13 Loss 1.4535\n",
      "(64, 21)\n",
      "Epoch 2 Batch 14 Loss 1.6454\n",
      "(64, 21)\n",
      "Epoch 2 Batch 15 Loss 1.5117\n",
      "(64, 21)\n",
      "Epoch 2 Batch 16 Loss 1.3190\n",
      "(64, 21)\n",
      "Epoch 2 Batch 17 Loss 1.5107\n",
      "(64, 21)\n",
      "Epoch 2 Batch 18 Loss 1.4179\n",
      "(64, 21)\n",
      "Epoch 2 Batch 19 Loss 1.4391\n",
      "(64, 21)\n",
      "Epoch 2 Batch 20 Loss 1.5568\n",
      "(64, 21)\n",
      "Epoch 2 Batch 21 Loss 1.4665\n",
      "(64, 21)\n",
      "Epoch 2 Batch 22 Loss 1.4623\n",
      "(64, 21)\n",
      "Epoch 2 Batch 23 Loss 1.4741\n",
      "(64, 21)\n",
      "Epoch 2 Batch 24 Loss 1.5336\n",
      "(64, 21)\n",
      "Epoch 2 Batch 25 Loss 1.6366\n",
      "(64, 21)\n",
      "Epoch 2 Batch 26 Loss 1.3871\n",
      "(64, 21)\n",
      "Epoch 2 Batch 27 Loss 1.4424\n",
      "(64, 21)\n",
      "Epoch 2 Batch 28 Loss 1.4195\n",
      "(64, 21)\n",
      "Epoch 2 Batch 29 Loss 1.6144\n",
      "(64, 21)\n",
      "Epoch 2 Batch 30 Loss 1.5446\n",
      "(64, 21)\n",
      "Epoch 2 Batch 31 Loss 1.4592\n",
      "(64, 21)\n",
      "Epoch 2 Batch 32 Loss 1.5281\n",
      "(64, 21)\n",
      "Epoch 2 Batch 33 Loss 1.5344\n",
      "(64, 21)\n",
      "Epoch 2 Batch 34 Loss 1.3103\n",
      "(64, 21)\n",
      "Epoch 2 Batch 35 Loss 1.3480\n",
      "(64, 21)\n",
      "Epoch 2 Batch 36 Loss 1.5070\n",
      "(64, 21)\n",
      "Epoch 2 Batch 37 Loss 1.4521\n",
      "(64, 21)\n",
      "Epoch 2 Batch 38 Loss 1.3789\n",
      "(64, 21)\n",
      "Epoch 2 Batch 39 Loss 1.4955\n",
      "(64, 21)\n",
      "Epoch 2 Batch 40 Loss 1.3985\n",
      "(64, 21)\n",
      "Epoch 2 Batch 41 Loss 1.3989\n",
      "(64, 21)\n",
      "Epoch 2 Batch 42 Loss 1.5257\n",
      "(64, 21)\n",
      "Epoch 2 Batch 43 Loss 1.4967\n",
      "(64, 21)\n",
      "Epoch 2 Batch 44 Loss 1.4428\n",
      "(64, 21)\n",
      "Epoch 2 Batch 45 Loss 1.5275\n",
      "(64, 21)\n",
      "Epoch 2 Batch 46 Loss 1.4121\n",
      "(64, 21)\n",
      "Epoch 2 Batch 47 Loss 1.4256\n",
      "(64, 21)\n",
      "Epoch 2 Batch 48 Loss 1.4633\n",
      "(64, 21)\n",
      "Epoch 2 Batch 49 Loss 1.3521\n",
      "(64, 21)\n",
      "Epoch 2 Batch 50 Loss 1.6124\n",
      "(64, 21)\n",
      "Epoch 2 Batch 51 Loss 1.3854\n",
      "(64, 21)\n",
      "Epoch 2 Batch 52 Loss 1.4435\n",
      "(64, 21)\n",
      "Epoch 2 Batch 53 Loss 1.6339\n",
      "(64, 21)\n",
      "Epoch 2 Batch 54 Loss 1.5368\n",
      "(64, 21)\n",
      "Epoch 2 Batch 55 Loss 1.3800\n",
      "(64, 21)\n",
      "Epoch 2 Batch 56 Loss 1.5129\n",
      "(64, 21)\n",
      "Epoch 2 Batch 57 Loss 1.5370\n",
      "(64, 21)\n",
      "Epoch 2 Batch 58 Loss 1.5294\n",
      "(64, 21)\n",
      "Epoch 2 Batch 59 Loss 1.3764\n",
      "(64, 21)\n",
      "Epoch 2 Batch 60 Loss 1.3559\n",
      "(64, 21)\n",
      "Epoch 2 Batch 61 Loss 1.3591\n",
      "(64, 21)\n",
      "Epoch 2 Batch 62 Loss 1.3932\n",
      "(64, 21)\n",
      "Epoch 2 Batch 63 Loss 1.4450\n",
      "(64, 21)\n",
      "Epoch 2 Batch 64 Loss 1.3936\n",
      "(64, 21)\n",
      "Epoch 2 Batch 65 Loss 1.4024\n",
      "(64, 21)\n",
      "Epoch 2 Batch 66 Loss 1.4763\n",
      "(64, 21)\n",
      "Epoch 2 Batch 67 Loss 1.2682\n",
      "(64, 21)\n",
      "Epoch 2 Batch 68 Loss 1.5522\n",
      "(64, 21)\n",
      "Epoch 2 Batch 69 Loss 1.3922\n",
      "(64, 21)\n",
      "Epoch 2 Batch 70 Loss 1.4095\n",
      "(64, 21)\n",
      "Epoch 2 Batch 71 Loss 1.3555\n",
      "(64, 21)\n",
      "Epoch 2 Batch 72 Loss 1.3841\n",
      "(64, 21)\n",
      "Epoch 2 Batch 73 Loss 1.3211\n",
      "(64, 21)\n",
      "Epoch 2 Batch 74 Loss 1.3424\n",
      "(64, 21)\n",
      "Epoch 2 Batch 75 Loss 1.5738\n",
      "(64, 21)\n",
      "Epoch 2 Batch 76 Loss 1.3154\n",
      "(64, 21)\n",
      "Epoch 2 Batch 77 Loss 1.3708\n",
      "(64, 21)\n",
      "Epoch 2 Batch 78 Loss 1.4338\n",
      "(64, 21)\n",
      "Epoch 2 Batch 79 Loss 1.3891\n",
      "(64, 21)\n",
      "Epoch 2 Batch 80 Loss 1.4901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:34:36.718338: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 1.4497\n",
      "Time taken for 1 epoch 245.64204716682434 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 3 Batch 0 Loss 1.3370\n",
      "(64, 21)\n",
      "Epoch 3 Batch 1 Loss 1.4808\n",
      "(64, 21)\n",
      "Epoch 3 Batch 2 Loss 1.2753\n",
      "(64, 21)\n",
      "Epoch 3 Batch 3 Loss 1.3322\n",
      "(64, 21)\n",
      "Epoch 3 Batch 4 Loss 1.2838\n",
      "(64, 21)\n",
      "Epoch 3 Batch 5 Loss 1.2901\n",
      "(64, 21)\n",
      "Epoch 3 Batch 6 Loss 1.2254\n",
      "(64, 21)\n",
      "Epoch 3 Batch 7 Loss 1.3818\n",
      "(64, 21)\n",
      "Epoch 3 Batch 8 Loss 1.3775\n",
      "(64, 21)\n",
      "Epoch 3 Batch 9 Loss 1.3373\n",
      "(64, 21)\n",
      "Epoch 3 Batch 10 Loss 1.3740\n",
      "(64, 21)\n",
      "Epoch 3 Batch 11 Loss 1.3700\n",
      "(64, 21)\n",
      "Epoch 3 Batch 12 Loss 1.2589\n",
      "(64, 21)\n",
      "Epoch 3 Batch 13 Loss 1.3935\n",
      "(64, 21)\n",
      "Epoch 3 Batch 14 Loss 1.2512\n",
      "(64, 21)\n",
      "Epoch 3 Batch 15 Loss 1.3989\n",
      "(64, 21)\n",
      "Epoch 3 Batch 16 Loss 1.2429\n",
      "(64, 21)\n",
      "Epoch 3 Batch 17 Loss 1.4185\n",
      "(64, 21)\n",
      "Epoch 3 Batch 18 Loss 1.2523\n",
      "(64, 21)\n",
      "Epoch 3 Batch 19 Loss 1.3816\n",
      "(64, 21)\n",
      "Epoch 3 Batch 20 Loss 1.4095\n",
      "(64, 21)\n",
      "Epoch 3 Batch 21 Loss 1.2782\n",
      "(64, 21)\n",
      "Epoch 3 Batch 22 Loss 1.2474\n",
      "(64, 21)\n",
      "Epoch 3 Batch 23 Loss 1.3091\n",
      "(64, 21)\n",
      "Epoch 3 Batch 24 Loss 1.2206\n",
      "(64, 21)\n",
      "Epoch 3 Batch 25 Loss 1.3218\n",
      "(64, 21)\n",
      "Epoch 3 Batch 26 Loss 1.2618\n",
      "(64, 21)\n",
      "Epoch 3 Batch 27 Loss 1.4408\n",
      "(64, 21)\n",
      "Epoch 3 Batch 28 Loss 1.2920\n",
      "(64, 21)\n",
      "Epoch 3 Batch 29 Loss 1.1735\n",
      "(64, 21)\n",
      "Epoch 3 Batch 30 Loss 1.4074\n",
      "(64, 21)\n",
      "Epoch 3 Batch 31 Loss 1.3671\n",
      "(64, 21)\n",
      "Epoch 3 Batch 32 Loss 1.4300\n",
      "(64, 21)\n",
      "Epoch 3 Batch 33 Loss 1.2973\n",
      "(64, 21)\n",
      "Epoch 3 Batch 34 Loss 1.3361\n",
      "(64, 21)\n",
      "Epoch 3 Batch 35 Loss 1.4797\n",
      "(64, 21)\n",
      "Epoch 3 Batch 36 Loss 1.3654\n",
      "(64, 21)\n",
      "Epoch 3 Batch 37 Loss 1.3663\n",
      "(64, 21)\n",
      "Epoch 3 Batch 38 Loss 1.4604\n",
      "(64, 21)\n",
      "Epoch 3 Batch 39 Loss 1.2232\n",
      "(64, 21)\n",
      "Epoch 3 Batch 40 Loss 1.4625\n",
      "(64, 21)\n",
      "Epoch 3 Batch 41 Loss 1.3518\n",
      "(64, 21)\n",
      "Epoch 3 Batch 42 Loss 1.3446\n",
      "(64, 21)\n",
      "Epoch 3 Batch 43 Loss 1.2651\n",
      "(64, 21)\n",
      "Epoch 3 Batch 44 Loss 1.2570\n",
      "(64, 21)\n",
      "Epoch 3 Batch 45 Loss 1.3318\n",
      "(64, 21)\n",
      "Epoch 3 Batch 46 Loss 1.3318\n",
      "(64, 21)\n",
      "Epoch 3 Batch 47 Loss 1.3724\n",
      "(64, 21)\n",
      "Epoch 3 Batch 48 Loss 1.3578\n",
      "(64, 21)\n",
      "Epoch 3 Batch 49 Loss 1.4626\n",
      "(64, 21)\n",
      "Epoch 3 Batch 50 Loss 1.2830\n",
      "(64, 21)\n",
      "Epoch 3 Batch 51 Loss 1.2211\n",
      "(64, 21)\n",
      "Epoch 3 Batch 52 Loss 1.2735\n",
      "(64, 21)\n",
      "Epoch 3 Batch 53 Loss 1.2377\n",
      "(64, 21)\n",
      "Epoch 3 Batch 54 Loss 1.4185\n",
      "(64, 21)\n",
      "Epoch 3 Batch 55 Loss 1.3194\n",
      "(64, 21)\n",
      "Epoch 3 Batch 56 Loss 1.4295\n",
      "(64, 21)\n",
      "Epoch 3 Batch 57 Loss 1.2832\n",
      "(64, 21)\n",
      "Epoch 3 Batch 58 Loss 1.3174\n",
      "(64, 21)\n",
      "Epoch 3 Batch 59 Loss 1.2193\n",
      "(64, 21)\n",
      "Epoch 3 Batch 60 Loss 1.2263\n",
      "(64, 21)\n",
      "Epoch 3 Batch 61 Loss 1.2802\n",
      "(64, 21)\n",
      "Epoch 3 Batch 62 Loss 1.1917\n",
      "(64, 21)\n",
      "Epoch 3 Batch 63 Loss 1.3814\n",
      "(64, 21)\n",
      "Epoch 3 Batch 64 Loss 1.3936\n",
      "(64, 21)\n",
      "Epoch 3 Batch 65 Loss 1.3048\n",
      "(64, 21)\n",
      "Epoch 3 Batch 66 Loss 1.2550\n",
      "(64, 21)\n",
      "Epoch 3 Batch 67 Loss 1.3177\n",
      "(64, 21)\n",
      "Epoch 3 Batch 68 Loss 1.3726\n",
      "(64, 21)\n",
      "Epoch 3 Batch 69 Loss 1.2240\n",
      "(64, 21)\n",
      "Epoch 3 Batch 70 Loss 1.4173\n",
      "(64, 21)\n",
      "Epoch 3 Batch 71 Loss 1.2938\n",
      "(64, 21)\n",
      "Epoch 3 Batch 72 Loss 1.2987\n",
      "(64, 21)\n",
      "Epoch 3 Batch 73 Loss 1.2773\n",
      "(64, 21)\n",
      "Epoch 3 Batch 74 Loss 1.3236\n",
      "(64, 21)\n",
      "Epoch 3 Batch 75 Loss 1.3618\n",
      "(64, 21)\n",
      "Epoch 3 Batch 76 Loss 1.4168\n",
      "(64, 21)\n",
      "Epoch 3 Batch 77 Loss 1.3390\n",
      "(64, 21)\n",
      "Epoch 3 Batch 78 Loss 1.2579\n",
      "(64, 21)\n",
      "Epoch 3 Batch 79 Loss 1.3705\n",
      "(64, 21)\n",
      "Epoch 3 Batch 80 Loss 1.4183\n",
      "Epoch 3 Loss 1.3286\n",
      "Time taken for 1 epoch 246.75160217285156 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:38:44.086966: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 4 Batch 0 Loss 1.2917\n",
      "(64, 21)\n",
      "Epoch 4 Batch 1 Loss 1.1637\n",
      "(64, 21)\n",
      "Epoch 4 Batch 2 Loss 1.1359\n",
      "(64, 21)\n",
      "Epoch 4 Batch 3 Loss 1.2285\n",
      "(64, 21)\n",
      "Epoch 4 Batch 4 Loss 1.2718\n",
      "(64, 21)\n",
      "Epoch 4 Batch 5 Loss 1.2722\n",
      "(64, 21)\n",
      "Epoch 4 Batch 6 Loss 1.1713\n",
      "(64, 21)\n",
      "Epoch 4 Batch 7 Loss 1.1635\n",
      "(64, 21)\n",
      "Epoch 4 Batch 8 Loss 1.2503\n",
      "(64, 21)\n",
      "Epoch 4 Batch 9 Loss 1.1970\n",
      "(64, 21)\n",
      "Epoch 4 Batch 10 Loss 1.1777\n",
      "(64, 21)\n",
      "Epoch 4 Batch 11 Loss 1.1956\n",
      "(64, 21)\n",
      "Epoch 4 Batch 12 Loss 1.0717\n",
      "(64, 21)\n",
      "Epoch 4 Batch 13 Loss 1.1982\n",
      "(64, 21)\n",
      "Epoch 4 Batch 14 Loss 1.0949\n",
      "(64, 21)\n",
      "Epoch 4 Batch 15 Loss 1.2398\n",
      "(64, 21)\n",
      "Epoch 4 Batch 16 Loss 1.3349\n",
      "(64, 21)\n",
      "Epoch 4 Batch 17 Loss 1.0353\n",
      "(64, 21)\n",
      "Epoch 4 Batch 18 Loss 1.2031\n",
      "(64, 21)\n",
      "Epoch 4 Batch 19 Loss 1.2863\n",
      "(64, 21)\n",
      "Epoch 4 Batch 20 Loss 1.1896\n",
      "(64, 21)\n",
      "Epoch 4 Batch 21 Loss 1.1477\n",
      "(64, 21)\n",
      "Epoch 4 Batch 22 Loss 1.1173\n",
      "(64, 21)\n",
      "Epoch 4 Batch 23 Loss 1.1741\n",
      "(64, 21)\n",
      "Epoch 4 Batch 24 Loss 1.2607\n",
      "(64, 21)\n",
      "Epoch 4 Batch 25 Loss 1.2856\n",
      "(64, 21)\n",
      "Epoch 4 Batch 26 Loss 1.1432\n",
      "(64, 21)\n",
      "Epoch 4 Batch 27 Loss 1.3414\n",
      "(64, 21)\n",
      "Epoch 4 Batch 28 Loss 1.1528\n",
      "(64, 21)\n",
      "Epoch 4 Batch 29 Loss 1.1868\n",
      "(64, 21)\n",
      "Epoch 4 Batch 30 Loss 1.0931\n",
      "(64, 21)\n",
      "Epoch 4 Batch 31 Loss 1.2108\n",
      "(64, 21)\n",
      "Epoch 4 Batch 32 Loss 1.1248\n",
      "(64, 21)\n",
      "Epoch 4 Batch 33 Loss 1.1761\n",
      "(64, 21)\n",
      "Epoch 4 Batch 34 Loss 1.3090\n",
      "(64, 21)\n",
      "Epoch 4 Batch 35 Loss 1.2480\n",
      "(64, 21)\n",
      "Epoch 4 Batch 36 Loss 1.3651\n",
      "(64, 21)\n",
      "Epoch 4 Batch 37 Loss 1.1299\n",
      "(64, 21)\n",
      "Epoch 4 Batch 38 Loss 1.1685\n",
      "(64, 21)\n",
      "Epoch 4 Batch 39 Loss 1.2245\n",
      "(64, 21)\n",
      "Epoch 4 Batch 40 Loss 1.2681\n",
      "(64, 21)\n",
      "Epoch 4 Batch 41 Loss 1.1226\n",
      "(64, 21)\n",
      "Epoch 4 Batch 42 Loss 1.0984\n",
      "(64, 21)\n",
      "Epoch 4 Batch 43 Loss 1.2780\n",
      "(64, 21)\n",
      "Epoch 4 Batch 44 Loss 1.1211\n",
      "(64, 21)\n",
      "Epoch 4 Batch 45 Loss 1.3176\n",
      "(64, 21)\n",
      "Epoch 4 Batch 46 Loss 1.3039\n",
      "(64, 21)\n",
      "Epoch 4 Batch 47 Loss 1.3638\n",
      "(64, 21)\n",
      "Epoch 4 Batch 48 Loss 1.4518\n",
      "(64, 21)\n",
      "Epoch 4 Batch 49 Loss 1.1107\n",
      "(64, 21)\n",
      "Epoch 4 Batch 50 Loss 1.2881\n",
      "(64, 21)\n",
      "Epoch 4 Batch 51 Loss 1.0965\n",
      "(64, 21)\n",
      "Epoch 4 Batch 52 Loss 1.2091\n",
      "(64, 21)\n",
      "Epoch 4 Batch 53 Loss 1.2364\n",
      "(64, 21)\n",
      "Epoch 4 Batch 54 Loss 1.2208\n",
      "(64, 21)\n",
      "Epoch 4 Batch 55 Loss 1.1370\n",
      "(64, 21)\n",
      "Epoch 4 Batch 56 Loss 1.1605\n",
      "(64, 21)\n",
      "Epoch 4 Batch 57 Loss 1.2570\n",
      "(64, 21)\n",
      "Epoch 4 Batch 58 Loss 1.2808\n",
      "(64, 21)\n",
      "Epoch 4 Batch 59 Loss 1.1788\n",
      "(64, 21)\n",
      "Epoch 4 Batch 60 Loss 1.2209\n",
      "(64, 21)\n",
      "Epoch 4 Batch 61 Loss 1.1661\n",
      "(64, 21)\n",
      "Epoch 4 Batch 62 Loss 1.2274\n",
      "(64, 21)\n",
      "Epoch 4 Batch 63 Loss 1.0478\n",
      "(64, 21)\n",
      "Epoch 4 Batch 64 Loss 1.3343\n",
      "(64, 21)\n",
      "Epoch 4 Batch 65 Loss 1.2644\n",
      "(64, 21)\n",
      "Epoch 4 Batch 66 Loss 1.1307\n",
      "(64, 21)\n",
      "Epoch 4 Batch 67 Loss 1.2661\n",
      "(64, 21)\n",
      "Epoch 4 Batch 68 Loss 1.1510\n",
      "(64, 21)\n",
      "Epoch 4 Batch 69 Loss 1.3156\n",
      "(64, 21)\n",
      "Epoch 4 Batch 70 Loss 1.1873\n",
      "(64, 21)\n",
      "Epoch 4 Batch 71 Loss 1.2644\n",
      "(64, 21)\n",
      "Epoch 4 Batch 72 Loss 1.1013\n",
      "(64, 21)\n",
      "Epoch 4 Batch 73 Loss 1.1135\n",
      "(64, 21)\n",
      "Epoch 4 Batch 74 Loss 1.1661\n",
      "(64, 21)\n",
      "Epoch 4 Batch 75 Loss 1.1761\n",
      "(64, 21)\n",
      "Epoch 4 Batch 76 Loss 1.2159\n",
      "(64, 21)\n",
      "Epoch 4 Batch 77 Loss 1.0633\n",
      "(64, 21)\n",
      "Epoch 4 Batch 78 Loss 1.1221\n",
      "(64, 21)\n",
      "Epoch 4 Batch 79 Loss 1.1449\n",
      "(64, 21)\n",
      "Epoch 4 Batch 80 Loss 1.1940\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:42:48.087053: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 1.2026\n",
      "Time taken for 1 epoch 244.60455107688904 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 5 Batch 0 Loss 1.0907\n",
      "(64, 21)\n",
      "Epoch 5 Batch 1 Loss 1.0677\n",
      "(64, 21)\n",
      "Epoch 5 Batch 2 Loss 1.0413\n",
      "(64, 21)\n",
      "Epoch 5 Batch 3 Loss 0.9974\n",
      "(64, 21)\n",
      "Epoch 5 Batch 4 Loss 1.0470\n",
      "(64, 21)\n",
      "Epoch 5 Batch 5 Loss 1.0015\n",
      "(64, 21)\n",
      "Epoch 5 Batch 6 Loss 1.0182\n",
      "(64, 21)\n",
      "Epoch 5 Batch 7 Loss 0.9735\n",
      "(64, 21)\n",
      "Epoch 5 Batch 8 Loss 1.1386\n",
      "(64, 21)\n",
      "Epoch 5 Batch 9 Loss 1.1137\n",
      "(64, 21)\n",
      "Epoch 5 Batch 10 Loss 1.2228\n",
      "(64, 21)\n",
      "Epoch 5 Batch 11 Loss 1.0653\n",
      "(64, 21)\n",
      "Epoch 5 Batch 12 Loss 1.0191\n",
      "(64, 21)\n",
      "Epoch 5 Batch 13 Loss 1.0312\n",
      "(64, 21)\n",
      "Epoch 5 Batch 14 Loss 1.0306\n",
      "(64, 21)\n",
      "Epoch 5 Batch 15 Loss 0.9621\n",
      "(64, 21)\n",
      "Epoch 5 Batch 16 Loss 1.0641\n",
      "(64, 21)\n",
      "Epoch 5 Batch 17 Loss 1.1960\n",
      "(64, 21)\n",
      "Epoch 5 Batch 18 Loss 1.0312\n",
      "(64, 21)\n",
      "Epoch 5 Batch 19 Loss 1.0284\n",
      "(64, 21)\n",
      "Epoch 5 Batch 20 Loss 1.0565\n",
      "(64, 21)\n",
      "Epoch 5 Batch 21 Loss 1.0683\n",
      "(64, 21)\n",
      "Epoch 5 Batch 22 Loss 1.1148\n",
      "(64, 21)\n",
      "Epoch 5 Batch 23 Loss 1.1068\n",
      "(64, 21)\n",
      "Epoch 5 Batch 24 Loss 1.0923\n",
      "(64, 21)\n",
      "Epoch 5 Batch 25 Loss 1.0689\n",
      "(64, 21)\n",
      "Epoch 5 Batch 26 Loss 1.0596\n",
      "(64, 21)\n",
      "Epoch 5 Batch 27 Loss 1.0554\n",
      "(64, 21)\n",
      "Epoch 5 Batch 28 Loss 1.1698\n",
      "(64, 21)\n",
      "Epoch 5 Batch 29 Loss 1.1013\n",
      "(64, 21)\n",
      "Epoch 5 Batch 30 Loss 1.1188\n",
      "(64, 21)\n",
      "Epoch 5 Batch 31 Loss 1.1156\n",
      "(64, 21)\n",
      "Epoch 5 Batch 32 Loss 0.9762\n",
      "(64, 21)\n",
      "Epoch 5 Batch 33 Loss 1.0258\n",
      "(64, 21)\n",
      "Epoch 5 Batch 34 Loss 1.1067\n",
      "(64, 21)\n",
      "Epoch 5 Batch 35 Loss 1.1710\n",
      "(64, 21)\n",
      "Epoch 5 Batch 36 Loss 1.1333\n",
      "(64, 21)\n",
      "Epoch 5 Batch 37 Loss 0.9572\n",
      "(64, 21)\n",
      "Epoch 5 Batch 38 Loss 0.9334\n",
      "(64, 21)\n",
      "Epoch 5 Batch 39 Loss 1.1030\n",
      "(64, 21)\n",
      "Epoch 5 Batch 40 Loss 1.1193\n",
      "(64, 21)\n",
      "Epoch 5 Batch 41 Loss 1.1842\n",
      "(64, 21)\n",
      "Epoch 5 Batch 42 Loss 0.9950\n",
      "(64, 21)\n",
      "Epoch 5 Batch 43 Loss 1.2052\n",
      "(64, 21)\n",
      "Epoch 5 Batch 44 Loss 1.0628\n",
      "(64, 21)\n",
      "Epoch 5 Batch 45 Loss 1.2313\n",
      "(64, 21)\n",
      "Epoch 5 Batch 46 Loss 1.2620\n",
      "(64, 21)\n",
      "Epoch 5 Batch 47 Loss 0.9820\n",
      "(64, 21)\n",
      "Epoch 5 Batch 48 Loss 1.1486\n",
      "(64, 21)\n",
      "Epoch 5 Batch 49 Loss 1.1052\n",
      "(64, 21)\n",
      "Epoch 5 Batch 50 Loss 1.1231\n",
      "(64, 21)\n",
      "Epoch 5 Batch 51 Loss 1.1253\n",
      "(64, 21)\n",
      "Epoch 5 Batch 52 Loss 1.1231\n",
      "(64, 21)\n",
      "Epoch 5 Batch 53 Loss 1.1499\n",
      "(64, 21)\n",
      "Epoch 5 Batch 54 Loss 1.0904\n",
      "(64, 21)\n",
      "Epoch 5 Batch 55 Loss 1.0405\n",
      "(64, 21)\n",
      "Epoch 5 Batch 56 Loss 1.0597\n",
      "(64, 21)\n",
      "Epoch 5 Batch 57 Loss 1.0015\n",
      "(64, 21)\n",
      "Epoch 5 Batch 58 Loss 1.1693\n",
      "(64, 21)\n",
      "Epoch 5 Batch 59 Loss 1.0100\n",
      "(64, 21)\n",
      "Epoch 5 Batch 60 Loss 1.0630\n",
      "(64, 21)\n",
      "Epoch 5 Batch 61 Loss 1.0725\n",
      "(64, 21)\n",
      "Epoch 5 Batch 62 Loss 1.0279\n",
      "(64, 21)\n",
      "Epoch 5 Batch 63 Loss 1.1023\n",
      "(64, 21)\n",
      "Epoch 5 Batch 64 Loss 1.1363\n",
      "(64, 21)\n",
      "Epoch 5 Batch 65 Loss 1.2278\n",
      "(64, 21)\n",
      "Epoch 5 Batch 66 Loss 1.0262\n",
      "(64, 21)\n",
      "Epoch 5 Batch 67 Loss 0.9550\n",
      "(64, 21)\n",
      "Epoch 5 Batch 68 Loss 1.0596\n",
      "(64, 21)\n",
      "Epoch 5 Batch 69 Loss 0.9437\n",
      "(64, 21)\n",
      "Epoch 5 Batch 70 Loss 1.1596\n",
      "(64, 21)\n",
      "Epoch 5 Batch 71 Loss 1.1199\n",
      "(64, 21)\n",
      "Epoch 5 Batch 72 Loss 1.1096\n",
      "(64, 21)\n",
      "Epoch 5 Batch 73 Loss 1.1578\n",
      "(64, 21)\n",
      "Epoch 5 Batch 74 Loss 1.0255\n",
      "(64, 21)\n",
      "Epoch 5 Batch 75 Loss 1.0259\n",
      "(64, 21)\n",
      "Epoch 5 Batch 76 Loss 0.9791\n",
      "(64, 21)\n",
      "Epoch 5 Batch 77 Loss 0.9868\n",
      "(64, 21)\n",
      "Epoch 5 Batch 78 Loss 1.1131\n",
      "(64, 21)\n",
      "Epoch 5 Batch 79 Loss 0.9911\n",
      "(64, 21)\n",
      "Epoch 5 Batch 80 Loss 1.1232\n",
      "Epoch 5 Loss 1.0774\n",
      "Time taken for 1 epoch 246.30391383171082 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:46:54.995148: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 6 Batch 0 Loss 1.0682\n",
      "(64, 21)\n",
      "Epoch 6 Batch 1 Loss 0.9903\n",
      "(64, 21)\n",
      "Epoch 6 Batch 2 Loss 0.9087\n",
      "(64, 21)\n",
      "Epoch 6 Batch 3 Loss 0.9253\n",
      "(64, 21)\n",
      "Epoch 6 Batch 4 Loss 1.0129\n",
      "(64, 21)\n",
      "Epoch 6 Batch 5 Loss 0.8764\n",
      "(64, 21)\n",
      "Epoch 6 Batch 6 Loss 0.9230\n",
      "(64, 21)\n",
      "Epoch 6 Batch 7 Loss 0.9004\n",
      "(64, 21)\n",
      "Epoch 6 Batch 8 Loss 0.8346\n",
      "(64, 21)\n",
      "Epoch 6 Batch 9 Loss 0.9603\n",
      "(64, 21)\n",
      "Epoch 6 Batch 10 Loss 0.8836\n",
      "(64, 21)\n",
      "Epoch 6 Batch 11 Loss 1.0670\n",
      "(64, 21)\n",
      "Epoch 6 Batch 12 Loss 0.9865\n",
      "(64, 21)\n",
      "Epoch 6 Batch 13 Loss 1.0402\n",
      "(64, 21)\n",
      "Epoch 6 Batch 14 Loss 0.9686\n",
      "(64, 21)\n",
      "Epoch 6 Batch 15 Loss 0.9463\n",
      "(64, 21)\n",
      "Epoch 6 Batch 16 Loss 0.9151\n",
      "(64, 21)\n",
      "Epoch 6 Batch 17 Loss 0.8290\n",
      "(64, 21)\n",
      "Epoch 6 Batch 18 Loss 0.8075\n",
      "(64, 21)\n",
      "Epoch 6 Batch 19 Loss 0.8995\n",
      "(64, 21)\n",
      "Epoch 6 Batch 20 Loss 1.1989\n",
      "(64, 21)\n",
      "Epoch 6 Batch 21 Loss 0.9933\n",
      "(64, 21)\n",
      "Epoch 6 Batch 22 Loss 1.0262\n",
      "(64, 21)\n",
      "Epoch 6 Batch 23 Loss 0.9431\n",
      "(64, 21)\n",
      "Epoch 6 Batch 24 Loss 0.9019\n",
      "(64, 21)\n",
      "Epoch 6 Batch 25 Loss 0.9675\n",
      "(64, 21)\n",
      "Epoch 6 Batch 26 Loss 1.0306\n",
      "(64, 21)\n",
      "Epoch 6 Batch 27 Loss 0.9141\n",
      "(64, 21)\n",
      "Epoch 6 Batch 28 Loss 1.0197\n",
      "(64, 21)\n",
      "Epoch 6 Batch 29 Loss 0.9979\n",
      "(64, 21)\n",
      "Epoch 6 Batch 30 Loss 1.0630\n",
      "(64, 21)\n",
      "Epoch 6 Batch 31 Loss 0.9068\n",
      "(64, 21)\n",
      "Epoch 6 Batch 32 Loss 0.9771\n",
      "(64, 21)\n",
      "Epoch 6 Batch 33 Loss 0.8521\n",
      "(64, 21)\n",
      "Epoch 6 Batch 34 Loss 0.8656\n",
      "(64, 21)\n",
      "Epoch 6 Batch 35 Loss 1.0097\n",
      "(64, 21)\n",
      "Epoch 6 Batch 36 Loss 0.9497\n",
      "(64, 21)\n",
      "Epoch 6 Batch 37 Loss 0.9542\n",
      "(64, 21)\n",
      "Epoch 6 Batch 38 Loss 0.9827\n",
      "(64, 21)\n",
      "Epoch 6 Batch 39 Loss 0.9380\n",
      "(64, 21)\n",
      "Epoch 6 Batch 40 Loss 0.9264\n",
      "(64, 21)\n",
      "Epoch 6 Batch 41 Loss 0.9875\n",
      "(64, 21)\n",
      "Epoch 6 Batch 42 Loss 0.8704\n",
      "(64, 21)\n",
      "Epoch 6 Batch 43 Loss 0.9539\n",
      "(64, 21)\n",
      "Epoch 6 Batch 44 Loss 1.0104\n",
      "(64, 21)\n",
      "Epoch 6 Batch 45 Loss 0.9391\n",
      "(64, 21)\n",
      "Epoch 6 Batch 46 Loss 1.0836\n",
      "(64, 21)\n",
      "Epoch 6 Batch 47 Loss 1.0521\n",
      "(64, 21)\n",
      "Epoch 6 Batch 48 Loss 0.9083\n",
      "(64, 21)\n",
      "Epoch 6 Batch 49 Loss 0.9430\n",
      "(64, 21)\n",
      "Epoch 6 Batch 50 Loss 1.0323\n",
      "(64, 21)\n",
      "Epoch 6 Batch 51 Loss 0.8699\n",
      "(64, 21)\n",
      "Epoch 6 Batch 52 Loss 1.0135\n",
      "(64, 21)\n",
      "Epoch 6 Batch 53 Loss 0.9265\n",
      "(64, 21)\n",
      "Epoch 6 Batch 54 Loss 0.9715\n",
      "(64, 21)\n",
      "Epoch 6 Batch 55 Loss 0.9324\n",
      "(64, 21)\n",
      "Epoch 6 Batch 56 Loss 0.9394\n",
      "(64, 21)\n",
      "Epoch 6 Batch 57 Loss 0.9736\n",
      "(64, 21)\n",
      "Epoch 6 Batch 58 Loss 0.8518\n",
      "(64, 21)\n",
      "Epoch 6 Batch 59 Loss 0.9360\n",
      "(64, 21)\n",
      "Epoch 6 Batch 60 Loss 0.9015\n",
      "(64, 21)\n",
      "Epoch 6 Batch 61 Loss 0.9034\n",
      "(64, 21)\n",
      "Epoch 6 Batch 62 Loss 0.8944\n",
      "(64, 21)\n",
      "Epoch 6 Batch 63 Loss 0.9390\n",
      "(64, 21)\n",
      "Epoch 6 Batch 64 Loss 1.0039\n",
      "(64, 21)\n",
      "Epoch 6 Batch 65 Loss 1.0159\n",
      "(64, 21)\n",
      "Epoch 6 Batch 66 Loss 0.9081\n",
      "(64, 21)\n",
      "Epoch 6 Batch 67 Loss 0.8551\n",
      "(64, 21)\n",
      "Epoch 6 Batch 68 Loss 0.9494\n",
      "(64, 21)\n",
      "Epoch 6 Batch 69 Loss 1.0020\n",
      "(64, 21)\n",
      "Epoch 6 Batch 70 Loss 0.8819\n",
      "(64, 21)\n",
      "Epoch 6 Batch 71 Loss 1.0025\n",
      "(64, 21)\n",
      "Epoch 6 Batch 72 Loss 1.0158\n",
      "(64, 21)\n",
      "Epoch 6 Batch 73 Loss 1.0655\n",
      "(64, 21)\n",
      "Epoch 6 Batch 74 Loss 0.8402\n",
      "(64, 21)\n",
      "Epoch 6 Batch 75 Loss 1.0226\n",
      "(64, 21)\n",
      "Epoch 6 Batch 76 Loss 1.0599\n",
      "(64, 21)\n",
      "Epoch 6 Batch 77 Loss 1.0343\n",
      "(64, 21)\n",
      "Epoch 6 Batch 78 Loss 1.0043\n",
      "(64, 21)\n",
      "Epoch 6 Batch 79 Loss 1.0013\n",
      "(64, 21)\n",
      "Epoch 6 Batch 80 Loss 0.9070\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:50:53.197086: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.9576\n",
      "Time taken for 1 epoch 238.78464794158936 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 7 Batch 0 Loss 0.8505\n",
      "(64, 21)\n",
      "Epoch 7 Batch 1 Loss 0.8235\n",
      "(64, 21)\n",
      "Epoch 7 Batch 2 Loss 0.8626\n",
      "(64, 21)\n",
      "Epoch 7 Batch 3 Loss 0.8983\n",
      "(64, 21)\n",
      "Epoch 7 Batch 4 Loss 0.7762\n",
      "(64, 21)\n",
      "Epoch 7 Batch 5 Loss 0.8641\n",
      "(64, 21)\n",
      "Epoch 7 Batch 6 Loss 0.6539\n",
      "(64, 21)\n",
      "Epoch 7 Batch 7 Loss 0.7816\n",
      "(64, 21)\n",
      "Epoch 7 Batch 8 Loss 0.7985\n",
      "(64, 21)\n",
      "Epoch 7 Batch 9 Loss 0.7845\n",
      "(64, 21)\n",
      "Epoch 7 Batch 10 Loss 0.8286\n",
      "(64, 21)\n",
      "Epoch 7 Batch 11 Loss 0.8757\n",
      "(64, 21)\n",
      "Epoch 7 Batch 12 Loss 0.9383\n",
      "(64, 21)\n",
      "Epoch 7 Batch 13 Loss 0.7724\n",
      "(64, 21)\n",
      "Epoch 7 Batch 14 Loss 0.8418\n",
      "(64, 21)\n",
      "Epoch 7 Batch 15 Loss 0.7819\n",
      "(64, 21)\n",
      "Epoch 7 Batch 16 Loss 0.8864\n",
      "(64, 21)\n",
      "Epoch 7 Batch 17 Loss 0.7975\n",
      "(64, 21)\n",
      "Epoch 7 Batch 18 Loss 0.8129\n",
      "(64, 21)\n",
      "Epoch 7 Batch 19 Loss 0.9008\n",
      "(64, 21)\n",
      "Epoch 7 Batch 20 Loss 0.8723\n",
      "(64, 21)\n",
      "Epoch 7 Batch 21 Loss 0.8813\n",
      "(64, 21)\n",
      "Epoch 7 Batch 22 Loss 0.7707\n",
      "(64, 21)\n",
      "Epoch 7 Batch 23 Loss 0.8301\n",
      "(64, 21)\n",
      "Epoch 7 Batch 24 Loss 0.8982\n",
      "(64, 21)\n",
      "Epoch 7 Batch 25 Loss 0.8042\n",
      "(64, 21)\n",
      "Epoch 7 Batch 26 Loss 0.7164\n",
      "(64, 21)\n",
      "Epoch 7 Batch 27 Loss 0.9082\n",
      "(64, 21)\n",
      "Epoch 7 Batch 28 Loss 0.7604\n",
      "(64, 21)\n",
      "Epoch 7 Batch 29 Loss 0.7689\n",
      "(64, 21)\n",
      "Epoch 7 Batch 30 Loss 0.7623\n",
      "(64, 21)\n",
      "Epoch 7 Batch 31 Loss 0.8050\n",
      "(64, 21)\n",
      "Epoch 7 Batch 32 Loss 0.8948\n",
      "(64, 21)\n",
      "Epoch 7 Batch 33 Loss 0.9259\n",
      "(64, 21)\n",
      "Epoch 7 Batch 34 Loss 0.8048\n",
      "(64, 21)\n",
      "Epoch 7 Batch 35 Loss 0.8015\n",
      "(64, 21)\n",
      "Epoch 7 Batch 36 Loss 0.8570\n",
      "(64, 21)\n",
      "Epoch 7 Batch 37 Loss 0.8438\n",
      "(64, 21)\n",
      "Epoch 7 Batch 38 Loss 0.8839\n",
      "(64, 21)\n",
      "Epoch 7 Batch 39 Loss 0.8664\n",
      "(64, 21)\n",
      "Epoch 7 Batch 40 Loss 0.7607\n",
      "(64, 21)\n",
      "Epoch 7 Batch 41 Loss 0.7666\n",
      "(64, 21)\n",
      "Epoch 7 Batch 42 Loss 0.7725\n",
      "(64, 21)\n",
      "Epoch 7 Batch 43 Loss 0.8636\n",
      "(64, 21)\n",
      "Epoch 7 Batch 44 Loss 0.8218\n",
      "(64, 21)\n",
      "Epoch 7 Batch 45 Loss 0.9125\n",
      "(64, 21)\n",
      "Epoch 7 Batch 46 Loss 0.9505\n",
      "(64, 21)\n",
      "Epoch 7 Batch 47 Loss 0.9259\n",
      "(64, 21)\n",
      "Epoch 7 Batch 48 Loss 0.8713\n",
      "(64, 21)\n",
      "Epoch 7 Batch 49 Loss 0.8202\n",
      "(64, 21)\n",
      "Epoch 7 Batch 50 Loss 0.8851\n",
      "(64, 21)\n",
      "Epoch 7 Batch 51 Loss 0.9507\n",
      "(64, 21)\n",
      "Epoch 7 Batch 52 Loss 0.7980\n",
      "(64, 21)\n",
      "Epoch 7 Batch 53 Loss 0.8566\n",
      "(64, 21)\n",
      "Epoch 7 Batch 54 Loss 0.9276\n",
      "(64, 21)\n",
      "Epoch 7 Batch 55 Loss 0.8162\n",
      "(64, 21)\n",
      "Epoch 7 Batch 56 Loss 0.7875\n",
      "(64, 21)\n",
      "Epoch 7 Batch 57 Loss 0.8541\n",
      "(64, 21)\n",
      "Epoch 7 Batch 58 Loss 0.8997\n",
      "(64, 21)\n",
      "Epoch 7 Batch 59 Loss 0.8421\n",
      "(64, 21)\n",
      "Epoch 7 Batch 60 Loss 0.8826\n",
      "(64, 21)\n",
      "Epoch 7 Batch 61 Loss 0.8191\n",
      "(64, 21)\n",
      "Epoch 7 Batch 62 Loss 0.8106\n",
      "(64, 21)\n",
      "Epoch 7 Batch 63 Loss 0.8290\n",
      "(64, 21)\n",
      "Epoch 7 Batch 64 Loss 0.8706\n",
      "(64, 21)\n",
      "Epoch 7 Batch 65 Loss 0.9641\n",
      "(64, 21)\n",
      "Epoch 7 Batch 66 Loss 0.9463\n",
      "(64, 21)\n",
      "Epoch 7 Batch 67 Loss 0.8043\n",
      "(64, 21)\n",
      "Epoch 7 Batch 68 Loss 0.8092\n",
      "(64, 21)\n",
      "Epoch 7 Batch 69 Loss 0.8017\n",
      "(64, 21)\n",
      "Epoch 7 Batch 70 Loss 0.8416\n",
      "(64, 21)\n",
      "Epoch 7 Batch 71 Loss 0.8641\n",
      "(64, 21)\n",
      "Epoch 7 Batch 72 Loss 0.8088\n",
      "(64, 21)\n",
      "Epoch 7 Batch 73 Loss 0.8172\n",
      "(64, 21)\n",
      "Epoch 7 Batch 74 Loss 0.8733\n",
      "(64, 21)\n",
      "Epoch 7 Batch 75 Loss 0.9314\n",
      "(64, 21)\n",
      "Epoch 7 Batch 76 Loss 0.8043\n",
      "(64, 21)\n",
      "Epoch 7 Batch 77 Loss 0.8746\n",
      "(64, 21)\n",
      "Epoch 7 Batch 78 Loss 0.9362\n",
      "(64, 21)\n",
      "Epoch 7 Batch 79 Loss 0.8041\n",
      "(64, 21)\n",
      "Epoch 7 Batch 80 Loss 0.8033\n",
      "Epoch 7 Loss 0.8416\n",
      "Time taken for 1 epoch 249.27839303016663 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:55:03.057514: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 8 Batch 0 Loss 0.7145\n",
      "(64, 21)\n",
      "Epoch 8 Batch 1 Loss 0.7546\n",
      "(64, 21)\n",
      "Epoch 8 Batch 2 Loss 0.6499\n",
      "(64, 21)\n",
      "Epoch 8 Batch 3 Loss 0.6597\n",
      "(64, 21)\n",
      "Epoch 8 Batch 4 Loss 0.6519\n",
      "(64, 21)\n",
      "Epoch 8 Batch 5 Loss 0.7481\n",
      "(64, 21)\n",
      "Epoch 8 Batch 6 Loss 0.7370\n",
      "(64, 21)\n",
      "Epoch 8 Batch 7 Loss 0.6536\n",
      "(64, 21)\n",
      "Epoch 8 Batch 8 Loss 0.8165\n",
      "(64, 21)\n",
      "Epoch 8 Batch 9 Loss 0.7305\n",
      "(64, 21)\n",
      "Epoch 8 Batch 10 Loss 0.7145\n",
      "(64, 21)\n",
      "Epoch 8 Batch 11 Loss 0.6771\n",
      "(64, 21)\n",
      "Epoch 8 Batch 12 Loss 0.7454\n",
      "(64, 21)\n",
      "Epoch 8 Batch 13 Loss 0.7898\n",
      "(64, 21)\n",
      "Epoch 8 Batch 14 Loss 0.7919\n",
      "(64, 21)\n",
      "Epoch 8 Batch 15 Loss 0.7128\n",
      "(64, 21)\n",
      "Epoch 8 Batch 16 Loss 0.7125\n",
      "(64, 21)\n",
      "Epoch 8 Batch 17 Loss 0.7387\n",
      "(64, 21)\n",
      "Epoch 8 Batch 18 Loss 0.6760\n",
      "(64, 21)\n",
      "Epoch 8 Batch 19 Loss 0.7011\n",
      "(64, 21)\n",
      "Epoch 8 Batch 20 Loss 0.7513\n",
      "(64, 21)\n",
      "Epoch 8 Batch 21 Loss 0.6920\n",
      "(64, 21)\n",
      "Epoch 8 Batch 22 Loss 0.6654\n",
      "(64, 21)\n",
      "Epoch 8 Batch 23 Loss 0.7253\n",
      "(64, 21)\n",
      "Epoch 8 Batch 24 Loss 0.7189\n",
      "(64, 21)\n",
      "Epoch 8 Batch 25 Loss 0.7676\n",
      "(64, 21)\n",
      "Epoch 8 Batch 26 Loss 0.7789\n",
      "(64, 21)\n",
      "Epoch 8 Batch 27 Loss 0.7680\n",
      "(64, 21)\n",
      "Epoch 8 Batch 28 Loss 0.7222\n",
      "(64, 21)\n",
      "Epoch 8 Batch 29 Loss 0.6692\n",
      "(64, 21)\n",
      "Epoch 8 Batch 30 Loss 0.6543\n",
      "(64, 21)\n",
      "Epoch 8 Batch 31 Loss 0.6866\n",
      "(64, 21)\n",
      "Epoch 8 Batch 32 Loss 0.7464\n",
      "(64, 21)\n",
      "Epoch 8 Batch 33 Loss 0.7938\n",
      "(64, 21)\n",
      "Epoch 8 Batch 34 Loss 0.6770\n",
      "(64, 21)\n",
      "Epoch 8 Batch 35 Loss 0.7211\n",
      "(64, 21)\n",
      "Epoch 8 Batch 36 Loss 0.7940\n",
      "(64, 21)\n",
      "Epoch 8 Batch 37 Loss 0.6062\n",
      "(64, 21)\n",
      "Epoch 8 Batch 38 Loss 0.6564\n",
      "(64, 21)\n",
      "Epoch 8 Batch 39 Loss 0.7050\n",
      "(64, 21)\n",
      "Epoch 8 Batch 40 Loss 0.7089\n",
      "(64, 21)\n",
      "Epoch 8 Batch 41 Loss 0.7788\n",
      "(64, 21)\n",
      "Epoch 8 Batch 42 Loss 0.6842\n",
      "(64, 21)\n",
      "Epoch 8 Batch 43 Loss 0.7769\n",
      "(64, 21)\n",
      "Epoch 8 Batch 44 Loss 0.7218\n",
      "(64, 21)\n",
      "Epoch 8 Batch 45 Loss 0.8035\n",
      "(64, 21)\n",
      "Epoch 8 Batch 46 Loss 0.7610\n",
      "(64, 21)\n",
      "Epoch 8 Batch 47 Loss 0.7883\n",
      "(64, 21)\n",
      "Epoch 8 Batch 48 Loss 0.8044\n",
      "(64, 21)\n",
      "Epoch 8 Batch 49 Loss 0.6919\n",
      "(64, 21)\n",
      "Epoch 8 Batch 50 Loss 0.6407\n",
      "(64, 21)\n",
      "Epoch 8 Batch 51 Loss 0.7615\n",
      "(64, 21)\n",
      "Epoch 8 Batch 52 Loss 0.7072\n",
      "(64, 21)\n",
      "Epoch 8 Batch 53 Loss 0.7591\n",
      "(64, 21)\n",
      "Epoch 8 Batch 54 Loss 0.6747\n",
      "(64, 21)\n",
      "Epoch 8 Batch 55 Loss 0.5988\n",
      "(64, 21)\n",
      "Epoch 8 Batch 56 Loss 0.6683\n",
      "(64, 21)\n",
      "Epoch 8 Batch 57 Loss 0.6860\n",
      "(64, 21)\n",
      "Epoch 8 Batch 58 Loss 0.7224\n",
      "(64, 21)\n",
      "Epoch 8 Batch 59 Loss 0.7220\n",
      "(64, 21)\n",
      "Epoch 8 Batch 60 Loss 0.7634\n",
      "(64, 21)\n",
      "Epoch 8 Batch 61 Loss 0.7523\n",
      "(64, 21)\n",
      "Epoch 8 Batch 62 Loss 0.6724\n",
      "(64, 21)\n",
      "Epoch 8 Batch 63 Loss 0.7476\n",
      "(64, 21)\n",
      "Epoch 8 Batch 64 Loss 0.6863\n",
      "(64, 21)\n",
      "Epoch 8 Batch 65 Loss 0.8025\n",
      "(64, 21)\n",
      "Epoch 8 Batch 66 Loss 0.7853\n",
      "(64, 21)\n",
      "Epoch 8 Batch 67 Loss 0.6895\n",
      "(64, 21)\n",
      "Epoch 8 Batch 68 Loss 0.8189\n",
      "(64, 21)\n",
      "Epoch 8 Batch 69 Loss 0.8017\n",
      "(64, 21)\n",
      "Epoch 8 Batch 70 Loss 0.7340\n",
      "(64, 21)\n",
      "Epoch 8 Batch 71 Loss 0.7840\n",
      "(64, 21)\n",
      "Epoch 8 Batch 72 Loss 0.8190\n",
      "(64, 21)\n",
      "Epoch 8 Batch 73 Loss 0.7077\n",
      "(64, 21)\n",
      "Epoch 8 Batch 74 Loss 0.6918\n",
      "(64, 21)\n",
      "Epoch 8 Batch 75 Loss 0.6395\n",
      "(64, 21)\n",
      "Epoch 8 Batch 76 Loss 0.7386\n",
      "(64, 21)\n",
      "Epoch 8 Batch 77 Loss 0.6677\n",
      "(64, 21)\n",
      "Epoch 8 Batch 78 Loss 0.7305\n",
      "(64, 21)\n",
      "Epoch 8 Batch 79 Loss 0.8677\n",
      "(64, 21)\n",
      "Epoch 8 Batch 80 Loss 0.7283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 15:59:35.976682: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.7255\n",
      "Time taken for 1 epoch 273.50150299072266 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 9 Batch 0 Loss 0.7005\n",
      "(64, 21)\n",
      "Epoch 9 Batch 1 Loss 0.5236\n",
      "(64, 21)\n",
      "Epoch 9 Batch 2 Loss 0.6338\n",
      "(64, 21)\n",
      "Epoch 9 Batch 3 Loss 0.6387\n",
      "(64, 21)\n",
      "Epoch 9 Batch 4 Loss 0.5913\n",
      "(64, 21)\n",
      "Epoch 9 Batch 5 Loss 0.6115\n",
      "(64, 21)\n",
      "Epoch 9 Batch 6 Loss 0.7098\n",
      "(64, 21)\n",
      "Epoch 9 Batch 7 Loss 0.6780\n",
      "(64, 21)\n",
      "Epoch 9 Batch 8 Loss 0.5715\n",
      "(64, 21)\n",
      "Epoch 9 Batch 9 Loss 0.5890\n",
      "(64, 21)\n",
      "Epoch 9 Batch 10 Loss 0.4823\n",
      "(64, 21)\n",
      "Epoch 9 Batch 11 Loss 0.6299\n",
      "(64, 21)\n",
      "Epoch 9 Batch 12 Loss 0.6367\n",
      "(64, 21)\n",
      "Epoch 9 Batch 13 Loss 0.5151\n",
      "(64, 21)\n",
      "Epoch 9 Batch 14 Loss 0.5661\n",
      "(64, 21)\n",
      "Epoch 9 Batch 15 Loss 0.5319\n",
      "(64, 21)\n",
      "Epoch 9 Batch 16 Loss 0.5846\n",
      "(64, 21)\n",
      "Epoch 9 Batch 17 Loss 0.5439\n",
      "(64, 21)\n",
      "Epoch 9 Batch 18 Loss 0.6692\n",
      "(64, 21)\n",
      "Epoch 9 Batch 19 Loss 0.6270\n",
      "(64, 21)\n",
      "Epoch 9 Batch 20 Loss 0.5523\n",
      "(64, 21)\n",
      "Epoch 9 Batch 21 Loss 0.5816\n",
      "(64, 21)\n",
      "Epoch 9 Batch 22 Loss 0.5715\n",
      "(64, 21)\n",
      "Epoch 9 Batch 23 Loss 0.6029\n",
      "(64, 21)\n",
      "Epoch 9 Batch 24 Loss 0.6445\n",
      "(64, 21)\n",
      "Epoch 9 Batch 25 Loss 0.5653\n",
      "(64, 21)\n",
      "Epoch 9 Batch 26 Loss 0.5766\n",
      "(64, 21)\n",
      "Epoch 9 Batch 27 Loss 0.6026\n",
      "(64, 21)\n",
      "Epoch 9 Batch 28 Loss 0.6399\n",
      "(64, 21)\n",
      "Epoch 9 Batch 29 Loss 0.5765\n",
      "(64, 21)\n",
      "Epoch 9 Batch 30 Loss 0.4882\n",
      "(64, 21)\n",
      "Epoch 9 Batch 31 Loss 0.6336\n",
      "(64, 21)\n",
      "Epoch 9 Batch 32 Loss 0.6660\n",
      "(64, 21)\n",
      "Epoch 9 Batch 33 Loss 0.6808\n",
      "(64, 21)\n",
      "Epoch 9 Batch 34 Loss 0.5680\n",
      "(64, 21)\n",
      "Epoch 9 Batch 35 Loss 0.6084\n",
      "(64, 21)\n",
      "Epoch 9 Batch 36 Loss 0.5786\n",
      "(64, 21)\n",
      "Epoch 9 Batch 37 Loss 0.6277\n",
      "(64, 21)\n",
      "Epoch 9 Batch 38 Loss 0.5691\n",
      "(64, 21)\n",
      "Epoch 9 Batch 39 Loss 0.6174\n",
      "(64, 21)\n",
      "Epoch 9 Batch 40 Loss 0.5007\n",
      "(64, 21)\n",
      "Epoch 9 Batch 41 Loss 0.6046\n",
      "(64, 21)\n",
      "Epoch 9 Batch 42 Loss 0.6287\n",
      "(64, 21)\n",
      "Epoch 9 Batch 43 Loss 0.5778\n",
      "(64, 21)\n",
      "Epoch 9 Batch 44 Loss 0.6679\n",
      "(64, 21)\n",
      "Epoch 9 Batch 45 Loss 0.6690\n",
      "(64, 21)\n",
      "Epoch 9 Batch 46 Loss 0.6021\n",
      "(64, 21)\n",
      "Epoch 9 Batch 47 Loss 0.6047\n",
      "(64, 21)\n",
      "Epoch 9 Batch 48 Loss 0.7258\n",
      "(64, 21)\n",
      "Epoch 9 Batch 49 Loss 0.6858\n",
      "(64, 21)\n",
      "Epoch 9 Batch 50 Loss 0.5504\n",
      "(64, 21)\n",
      "Epoch 9 Batch 51 Loss 0.5910\n",
      "(64, 21)\n",
      "Epoch 9 Batch 52 Loss 0.6178\n",
      "(64, 21)\n",
      "Epoch 9 Batch 53 Loss 0.6780\n",
      "(64, 21)\n",
      "Epoch 9 Batch 54 Loss 0.6550\n",
      "(64, 21)\n",
      "Epoch 9 Batch 55 Loss 0.6154\n",
      "(64, 21)\n",
      "Epoch 9 Batch 56 Loss 0.6290\n",
      "(64, 21)\n",
      "Epoch 9 Batch 57 Loss 0.6660\n",
      "(64, 21)\n",
      "Epoch 9 Batch 58 Loss 0.6428\n",
      "(64, 21)\n",
      "Epoch 9 Batch 59 Loss 0.5819\n",
      "(64, 21)\n",
      "Epoch 9 Batch 60 Loss 0.6593\n",
      "(64, 21)\n",
      "Epoch 9 Batch 61 Loss 0.5864\n",
      "(64, 21)\n",
      "Epoch 9 Batch 62 Loss 0.6730\n",
      "(64, 21)\n",
      "Epoch 9 Batch 63 Loss 0.6514\n",
      "(64, 21)\n",
      "Epoch 9 Batch 64 Loss 0.6937\n",
      "(64, 21)\n",
      "Epoch 9 Batch 65 Loss 0.6380\n",
      "(64, 21)\n",
      "Epoch 9 Batch 66 Loss 0.6486\n",
      "(64, 21)\n",
      "Epoch 9 Batch 67 Loss 0.6082\n",
      "(64, 21)\n",
      "Epoch 9 Batch 68 Loss 0.7228\n",
      "(64, 21)\n",
      "Epoch 9 Batch 69 Loss 0.7269\n",
      "(64, 21)\n",
      "Epoch 9 Batch 70 Loss 0.5144\n",
      "(64, 21)\n",
      "Epoch 9 Batch 71 Loss 0.6236\n",
      "(64, 21)\n",
      "Epoch 9 Batch 72 Loss 0.6624\n",
      "(64, 21)\n",
      "Epoch 9 Batch 73 Loss 0.5391\n",
      "(64, 21)\n",
      "Epoch 9 Batch 74 Loss 0.6548\n",
      "(64, 21)\n",
      "Epoch 9 Batch 75 Loss 0.7363\n",
      "(64, 21)\n",
      "Epoch 9 Batch 76 Loss 0.6600\n",
      "(64, 21)\n",
      "Epoch 9 Batch 77 Loss 0.7223\n",
      "(64, 21)\n",
      "Epoch 9 Batch 78 Loss 0.6490\n",
      "(64, 21)\n",
      "Epoch 9 Batch 79 Loss 0.6577\n",
      "(64, 21)\n",
      "Epoch 9 Batch 80 Loss 0.5699\n",
      "Epoch 9 Loss 0.6182\n",
      "Time taken for 1 epoch 254.83734798431396 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:03:51.396968: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 10 Batch 0 Loss 0.5323\n",
      "(64, 21)\n",
      "Epoch 10 Batch 1 Loss 0.4480\n",
      "(64, 21)\n",
      "Epoch 10 Batch 2 Loss 0.5033\n",
      "(64, 21)\n",
      "Epoch 10 Batch 3 Loss 0.5320\n",
      "(64, 21)\n",
      "Epoch 10 Batch 4 Loss 0.4847\n",
      "(64, 21)\n",
      "Epoch 10 Batch 5 Loss 0.4097\n",
      "(64, 21)\n",
      "Epoch 10 Batch 6 Loss 0.4469\n",
      "(64, 21)\n",
      "Epoch 10 Batch 7 Loss 0.4008\n",
      "(64, 21)\n",
      "Epoch 10 Batch 8 Loss 0.5025\n",
      "(64, 21)\n",
      "Epoch 10 Batch 9 Loss 0.4962\n",
      "(64, 21)\n",
      "Epoch 10 Batch 10 Loss 0.4983\n",
      "(64, 21)\n",
      "Epoch 10 Batch 11 Loss 0.5348\n",
      "(64, 21)\n",
      "Epoch 10 Batch 12 Loss 0.4579\n",
      "(64, 21)\n",
      "Epoch 10 Batch 13 Loss 0.4155\n",
      "(64, 21)\n",
      "Epoch 10 Batch 14 Loss 0.4522\n",
      "(64, 21)\n",
      "Epoch 10 Batch 15 Loss 0.4999\n",
      "(64, 21)\n",
      "Epoch 10 Batch 16 Loss 0.4342\n",
      "(64, 21)\n",
      "Epoch 10 Batch 17 Loss 0.4455\n",
      "(64, 21)\n",
      "Epoch 10 Batch 18 Loss 0.6220\n",
      "(64, 21)\n",
      "Epoch 10 Batch 19 Loss 0.5788\n",
      "(64, 21)\n",
      "Epoch 10 Batch 20 Loss 0.4921\n",
      "(64, 21)\n",
      "Epoch 10 Batch 21 Loss 0.4487\n",
      "(64, 21)\n",
      "Epoch 10 Batch 22 Loss 0.5053\n",
      "(64, 21)\n",
      "Epoch 10 Batch 23 Loss 0.5594\n",
      "(64, 21)\n",
      "Epoch 10 Batch 24 Loss 0.4555\n",
      "(64, 21)\n",
      "Epoch 10 Batch 25 Loss 0.5527\n",
      "(64, 21)\n",
      "Epoch 10 Batch 26 Loss 0.5035\n",
      "(64, 21)\n",
      "Epoch 10 Batch 27 Loss 0.4505\n",
      "(64, 21)\n",
      "Epoch 10 Batch 28 Loss 0.5672\n",
      "(64, 21)\n",
      "Epoch 10 Batch 29 Loss 0.5037\n",
      "(64, 21)\n",
      "Epoch 10 Batch 30 Loss 0.5381\n",
      "(64, 21)\n",
      "Epoch 10 Batch 31 Loss 0.5460\n",
      "(64, 21)\n",
      "Epoch 10 Batch 32 Loss 0.5435\n",
      "(64, 21)\n",
      "Epoch 10 Batch 33 Loss 0.5341\n",
      "(64, 21)\n",
      "Epoch 10 Batch 34 Loss 0.5288\n",
      "(64, 21)\n",
      "Epoch 10 Batch 35 Loss 0.5678\n",
      "(64, 21)\n",
      "Epoch 10 Batch 36 Loss 0.5053\n",
      "(64, 21)\n",
      "Epoch 10 Batch 37 Loss 0.5545\n",
      "(64, 21)\n",
      "Epoch 10 Batch 38 Loss 0.5054\n",
      "(64, 21)\n",
      "Epoch 10 Batch 39 Loss 0.5668\n",
      "(64, 21)\n",
      "Epoch 10 Batch 40 Loss 0.4406\n",
      "(64, 21)\n",
      "Epoch 10 Batch 41 Loss 0.5252\n",
      "(64, 21)\n",
      "Epoch 10 Batch 42 Loss 0.5811\n",
      "(64, 21)\n",
      "Epoch 10 Batch 43 Loss 0.5078\n",
      "(64, 21)\n",
      "Epoch 10 Batch 44 Loss 0.5170\n",
      "(64, 21)\n",
      "Epoch 10 Batch 45 Loss 0.5510\n",
      "(64, 21)\n",
      "Epoch 10 Batch 46 Loss 0.5281\n",
      "(64, 21)\n",
      "Epoch 10 Batch 47 Loss 0.5040\n",
      "(64, 21)\n",
      "Epoch 10 Batch 48 Loss 0.5199\n",
      "(64, 21)\n",
      "Epoch 10 Batch 49 Loss 0.4367\n",
      "(64, 21)\n",
      "Epoch 10 Batch 50 Loss 0.6097\n",
      "(64, 21)\n",
      "Epoch 10 Batch 51 Loss 0.5567\n",
      "(64, 21)\n",
      "Epoch 10 Batch 52 Loss 0.4609\n",
      "(64, 21)\n",
      "Epoch 10 Batch 53 Loss 0.4621\n",
      "(64, 21)\n",
      "Epoch 10 Batch 54 Loss 0.4995\n",
      "(64, 21)\n",
      "Epoch 10 Batch 55 Loss 0.5875\n",
      "(64, 21)\n",
      "Epoch 10 Batch 56 Loss 0.4721\n",
      "(64, 21)\n",
      "Epoch 10 Batch 57 Loss 0.6283\n",
      "(64, 21)\n",
      "Epoch 10 Batch 58 Loss 0.5758\n",
      "(64, 21)\n",
      "Epoch 10 Batch 59 Loss 0.4649\n",
      "(64, 21)\n",
      "Epoch 10 Batch 60 Loss 0.5682\n",
      "(64, 21)\n",
      "Epoch 10 Batch 61 Loss 0.5673\n",
      "(64, 21)\n",
      "Epoch 10 Batch 62 Loss 0.5843\n",
      "(64, 21)\n",
      "Epoch 10 Batch 63 Loss 0.4438\n",
      "(64, 21)\n",
      "Epoch 10 Batch 64 Loss 0.5177\n",
      "(64, 21)\n",
      "Epoch 10 Batch 65 Loss 0.6077\n",
      "(64, 21)\n",
      "Epoch 10 Batch 66 Loss 0.5126\n",
      "(64, 21)\n",
      "Epoch 10 Batch 67 Loss 0.6108\n",
      "(64, 21)\n",
      "Epoch 10 Batch 68 Loss 0.5116\n",
      "(64, 21)\n",
      "Epoch 10 Batch 69 Loss 0.5243\n",
      "(64, 21)\n",
      "Epoch 10 Batch 70 Loss 0.4356\n",
      "(64, 21)\n",
      "Epoch 10 Batch 71 Loss 0.5456\n",
      "(64, 21)\n",
      "Epoch 10 Batch 72 Loss 0.4840\n",
      "(64, 21)\n",
      "Epoch 10 Batch 73 Loss 0.5938\n",
      "(64, 21)\n",
      "Epoch 10 Batch 74 Loss 0.5292\n",
      "(64, 21)\n",
      "Epoch 10 Batch 75 Loss 0.5738\n",
      "(64, 21)\n",
      "Epoch 10 Batch 76 Loss 0.4746\n",
      "(64, 21)\n",
      "Epoch 10 Batch 77 Loss 0.5647\n",
      "(64, 21)\n",
      "Epoch 10 Batch 78 Loss 0.5196\n",
      "(64, 21)\n",
      "Epoch 10 Batch 79 Loss 0.5289\n",
      "(64, 21)\n",
      "Epoch 10 Batch 80 Loss 0.5514\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:07:53.958250: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.5161\n",
      "Time taken for 1 epoch 243.15833926200867 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 11 Batch 0 Loss 0.4224\n",
      "(64, 21)\n",
      "Epoch 11 Batch 1 Loss 0.3691\n",
      "(64, 21)\n",
      "Epoch 11 Batch 2 Loss 0.4027\n",
      "(64, 21)\n",
      "Epoch 11 Batch 3 Loss 0.3986\n",
      "(64, 21)\n",
      "Epoch 11 Batch 4 Loss 0.4163\n",
      "(64, 21)\n",
      "Epoch 11 Batch 5 Loss 0.3544\n",
      "(64, 21)\n",
      "Epoch 11 Batch 6 Loss 0.3209\n",
      "(64, 21)\n",
      "Epoch 11 Batch 7 Loss 0.4187\n",
      "(64, 21)\n",
      "Epoch 11 Batch 8 Loss 0.3930\n",
      "(64, 21)\n",
      "Epoch 11 Batch 9 Loss 0.4395\n",
      "(64, 21)\n",
      "Epoch 11 Batch 10 Loss 0.3932\n",
      "(64, 21)\n",
      "Epoch 11 Batch 11 Loss 0.4196\n",
      "(64, 21)\n",
      "Epoch 11 Batch 12 Loss 0.4259\n",
      "(64, 21)\n",
      "Epoch 11 Batch 13 Loss 0.4342\n",
      "(64, 21)\n",
      "Epoch 11 Batch 14 Loss 0.3843\n",
      "(64, 21)\n",
      "Epoch 11 Batch 15 Loss 0.4042\n",
      "(64, 21)\n",
      "Epoch 11 Batch 16 Loss 0.3768\n",
      "(64, 21)\n",
      "Epoch 11 Batch 17 Loss 0.3771\n",
      "(64, 21)\n",
      "Epoch 11 Batch 18 Loss 0.3604\n",
      "(64, 21)\n",
      "Epoch 11 Batch 19 Loss 0.4150\n",
      "(64, 21)\n",
      "Epoch 11 Batch 20 Loss 0.3333\n",
      "(64, 21)\n",
      "Epoch 11 Batch 21 Loss 0.3939\n",
      "(64, 21)\n",
      "Epoch 11 Batch 22 Loss 0.3920\n",
      "(64, 21)\n",
      "Epoch 11 Batch 23 Loss 0.3902\n",
      "(64, 21)\n",
      "Epoch 11 Batch 24 Loss 0.3689\n",
      "(64, 21)\n",
      "Epoch 11 Batch 25 Loss 0.4206\n",
      "(64, 21)\n",
      "Epoch 11 Batch 26 Loss 0.4070\n",
      "(64, 21)\n",
      "Epoch 11 Batch 27 Loss 0.3897\n",
      "(64, 21)\n",
      "Epoch 11 Batch 28 Loss 0.4883\n",
      "(64, 21)\n",
      "Epoch 11 Batch 29 Loss 0.4607\n",
      "(64, 21)\n",
      "Epoch 11 Batch 30 Loss 0.4202\n",
      "(64, 21)\n",
      "Epoch 11 Batch 31 Loss 0.4160\n",
      "(64, 21)\n",
      "Epoch 11 Batch 32 Loss 0.4540\n",
      "(64, 21)\n",
      "Epoch 11 Batch 33 Loss 0.4027\n",
      "(64, 21)\n",
      "Epoch 11 Batch 34 Loss 0.4805\n",
      "(64, 21)\n",
      "Epoch 11 Batch 35 Loss 0.4617\n",
      "(64, 21)\n",
      "Epoch 11 Batch 36 Loss 0.3960\n",
      "(64, 21)\n",
      "Epoch 11 Batch 37 Loss 0.4231\n",
      "(64, 21)\n",
      "Epoch 11 Batch 38 Loss 0.4561\n",
      "(64, 21)\n",
      "Epoch 11 Batch 39 Loss 0.4628\n",
      "(64, 21)\n",
      "Epoch 11 Batch 40 Loss 0.3633\n",
      "(64, 21)\n",
      "Epoch 11 Batch 41 Loss 0.4604\n",
      "(64, 21)\n",
      "Epoch 11 Batch 42 Loss 0.4439\n",
      "(64, 21)\n",
      "Epoch 11 Batch 43 Loss 0.3725\n",
      "(64, 21)\n",
      "Epoch 11 Batch 44 Loss 0.4679\n",
      "(64, 21)\n",
      "Epoch 11 Batch 45 Loss 0.4446\n",
      "(64, 21)\n",
      "Epoch 11 Batch 46 Loss 0.4152\n",
      "(64, 21)\n",
      "Epoch 11 Batch 47 Loss 0.3644\n",
      "(64, 21)\n",
      "Epoch 11 Batch 48 Loss 0.3982\n",
      "(64, 21)\n",
      "Epoch 11 Batch 49 Loss 0.4283\n",
      "(64, 21)\n",
      "Epoch 11 Batch 50 Loss 0.4462\n",
      "(64, 21)\n",
      "Epoch 11 Batch 51 Loss 0.3580\n",
      "(64, 21)\n",
      "Epoch 11 Batch 52 Loss 0.4290\n",
      "(64, 21)\n",
      "Epoch 11 Batch 53 Loss 0.4418\n",
      "(64, 21)\n",
      "Epoch 11 Batch 54 Loss 0.4286\n",
      "(64, 21)\n",
      "Epoch 11 Batch 55 Loss 0.3983\n",
      "(64, 21)\n",
      "Epoch 11 Batch 56 Loss 0.4390\n",
      "(64, 21)\n",
      "Epoch 11 Batch 57 Loss 0.4063\n",
      "(64, 21)\n",
      "Epoch 11 Batch 58 Loss 0.4668\n",
      "(64, 21)\n",
      "Epoch 11 Batch 59 Loss 0.4130\n",
      "(64, 21)\n",
      "Epoch 11 Batch 60 Loss 0.3573\n",
      "(64, 21)\n",
      "Epoch 11 Batch 61 Loss 0.4159\n",
      "(64, 21)\n",
      "Epoch 11 Batch 62 Loss 0.4052\n",
      "(64, 21)\n",
      "Epoch 11 Batch 63 Loss 0.4394\n",
      "(64, 21)\n",
      "Epoch 11 Batch 64 Loss 0.4667\n",
      "(64, 21)\n",
      "Epoch 11 Batch 65 Loss 0.4577\n",
      "(64, 21)\n",
      "Epoch 11 Batch 66 Loss 0.4823\n",
      "(64, 21)\n",
      "Epoch 11 Batch 67 Loss 0.4498\n",
      "(64, 21)\n",
      "Epoch 11 Batch 68 Loss 0.4984\n",
      "(64, 21)\n",
      "Epoch 11 Batch 69 Loss 0.3990\n",
      "(64, 21)\n",
      "Epoch 11 Batch 70 Loss 0.3907\n",
      "(64, 21)\n",
      "Epoch 11 Batch 71 Loss 0.4700\n",
      "(64, 21)\n",
      "Epoch 11 Batch 72 Loss 0.3987\n",
      "(64, 21)\n",
      "Epoch 11 Batch 73 Loss 0.5218\n",
      "(64, 21)\n",
      "Epoch 11 Batch 74 Loss 0.3917\n",
      "(64, 21)\n",
      "Epoch 11 Batch 75 Loss 0.4512\n",
      "(64, 21)\n",
      "Epoch 11 Batch 76 Loss 0.5047\n",
      "(64, 21)\n",
      "Epoch 11 Batch 77 Loss 0.4935\n",
      "(64, 21)\n",
      "Epoch 11 Batch 78 Loss 0.4498\n",
      "(64, 21)\n",
      "Epoch 11 Batch 79 Loss 0.5394\n",
      "(64, 21)\n",
      "Epoch 11 Batch 80 Loss 0.4967\n",
      "Epoch 11 Loss 0.4223\n",
      "Time taken for 1 epoch 246.42072701454163 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:12:00.976252: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 12 Batch 0 Loss 0.2403\n",
      "(64, 21)\n",
      "Epoch 12 Batch 1 Loss 0.2740\n",
      "(64, 21)\n",
      "Epoch 12 Batch 2 Loss 0.3043\n",
      "(64, 21)\n",
      "Epoch 12 Batch 3 Loss 0.3236\n",
      "(64, 21)\n",
      "Epoch 12 Batch 4 Loss 0.3065\n",
      "(64, 21)\n",
      "Epoch 12 Batch 5 Loss 0.2990\n",
      "(64, 21)\n",
      "Epoch 12 Batch 6 Loss 0.2958\n",
      "(64, 21)\n",
      "Epoch 12 Batch 7 Loss 0.3218\n",
      "(64, 21)\n",
      "Epoch 12 Batch 8 Loss 0.3014\n",
      "(64, 21)\n",
      "Epoch 12 Batch 9 Loss 0.3460\n",
      "(64, 21)\n",
      "Epoch 12 Batch 10 Loss 0.3273\n",
      "(64, 21)\n",
      "Epoch 12 Batch 11 Loss 0.2877\n",
      "(64, 21)\n",
      "Epoch 12 Batch 12 Loss 0.3024\n",
      "(64, 21)\n",
      "Epoch 12 Batch 13 Loss 0.2833\n",
      "(64, 21)\n",
      "Epoch 12 Batch 14 Loss 0.2511\n",
      "(64, 21)\n",
      "Epoch 12 Batch 15 Loss 0.3433\n",
      "(64, 21)\n",
      "Epoch 12 Batch 16 Loss 0.2688\n",
      "(64, 21)\n",
      "Epoch 12 Batch 17 Loss 0.3243\n",
      "(64, 21)\n",
      "Epoch 12 Batch 18 Loss 0.3640\n",
      "(64, 21)\n",
      "Epoch 12 Batch 19 Loss 0.3223\n",
      "(64, 21)\n",
      "Epoch 12 Batch 20 Loss 0.3164\n",
      "(64, 21)\n",
      "Epoch 12 Batch 21 Loss 0.2809\n",
      "(64, 21)\n",
      "Epoch 12 Batch 22 Loss 0.3485\n",
      "(64, 21)\n",
      "Epoch 12 Batch 23 Loss 0.3554\n",
      "(64, 21)\n",
      "Epoch 12 Batch 24 Loss 0.3086\n",
      "(64, 21)\n",
      "Epoch 12 Batch 25 Loss 0.2241\n",
      "(64, 21)\n",
      "Epoch 12 Batch 26 Loss 0.3649\n",
      "(64, 21)\n",
      "Epoch 12 Batch 27 Loss 0.3237\n",
      "(64, 21)\n",
      "Epoch 12 Batch 28 Loss 0.3407\n",
      "(64, 21)\n",
      "Epoch 12 Batch 29 Loss 0.3391\n",
      "(64, 21)\n",
      "Epoch 12 Batch 30 Loss 0.3575\n",
      "(64, 21)\n",
      "Epoch 12 Batch 31 Loss 0.3126\n",
      "(64, 21)\n",
      "Epoch 12 Batch 32 Loss 0.3398\n",
      "(64, 21)\n",
      "Epoch 12 Batch 33 Loss 0.3086\n",
      "(64, 21)\n",
      "Epoch 12 Batch 34 Loss 0.3266\n",
      "(64, 21)\n",
      "Epoch 12 Batch 35 Loss 0.3612\n",
      "(64, 21)\n",
      "Epoch 12 Batch 36 Loss 0.3907\n",
      "(64, 21)\n",
      "Epoch 12 Batch 37 Loss 0.3513\n",
      "(64, 21)\n",
      "Epoch 12 Batch 38 Loss 0.2798\n",
      "(64, 21)\n",
      "Epoch 12 Batch 39 Loss 0.3538\n",
      "(64, 21)\n",
      "Epoch 12 Batch 40 Loss 0.3937\n",
      "(64, 21)\n",
      "Epoch 12 Batch 41 Loss 0.4063\n",
      "(64, 21)\n",
      "Epoch 12 Batch 42 Loss 0.3647\n",
      "(64, 21)\n",
      "Epoch 12 Batch 43 Loss 0.3070\n",
      "(64, 21)\n",
      "Epoch 12 Batch 44 Loss 0.2981\n",
      "(64, 21)\n",
      "Epoch 12 Batch 45 Loss 0.3514\n",
      "(64, 21)\n",
      "Epoch 12 Batch 46 Loss 0.3225\n",
      "(64, 21)\n",
      "Epoch 12 Batch 47 Loss 0.3344\n",
      "(64, 21)\n",
      "Epoch 12 Batch 48 Loss 0.3829\n",
      "(64, 21)\n",
      "Epoch 12 Batch 49 Loss 0.3455\n",
      "(64, 21)\n",
      "Epoch 12 Batch 50 Loss 0.3546\n",
      "(64, 21)\n",
      "Epoch 12 Batch 51 Loss 0.4005\n",
      "(64, 21)\n",
      "Epoch 12 Batch 52 Loss 0.3601\n",
      "(64, 21)\n",
      "Epoch 12 Batch 53 Loss 0.3871\n",
      "(64, 21)\n",
      "Epoch 12 Batch 54 Loss 0.3338\n",
      "(64, 21)\n",
      "Epoch 12 Batch 55 Loss 0.3311\n",
      "(64, 21)\n",
      "Epoch 12 Batch 56 Loss 0.4321\n",
      "(64, 21)\n",
      "Epoch 12 Batch 57 Loss 0.3324\n",
      "(64, 21)\n",
      "Epoch 12 Batch 58 Loss 0.3553\n",
      "(64, 21)\n",
      "Epoch 12 Batch 59 Loss 0.3873\n",
      "(64, 21)\n",
      "Epoch 12 Batch 60 Loss 0.3184\n",
      "(64, 21)\n",
      "Epoch 12 Batch 61 Loss 0.3451\n",
      "(64, 21)\n",
      "Epoch 12 Batch 62 Loss 0.3811\n",
      "(64, 21)\n",
      "Epoch 12 Batch 63 Loss 0.3741\n",
      "(64, 21)\n",
      "Epoch 12 Batch 64 Loss 0.3084\n",
      "(64, 21)\n",
      "Epoch 12 Batch 65 Loss 0.3606\n",
      "(64, 21)\n",
      "Epoch 12 Batch 66 Loss 0.3525\n",
      "(64, 21)\n",
      "Epoch 12 Batch 67 Loss 0.3518\n",
      "(64, 21)\n",
      "Epoch 12 Batch 68 Loss 0.3570\n",
      "(64, 21)\n",
      "Epoch 12 Batch 69 Loss 0.3629\n",
      "(64, 21)\n",
      "Epoch 12 Batch 70 Loss 0.3321\n",
      "(64, 21)\n",
      "Epoch 12 Batch 71 Loss 0.3644\n",
      "(64, 21)\n",
      "Epoch 12 Batch 72 Loss 0.4104\n",
      "(64, 21)\n",
      "Epoch 12 Batch 73 Loss 0.4010\n",
      "(64, 21)\n",
      "Epoch 12 Batch 74 Loss 0.3524\n",
      "(64, 21)\n",
      "Epoch 12 Batch 75 Loss 0.3884\n",
      "(64, 21)\n",
      "Epoch 12 Batch 76 Loss 0.3880\n",
      "(64, 21)\n",
      "Epoch 12 Batch 77 Loss 0.3660\n",
      "(64, 21)\n",
      "Epoch 12 Batch 78 Loss 0.4076\n",
      "(64, 21)\n",
      "Epoch 12 Batch 79 Loss 0.3544\n",
      "(64, 21)\n",
      "Epoch 12 Batch 80 Loss 0.3414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:15:58.162020: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12 Loss 0.3391\n",
      "Time taken for 1 epoch 237.72154808044434 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 13 Batch 0 Loss 0.2766\n",
      "(64, 21)\n",
      "Epoch 13 Batch 1 Loss 0.2555\n",
      "(64, 21)\n",
      "Epoch 13 Batch 2 Loss 0.2478\n",
      "(64, 21)\n",
      "Epoch 13 Batch 3 Loss 0.2451\n",
      "(64, 21)\n",
      "Epoch 13 Batch 4 Loss 0.2479\n",
      "(64, 21)\n",
      "Epoch 13 Batch 5 Loss 0.2507\n",
      "(64, 21)\n",
      "Epoch 13 Batch 6 Loss 0.2143\n",
      "(64, 21)\n",
      "Epoch 13 Batch 7 Loss 0.2137\n",
      "(64, 21)\n",
      "Epoch 13 Batch 8 Loss 0.2253\n",
      "(64, 21)\n",
      "Epoch 13 Batch 9 Loss 0.2119\n",
      "(64, 21)\n",
      "Epoch 13 Batch 10 Loss 0.2578\n",
      "(64, 21)\n",
      "Epoch 13 Batch 11 Loss 0.2799\n",
      "(64, 21)\n",
      "Epoch 13 Batch 12 Loss 0.2725\n",
      "(64, 21)\n",
      "Epoch 13 Batch 13 Loss 0.2008\n",
      "(64, 21)\n",
      "Epoch 13 Batch 14 Loss 0.2497\n",
      "(64, 21)\n",
      "Epoch 13 Batch 15 Loss 0.2926\n",
      "(64, 21)\n",
      "Epoch 13 Batch 16 Loss 0.2473\n",
      "(64, 21)\n",
      "Epoch 13 Batch 17 Loss 0.2233\n",
      "(64, 21)\n",
      "Epoch 13 Batch 18 Loss 0.2698\n",
      "(64, 21)\n",
      "Epoch 13 Batch 19 Loss 0.2369\n",
      "(64, 21)\n",
      "Epoch 13 Batch 20 Loss 0.2513\n",
      "(64, 21)\n",
      "Epoch 13 Batch 21 Loss 0.2364\n",
      "(64, 21)\n",
      "Epoch 13 Batch 22 Loss 0.2858\n",
      "(64, 21)\n",
      "Epoch 13 Batch 23 Loss 0.2535\n",
      "(64, 21)\n",
      "Epoch 13 Batch 24 Loss 0.2644\n",
      "(64, 21)\n",
      "Epoch 13 Batch 25 Loss 0.2529\n",
      "(64, 21)\n",
      "Epoch 13 Batch 26 Loss 0.2647\n",
      "(64, 21)\n",
      "Epoch 13 Batch 27 Loss 0.2381\n",
      "(64, 21)\n",
      "Epoch 13 Batch 28 Loss 0.2773\n",
      "(64, 21)\n",
      "Epoch 13 Batch 29 Loss 0.2635\n",
      "(64, 21)\n",
      "Epoch 13 Batch 30 Loss 0.2807\n",
      "(64, 21)\n",
      "Epoch 13 Batch 31 Loss 0.3093\n",
      "(64, 21)\n",
      "Epoch 13 Batch 32 Loss 0.3076\n",
      "(64, 21)\n",
      "Epoch 13 Batch 33 Loss 0.2179\n",
      "(64, 21)\n",
      "Epoch 13 Batch 34 Loss 0.2984\n",
      "(64, 21)\n",
      "Epoch 13 Batch 35 Loss 0.2713\n",
      "(64, 21)\n",
      "Epoch 13 Batch 36 Loss 0.2618\n",
      "(64, 21)\n",
      "Epoch 13 Batch 37 Loss 0.2262\n",
      "(64, 21)\n",
      "Epoch 13 Batch 38 Loss 0.2703\n",
      "(64, 21)\n",
      "Epoch 13 Batch 39 Loss 0.2870\n",
      "(64, 21)\n",
      "Epoch 13 Batch 40 Loss 0.3199\n",
      "(64, 21)\n",
      "Epoch 13 Batch 41 Loss 0.2607\n",
      "(64, 21)\n",
      "Epoch 13 Batch 42 Loss 0.3222\n",
      "(64, 21)\n",
      "Epoch 13 Batch 43 Loss 0.3194\n",
      "(64, 21)\n",
      "Epoch 13 Batch 44 Loss 0.2859\n",
      "(64, 21)\n",
      "Epoch 13 Batch 45 Loss 0.2648\n",
      "(64, 21)\n",
      "Epoch 13 Batch 46 Loss 0.2451\n",
      "(64, 21)\n",
      "Epoch 13 Batch 47 Loss 0.2875\n",
      "(64, 21)\n",
      "Epoch 13 Batch 48 Loss 0.2314\n",
      "(64, 21)\n",
      "Epoch 13 Batch 49 Loss 0.2684\n",
      "(64, 21)\n",
      "Epoch 13 Batch 50 Loss 0.2683\n",
      "(64, 21)\n",
      "Epoch 13 Batch 51 Loss 0.2597\n",
      "(64, 21)\n",
      "Epoch 13 Batch 52 Loss 0.2104\n",
      "(64, 21)\n",
      "Epoch 13 Batch 53 Loss 0.2500\n",
      "(64, 21)\n",
      "Epoch 13 Batch 54 Loss 0.2795\n",
      "(64, 21)\n",
      "Epoch 13 Batch 55 Loss 0.2823\n",
      "(64, 21)\n",
      "Epoch 13 Batch 56 Loss 0.2537\n",
      "(64, 21)\n",
      "Epoch 13 Batch 57 Loss 0.3193\n",
      "(64, 21)\n",
      "Epoch 13 Batch 58 Loss 0.3029\n",
      "(64, 21)\n",
      "Epoch 13 Batch 59 Loss 0.3681\n",
      "(64, 21)\n",
      "Epoch 13 Batch 60 Loss 0.2873\n",
      "(64, 21)\n",
      "Epoch 13 Batch 61 Loss 0.2738\n",
      "(64, 21)\n",
      "Epoch 13 Batch 62 Loss 0.2844\n",
      "(64, 21)\n",
      "Epoch 13 Batch 63 Loss 0.3148\n",
      "(64, 21)\n",
      "Epoch 13 Batch 64 Loss 0.3756\n",
      "(64, 21)\n",
      "Epoch 13 Batch 65 Loss 0.2677\n",
      "(64, 21)\n",
      "Epoch 13 Batch 66 Loss 0.2956\n",
      "(64, 21)\n",
      "Epoch 13 Batch 67 Loss 0.3110\n",
      "(64, 21)\n",
      "Epoch 13 Batch 68 Loss 0.2826\n",
      "(64, 21)\n",
      "Epoch 13 Batch 69 Loss 0.3861\n",
      "(64, 21)\n",
      "Epoch 13 Batch 70 Loss 0.2604\n",
      "(64, 21)\n",
      "Epoch 13 Batch 71 Loss 0.3045\n",
      "(64, 21)\n",
      "Epoch 13 Batch 72 Loss 0.2615\n",
      "(64, 21)\n",
      "Epoch 13 Batch 73 Loss 0.3222\n",
      "(64, 21)\n",
      "Epoch 13 Batch 74 Loss 0.2690\n",
      "(64, 21)\n",
      "Epoch 13 Batch 75 Loss 0.2584\n",
      "(64, 21)\n",
      "Epoch 13 Batch 76 Loss 0.2884\n",
      "(64, 21)\n",
      "Epoch 13 Batch 77 Loss 0.3110\n",
      "(64, 21)\n",
      "Epoch 13 Batch 78 Loss 0.2956\n",
      "(64, 21)\n",
      "Epoch 13 Batch 79 Loss 0.2553\n",
      "(64, 21)\n",
      "Epoch 13 Batch 80 Loss 0.3060\n",
      "Epoch 13 Loss 0.2722\n",
      "Time taken for 1 epoch 236.77468299865723 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:19:55.472472: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 14 Batch 0 Loss 0.1927\n",
      "(64, 21)\n",
      "Epoch 14 Batch 1 Loss 0.1995\n",
      "(64, 21)\n",
      "Epoch 14 Batch 2 Loss 0.1820\n",
      "(64, 21)\n",
      "Epoch 14 Batch 3 Loss 0.1899\n",
      "(64, 21)\n",
      "Epoch 14 Batch 4 Loss 0.1927\n",
      "(64, 21)\n",
      "Epoch 14 Batch 5 Loss 0.1939\n",
      "(64, 21)\n",
      "Epoch 14 Batch 6 Loss 0.1699\n",
      "(64, 21)\n",
      "Epoch 14 Batch 7 Loss 0.2009\n",
      "(64, 21)\n",
      "Epoch 14 Batch 8 Loss 0.1796\n",
      "(64, 21)\n",
      "Epoch 14 Batch 9 Loss 0.2626\n",
      "(64, 21)\n",
      "Epoch 14 Batch 10 Loss 0.1703\n",
      "(64, 21)\n",
      "Epoch 14 Batch 11 Loss 0.2250\n",
      "(64, 21)\n",
      "Epoch 14 Batch 12 Loss 0.1803\n",
      "(64, 21)\n",
      "Epoch 14 Batch 13 Loss 0.1803\n",
      "(64, 21)\n",
      "Epoch 14 Batch 14 Loss 0.1874\n",
      "(64, 21)\n",
      "Epoch 14 Batch 15 Loss 0.2728\n",
      "(64, 21)\n",
      "Epoch 14 Batch 16 Loss 0.2121\n",
      "(64, 21)\n",
      "Epoch 14 Batch 17 Loss 0.1758\n",
      "(64, 21)\n",
      "Epoch 14 Batch 18 Loss 0.1742\n",
      "(64, 21)\n",
      "Epoch 14 Batch 19 Loss 0.1898\n",
      "(64, 21)\n",
      "Epoch 14 Batch 20 Loss 0.1918\n",
      "(64, 21)\n",
      "Epoch 14 Batch 21 Loss 0.2065\n",
      "(64, 21)\n",
      "Epoch 14 Batch 22 Loss 0.2297\n",
      "(64, 21)\n",
      "Epoch 14 Batch 23 Loss 0.1811\n",
      "(64, 21)\n",
      "Epoch 14 Batch 24 Loss 0.2366\n",
      "(64, 21)\n",
      "Epoch 14 Batch 25 Loss 0.2349\n",
      "(64, 21)\n",
      "Epoch 14 Batch 26 Loss 0.2077\n",
      "(64, 21)\n",
      "Epoch 14 Batch 27 Loss 0.2235\n",
      "(64, 21)\n",
      "Epoch 14 Batch 28 Loss 0.2435\n",
      "(64, 21)\n",
      "Epoch 14 Batch 29 Loss 0.2713\n",
      "(64, 21)\n",
      "Epoch 14 Batch 30 Loss 0.2164\n",
      "(64, 21)\n",
      "Epoch 14 Batch 31 Loss 0.2024\n",
      "(64, 21)\n",
      "Epoch 14 Batch 32 Loss 0.1868\n",
      "(64, 21)\n",
      "Epoch 14 Batch 33 Loss 0.2394\n",
      "(64, 21)\n",
      "Epoch 14 Batch 34 Loss 0.2052\n",
      "(64, 21)\n",
      "Epoch 14 Batch 35 Loss 0.2424\n",
      "(64, 21)\n",
      "Epoch 14 Batch 36 Loss 0.2246\n",
      "(64, 21)\n",
      "Epoch 14 Batch 37 Loss 0.1849\n",
      "(64, 21)\n",
      "Epoch 14 Batch 38 Loss 0.1859\n",
      "(64, 21)\n",
      "Epoch 14 Batch 39 Loss 0.2207\n",
      "(64, 21)\n",
      "Epoch 14 Batch 40 Loss 0.1856\n",
      "(64, 21)\n",
      "Epoch 14 Batch 41 Loss 0.2290\n",
      "(64, 21)\n",
      "Epoch 14 Batch 42 Loss 0.2441\n",
      "(64, 21)\n",
      "Epoch 14 Batch 43 Loss 0.2275\n",
      "(64, 21)\n",
      "Epoch 14 Batch 44 Loss 0.2462\n",
      "(64, 21)\n",
      "Epoch 14 Batch 45 Loss 0.2037\n",
      "(64, 21)\n",
      "Epoch 14 Batch 46 Loss 0.2151\n",
      "(64, 21)\n",
      "Epoch 14 Batch 47 Loss 0.2100\n",
      "(64, 21)\n",
      "Epoch 14 Batch 48 Loss 0.2162\n",
      "(64, 21)\n",
      "Epoch 14 Batch 49 Loss 0.2008\n",
      "(64, 21)\n",
      "Epoch 14 Batch 50 Loss 0.2233\n",
      "(64, 21)\n",
      "Epoch 14 Batch 51 Loss 0.2289\n",
      "(64, 21)\n",
      "Epoch 14 Batch 52 Loss 0.2348\n",
      "(64, 21)\n",
      "Epoch 14 Batch 53 Loss 0.1831\n",
      "(64, 21)\n",
      "Epoch 14 Batch 54 Loss 0.2332\n",
      "(64, 21)\n",
      "Epoch 14 Batch 55 Loss 0.2753\n",
      "(64, 21)\n",
      "Epoch 14 Batch 56 Loss 0.2525\n",
      "(64, 21)\n",
      "Epoch 14 Batch 57 Loss 0.2576\n",
      "(64, 21)\n",
      "Epoch 14 Batch 58 Loss 0.2090\n",
      "(64, 21)\n",
      "Epoch 14 Batch 59 Loss 0.1760\n",
      "(64, 21)\n",
      "Epoch 14 Batch 60 Loss 0.2088\n",
      "(64, 21)\n",
      "Epoch 14 Batch 61 Loss 0.2054\n",
      "(64, 21)\n",
      "Epoch 14 Batch 62 Loss 0.2239\n",
      "(64, 21)\n",
      "Epoch 14 Batch 63 Loss 0.2165\n",
      "(64, 21)\n",
      "Epoch 14 Batch 64 Loss 0.2060\n",
      "(64, 21)\n",
      "Epoch 14 Batch 65 Loss 0.2528\n",
      "(64, 21)\n",
      "Epoch 14 Batch 66 Loss 0.2538\n",
      "(64, 21)\n",
      "Epoch 14 Batch 67 Loss 0.2120\n",
      "(64, 21)\n",
      "Epoch 14 Batch 68 Loss 0.1987\n",
      "(64, 21)\n",
      "Epoch 14 Batch 69 Loss 0.2663\n",
      "(64, 21)\n",
      "Epoch 14 Batch 70 Loss 0.2407\n",
      "(64, 21)\n",
      "Epoch 14 Batch 71 Loss 0.2325\n",
      "(64, 21)\n",
      "Epoch 14 Batch 72 Loss 0.2513\n",
      "(64, 21)\n",
      "Epoch 14 Batch 73 Loss 0.2475\n",
      "(64, 21)\n",
      "Epoch 14 Batch 74 Loss 0.2330\n",
      "(64, 21)\n",
      "Epoch 14 Batch 75 Loss 0.2223\n",
      "(64, 21)\n",
      "Epoch 14 Batch 76 Loss 0.2376\n",
      "(64, 21)\n",
      "Epoch 14 Batch 77 Loss 0.2370\n",
      "(64, 21)\n",
      "Epoch 14 Batch 78 Loss 0.2194\n",
      "(64, 21)\n",
      "Epoch 14 Batch 79 Loss 0.2737\n",
      "(64, 21)\n",
      "Epoch 14 Batch 80 Loss 0.2074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:23:56.302880: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14 Loss 0.2169\n",
      "Time taken for 1 epoch 241.41856789588928 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 15 Batch 0 Loss 0.1831\n",
      "(64, 21)\n",
      "Epoch 15 Batch 1 Loss 0.1648\n",
      "(64, 21)\n",
      "Epoch 15 Batch 2 Loss 0.1567\n",
      "(64, 21)\n",
      "Epoch 15 Batch 3 Loss 0.1482\n",
      "(64, 21)\n",
      "Epoch 15 Batch 4 Loss 0.1625\n",
      "(64, 21)\n",
      "Epoch 15 Batch 5 Loss 0.1460\n",
      "(64, 21)\n",
      "Epoch 15 Batch 6 Loss 0.1666\n",
      "(64, 21)\n",
      "Epoch 15 Batch 7 Loss 0.1263\n",
      "(64, 21)\n",
      "Epoch 15 Batch 8 Loss 0.1648\n",
      "(64, 21)\n",
      "Epoch 15 Batch 9 Loss 0.2003\n",
      "(64, 21)\n",
      "Epoch 15 Batch 10 Loss 0.1314\n",
      "(64, 21)\n",
      "Epoch 15 Batch 11 Loss 0.1817\n",
      "(64, 21)\n",
      "Epoch 15 Batch 12 Loss 0.1552\n",
      "(64, 21)\n",
      "Epoch 15 Batch 13 Loss 0.1661\n",
      "(64, 21)\n",
      "Epoch 15 Batch 14 Loss 0.1379\n",
      "(64, 21)\n",
      "Epoch 15 Batch 15 Loss 0.1487\n",
      "(64, 21)\n",
      "Epoch 15 Batch 16 Loss 0.1840\n",
      "(64, 21)\n",
      "Epoch 15 Batch 17 Loss 0.1664\n",
      "(64, 21)\n",
      "Epoch 15 Batch 18 Loss 0.1636\n",
      "(64, 21)\n",
      "Epoch 15 Batch 19 Loss 0.1528\n",
      "(64, 21)\n",
      "Epoch 15 Batch 20 Loss 0.1369\n",
      "(64, 21)\n",
      "Epoch 15 Batch 21 Loss 0.1718\n",
      "(64, 21)\n",
      "Epoch 15 Batch 22 Loss 0.1710\n",
      "(64, 21)\n",
      "Epoch 15 Batch 23 Loss 0.1708\n",
      "(64, 21)\n",
      "Epoch 15 Batch 24 Loss 0.1578\n",
      "(64, 21)\n",
      "Epoch 15 Batch 25 Loss 0.1698\n",
      "(64, 21)\n",
      "Epoch 15 Batch 26 Loss 0.2004\n",
      "(64, 21)\n",
      "Epoch 15 Batch 27 Loss 0.1777\n",
      "(64, 21)\n",
      "Epoch 15 Batch 28 Loss 0.1806\n",
      "(64, 21)\n",
      "Epoch 15 Batch 29 Loss 0.1646\n",
      "(64, 21)\n",
      "Epoch 15 Batch 30 Loss 0.1358\n",
      "(64, 21)\n",
      "Epoch 15 Batch 31 Loss 0.1706\n",
      "(64, 21)\n",
      "Epoch 15 Batch 32 Loss 0.1863\n",
      "(64, 21)\n",
      "Epoch 15 Batch 33 Loss 0.1962\n",
      "(64, 21)\n",
      "Epoch 15 Batch 34 Loss 0.1611\n",
      "(64, 21)\n",
      "Epoch 15 Batch 35 Loss 0.2003\n",
      "(64, 21)\n",
      "Epoch 15 Batch 36 Loss 0.2078\n",
      "(64, 21)\n",
      "Epoch 15 Batch 37 Loss 0.1931\n",
      "(64, 21)\n",
      "Epoch 15 Batch 38 Loss 0.1708\n",
      "(64, 21)\n",
      "Epoch 15 Batch 39 Loss 0.2001\n",
      "(64, 21)\n",
      "Epoch 15 Batch 40 Loss 0.1777\n",
      "(64, 21)\n",
      "Epoch 15 Batch 41 Loss 0.1534\n",
      "(64, 21)\n",
      "Epoch 15 Batch 42 Loss 0.1998\n",
      "(64, 21)\n",
      "Epoch 15 Batch 43 Loss 0.1834\n",
      "(64, 21)\n",
      "Epoch 15 Batch 44 Loss 0.1700\n",
      "(64, 21)\n",
      "Epoch 15 Batch 45 Loss 0.1580\n",
      "(64, 21)\n",
      "Epoch 15 Batch 46 Loss 0.1419\n",
      "(64, 21)\n",
      "Epoch 15 Batch 47 Loss 0.1740\n",
      "(64, 21)\n",
      "Epoch 15 Batch 48 Loss 0.1834\n",
      "(64, 21)\n",
      "Epoch 15 Batch 49 Loss 0.1689\n",
      "(64, 21)\n",
      "Epoch 15 Batch 50 Loss 0.1863\n",
      "(64, 21)\n",
      "Epoch 15 Batch 51 Loss 0.1494\n",
      "(64, 21)\n",
      "Epoch 15 Batch 52 Loss 0.1701\n",
      "(64, 21)\n",
      "Epoch 15 Batch 53 Loss 0.1535\n",
      "(64, 21)\n",
      "Epoch 15 Batch 54 Loss 0.1860\n",
      "(64, 21)\n",
      "Epoch 15 Batch 55 Loss 0.1983\n",
      "(64, 21)\n",
      "Epoch 15 Batch 56 Loss 0.1968\n",
      "(64, 21)\n",
      "Epoch 15 Batch 57 Loss 0.1967\n",
      "(64, 21)\n",
      "Epoch 15 Batch 58 Loss 0.1649\n",
      "(64, 21)\n",
      "Epoch 15 Batch 59 Loss 0.1873\n",
      "(64, 21)\n",
      "Epoch 15 Batch 60 Loss 0.2193\n",
      "(64, 21)\n",
      "Epoch 15 Batch 61 Loss 0.1729\n",
      "(64, 21)\n",
      "Epoch 15 Batch 62 Loss 0.1909\n",
      "(64, 21)\n",
      "Epoch 15 Batch 63 Loss 0.1859\n",
      "(64, 21)\n",
      "Epoch 15 Batch 64 Loss 0.1504\n",
      "(64, 21)\n",
      "Epoch 15 Batch 65 Loss 0.1814\n",
      "(64, 21)\n",
      "Epoch 15 Batch 66 Loss 0.1631\n",
      "(64, 21)\n",
      "Epoch 15 Batch 67 Loss 0.1898\n",
      "(64, 21)\n",
      "Epoch 15 Batch 68 Loss 0.2228\n",
      "(64, 21)\n",
      "Epoch 15 Batch 69 Loss 0.1571\n",
      "(64, 21)\n",
      "Epoch 15 Batch 70 Loss 0.1819\n",
      "(64, 21)\n",
      "Epoch 15 Batch 71 Loss 0.2132\n",
      "(64, 21)\n",
      "Epoch 15 Batch 72 Loss 0.1821\n",
      "(64, 21)\n",
      "Epoch 15 Batch 73 Loss 0.1567\n",
      "(64, 21)\n",
      "Epoch 15 Batch 74 Loss 0.1814\n",
      "(64, 21)\n",
      "Epoch 15 Batch 75 Loss 0.1853\n",
      "(64, 21)\n",
      "Epoch 15 Batch 76 Loss 0.2187\n",
      "(64, 21)\n",
      "Epoch 15 Batch 77 Loss 0.2035\n",
      "(64, 21)\n",
      "Epoch 15 Batch 78 Loss 0.1475\n",
      "(64, 21)\n",
      "Epoch 15 Batch 79 Loss 0.2047\n",
      "(64, 21)\n",
      "Epoch 15 Batch 80 Loss 0.1960\n",
      "Epoch 15 Loss 0.1745\n",
      "Time taken for 1 epoch 253.47143125534058 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:28:10.362398: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 16 Batch 0 Loss 0.1425\n",
      "(64, 21)\n",
      "Epoch 16 Batch 1 Loss 0.1314\n",
      "(64, 21)\n",
      "Epoch 16 Batch 2 Loss 0.1173\n",
      "(64, 21)\n",
      "Epoch 16 Batch 3 Loss 0.1191\n",
      "(64, 21)\n",
      "Epoch 16 Batch 4 Loss 0.1212\n",
      "(64, 21)\n",
      "Epoch 16 Batch 5 Loss 0.1158\n",
      "(64, 21)\n",
      "Epoch 16 Batch 6 Loss 0.1253\n",
      "(64, 21)\n",
      "Epoch 16 Batch 7 Loss 0.0904\n",
      "(64, 21)\n",
      "Epoch 16 Batch 8 Loss 0.1369\n",
      "(64, 21)\n",
      "Epoch 16 Batch 9 Loss 0.1204\n",
      "(64, 21)\n",
      "Epoch 16 Batch 10 Loss 0.1005\n",
      "(64, 21)\n",
      "Epoch 16 Batch 11 Loss 0.1381\n",
      "(64, 21)\n",
      "Epoch 16 Batch 12 Loss 0.1046\n",
      "(64, 21)\n",
      "Epoch 16 Batch 13 Loss 0.1200\n",
      "(64, 21)\n",
      "Epoch 16 Batch 14 Loss 0.1247\n",
      "(64, 21)\n",
      "Epoch 16 Batch 15 Loss 0.1491\n",
      "(64, 21)\n",
      "Epoch 16 Batch 16 Loss 0.1392\n",
      "(64, 21)\n",
      "Epoch 16 Batch 17 Loss 0.1140\n",
      "(64, 21)\n",
      "Epoch 16 Batch 18 Loss 0.0972\n",
      "(64, 21)\n",
      "Epoch 16 Batch 19 Loss 0.1480\n",
      "(64, 21)\n",
      "Epoch 16 Batch 20 Loss 0.1146\n",
      "(64, 21)\n",
      "Epoch 16 Batch 21 Loss 0.1278\n",
      "(64, 21)\n",
      "Epoch 16 Batch 22 Loss 0.1300\n",
      "(64, 21)\n",
      "Epoch 16 Batch 23 Loss 0.1359\n",
      "(64, 21)\n",
      "Epoch 16 Batch 24 Loss 0.1173\n",
      "(64, 21)\n",
      "Epoch 16 Batch 25 Loss 0.1372\n",
      "(64, 21)\n",
      "Epoch 16 Batch 26 Loss 0.1511\n",
      "(64, 21)\n",
      "Epoch 16 Batch 27 Loss 0.1321\n",
      "(64, 21)\n",
      "Epoch 16 Batch 28 Loss 0.1278\n",
      "(64, 21)\n",
      "Epoch 16 Batch 29 Loss 0.1368\n",
      "(64, 21)\n",
      "Epoch 16 Batch 30 Loss 0.1456\n",
      "(64, 21)\n",
      "Epoch 16 Batch 31 Loss 0.1298\n",
      "(64, 21)\n",
      "Epoch 16 Batch 32 Loss 0.1491\n",
      "(64, 21)\n",
      "Epoch 16 Batch 33 Loss 0.1684\n",
      "(64, 21)\n",
      "Epoch 16 Batch 34 Loss 0.1518\n",
      "(64, 21)\n",
      "Epoch 16 Batch 35 Loss 0.1387\n",
      "(64, 21)\n",
      "Epoch 16 Batch 36 Loss 0.1177\n",
      "(64, 21)\n",
      "Epoch 16 Batch 37 Loss 0.1452\n",
      "(64, 21)\n",
      "Epoch 16 Batch 38 Loss 0.1225\n",
      "(64, 21)\n",
      "Epoch 16 Batch 39 Loss 0.1553\n",
      "(64, 21)\n",
      "Epoch 16 Batch 40 Loss 0.1269\n",
      "(64, 21)\n",
      "Epoch 16 Batch 41 Loss 0.1417\n",
      "(64, 21)\n",
      "Epoch 16 Batch 42 Loss 0.1434\n",
      "(64, 21)\n",
      "Epoch 16 Batch 43 Loss 0.1130\n",
      "(64, 21)\n",
      "Epoch 16 Batch 44 Loss 0.1873\n",
      "(64, 21)\n",
      "Epoch 16 Batch 45 Loss 0.1617\n",
      "(64, 21)\n",
      "Epoch 16 Batch 46 Loss 0.1368\n",
      "(64, 21)\n",
      "Epoch 16 Batch 47 Loss 0.1386\n",
      "(64, 21)\n",
      "Epoch 16 Batch 48 Loss 0.1682\n",
      "(64, 21)\n",
      "Epoch 16 Batch 49 Loss 0.1345\n",
      "(64, 21)\n",
      "Epoch 16 Batch 50 Loss 0.1671\n",
      "(64, 21)\n",
      "Epoch 16 Batch 51 Loss 0.1642\n",
      "(64, 21)\n",
      "Epoch 16 Batch 52 Loss 0.1515\n",
      "(64, 21)\n",
      "Epoch 16 Batch 53 Loss 0.1492\n",
      "(64, 21)\n",
      "Epoch 16 Batch 54 Loss 0.1264\n",
      "(64, 21)\n",
      "Epoch 16 Batch 55 Loss 0.1337\n",
      "(64, 21)\n",
      "Epoch 16 Batch 56 Loss 0.1325\n",
      "(64, 21)\n",
      "Epoch 16 Batch 57 Loss 0.1457\n",
      "(64, 21)\n",
      "Epoch 16 Batch 58 Loss 0.1284\n",
      "(64, 21)\n",
      "Epoch 16 Batch 59 Loss 0.1660\n",
      "(64, 21)\n",
      "Epoch 16 Batch 60 Loss 0.1508\n",
      "(64, 21)\n",
      "Epoch 16 Batch 61 Loss 0.1153\n",
      "(64, 21)\n",
      "Epoch 16 Batch 62 Loss 0.1570\n",
      "(64, 21)\n",
      "Epoch 16 Batch 63 Loss 0.1714\n",
      "(64, 21)\n",
      "Epoch 16 Batch 64 Loss 0.1227\n",
      "(64, 21)\n",
      "Epoch 16 Batch 65 Loss 0.1896\n",
      "(64, 21)\n",
      "Epoch 16 Batch 66 Loss 0.1526\n",
      "(64, 21)\n",
      "Epoch 16 Batch 67 Loss 0.1387\n",
      "(64, 21)\n",
      "Epoch 16 Batch 68 Loss 0.2062\n",
      "(64, 21)\n",
      "Epoch 16 Batch 69 Loss 0.1649\n",
      "(64, 21)\n",
      "Epoch 16 Batch 70 Loss 0.1273\n",
      "(64, 21)\n",
      "Epoch 16 Batch 71 Loss 0.1576\n",
      "(64, 21)\n",
      "Epoch 16 Batch 72 Loss 0.1162\n",
      "(64, 21)\n",
      "Epoch 16 Batch 73 Loss 0.1500\n",
      "(64, 21)\n",
      "Epoch 16 Batch 74 Loss 0.1661\n",
      "(64, 21)\n",
      "Epoch 16 Batch 75 Loss 0.1356\n",
      "(64, 21)\n",
      "Epoch 16 Batch 76 Loss 0.1692\n",
      "(64, 21)\n",
      "Epoch 16 Batch 77 Loss 0.1942\n",
      "(64, 21)\n",
      "Epoch 16 Batch 78 Loss 0.1833\n",
      "(64, 21)\n",
      "Epoch 16 Batch 79 Loss 0.1581\n",
      "(64, 21)\n",
      "Epoch 16 Batch 80 Loss 0.1483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:32:30.286547: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16 Loss 0.1405\n",
      "Time taken for 1 epoch 260.50762701034546 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 17 Batch 0 Loss 0.0993\n",
      "(64, 21)\n",
      "Epoch 17 Batch 1 Loss 0.1137\n",
      "(64, 21)\n",
      "Epoch 17 Batch 2 Loss 0.0842\n",
      "(64, 21)\n",
      "Epoch 17 Batch 3 Loss 0.1037\n",
      "(64, 21)\n",
      "Epoch 17 Batch 4 Loss 0.1070\n",
      "(64, 21)\n",
      "Epoch 17 Batch 5 Loss 0.1434\n",
      "(64, 21)\n",
      "Epoch 17 Batch 6 Loss 0.0932\n",
      "(64, 21)\n",
      "Epoch 17 Batch 7 Loss 0.1185\n",
      "(64, 21)\n",
      "Epoch 17 Batch 8 Loss 0.1018\n",
      "(64, 21)\n",
      "Epoch 17 Batch 9 Loss 0.0761\n",
      "(64, 21)\n",
      "Epoch 17 Batch 10 Loss 0.0998\n",
      "(64, 21)\n",
      "Epoch 17 Batch 11 Loss 0.1255\n",
      "(64, 21)\n",
      "Epoch 17 Batch 12 Loss 0.1341\n",
      "(64, 21)\n",
      "Epoch 17 Batch 13 Loss 0.0993\n",
      "(64, 21)\n",
      "Epoch 17 Batch 14 Loss 0.1155\n",
      "(64, 21)\n",
      "Epoch 17 Batch 15 Loss 0.0769\n",
      "(64, 21)\n",
      "Epoch 17 Batch 16 Loss 0.0818\n",
      "(64, 21)\n",
      "Epoch 17 Batch 17 Loss 0.1159\n",
      "(64, 21)\n",
      "Epoch 17 Batch 18 Loss 0.1155\n",
      "(64, 21)\n",
      "Epoch 17 Batch 19 Loss 0.0915\n",
      "(64, 21)\n",
      "Epoch 17 Batch 20 Loss 0.1295\n",
      "(64, 21)\n",
      "Epoch 17 Batch 21 Loss 0.0754\n",
      "(64, 21)\n",
      "Epoch 17 Batch 22 Loss 0.1133\n",
      "(64, 21)\n",
      "Epoch 17 Batch 23 Loss 0.0947\n",
      "(64, 21)\n",
      "Epoch 17 Batch 24 Loss 0.1039\n",
      "(64, 21)\n",
      "Epoch 17 Batch 25 Loss 0.1002\n",
      "(64, 21)\n",
      "Epoch 17 Batch 26 Loss 0.1125\n",
      "(64, 21)\n",
      "Epoch 17 Batch 27 Loss 0.1155\n",
      "(64, 21)\n",
      "Epoch 17 Batch 28 Loss 0.0939\n",
      "(64, 21)\n",
      "Epoch 17 Batch 29 Loss 0.1061\n",
      "(64, 21)\n",
      "Epoch 17 Batch 30 Loss 0.1278\n",
      "(64, 21)\n",
      "Epoch 17 Batch 31 Loss 0.1063\n",
      "(64, 21)\n",
      "Epoch 17 Batch 32 Loss 0.1001\n",
      "(64, 21)\n",
      "Epoch 17 Batch 33 Loss 0.1239\n",
      "(64, 21)\n",
      "Epoch 17 Batch 34 Loss 0.1121\n",
      "(64, 21)\n",
      "Epoch 17 Batch 35 Loss 0.1567\n",
      "(64, 21)\n",
      "Epoch 17 Batch 36 Loss 0.0761\n",
      "(64, 21)\n",
      "Epoch 17 Batch 37 Loss 0.1522\n",
      "(64, 21)\n",
      "Epoch 17 Batch 38 Loss 0.1051\n",
      "(64, 21)\n",
      "Epoch 17 Batch 39 Loss 0.1275\n",
      "(64, 21)\n",
      "Epoch 17 Batch 40 Loss 0.1082\n",
      "(64, 21)\n",
      "Epoch 17 Batch 41 Loss 0.1174\n",
      "(64, 21)\n",
      "Epoch 17 Batch 42 Loss 0.0918\n",
      "(64, 21)\n",
      "Epoch 17 Batch 43 Loss 0.1361\n",
      "(64, 21)\n",
      "Epoch 17 Batch 44 Loss 0.1094\n",
      "(64, 21)\n",
      "Epoch 17 Batch 45 Loss 0.1156\n",
      "(64, 21)\n",
      "Epoch 17 Batch 46 Loss 0.1040\n",
      "(64, 21)\n",
      "Epoch 17 Batch 47 Loss 0.1255\n",
      "(64, 21)\n",
      "Epoch 17 Batch 48 Loss 0.1197\n",
      "(64, 21)\n",
      "Epoch 17 Batch 49 Loss 0.1343\n",
      "(64, 21)\n",
      "Epoch 17 Batch 50 Loss 0.1504\n",
      "(64, 21)\n",
      "Epoch 17 Batch 51 Loss 0.1271\n",
      "(64, 21)\n",
      "Epoch 17 Batch 52 Loss 0.1313\n",
      "(64, 21)\n",
      "Epoch 17 Batch 53 Loss 0.1255\n",
      "(64, 21)\n",
      "Epoch 17 Batch 54 Loss 0.1248\n",
      "(64, 21)\n",
      "Epoch 17 Batch 55 Loss 0.1194\n",
      "(64, 21)\n",
      "Epoch 17 Batch 56 Loss 0.1092\n",
      "(64, 21)\n",
      "Epoch 17 Batch 57 Loss 0.1243\n",
      "(64, 21)\n",
      "Epoch 17 Batch 58 Loss 0.1139\n",
      "(64, 21)\n",
      "Epoch 17 Batch 59 Loss 0.1229\n",
      "(64, 21)\n",
      "Epoch 17 Batch 60 Loss 0.1195\n",
      "(64, 21)\n",
      "Epoch 17 Batch 61 Loss 0.1146\n",
      "(64, 21)\n",
      "Epoch 17 Batch 62 Loss 0.1362\n",
      "(64, 21)\n",
      "Epoch 17 Batch 63 Loss 0.1304\n",
      "(64, 21)\n",
      "Epoch 17 Batch 64 Loss 0.1321\n",
      "(64, 21)\n",
      "Epoch 17 Batch 65 Loss 0.1373\n",
      "(64, 21)\n",
      "Epoch 17 Batch 66 Loss 0.1089\n",
      "(64, 21)\n",
      "Epoch 17 Batch 67 Loss 0.1136\n",
      "(64, 21)\n",
      "Epoch 17 Batch 68 Loss 0.0991\n",
      "(64, 21)\n",
      "Epoch 17 Batch 69 Loss 0.1218\n",
      "(64, 21)\n",
      "Epoch 17 Batch 70 Loss 0.1244\n",
      "(64, 21)\n",
      "Epoch 17 Batch 71 Loss 0.1288\n",
      "(64, 21)\n",
      "Epoch 17 Batch 72 Loss 0.1339\n",
      "(64, 21)\n",
      "Epoch 17 Batch 73 Loss 0.1638\n",
      "(64, 21)\n",
      "Epoch 17 Batch 74 Loss 0.1478\n",
      "(64, 21)\n",
      "Epoch 17 Batch 75 Loss 0.1423\n",
      "(64, 21)\n",
      "Epoch 17 Batch 76 Loss 0.1526\n",
      "(64, 21)\n",
      "Epoch 17 Batch 77 Loss 0.1141\n",
      "(64, 21)\n",
      "Epoch 17 Batch 78 Loss 0.1100\n",
      "(64, 21)\n",
      "Epoch 17 Batch 79 Loss 0.1046\n",
      "(64, 21)\n",
      "Epoch 17 Batch 80 Loss 0.1318\n",
      "Epoch 17 Loss 0.1161\n",
      "Time taken for 1 epoch 267.47231793403625 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:36:58.342211: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 18 Batch 0 Loss 0.0844\n",
      "(64, 21)\n",
      "Epoch 18 Batch 1 Loss 0.0839\n",
      "(64, 21)\n",
      "Epoch 18 Batch 2 Loss 0.0674\n",
      "(64, 21)\n",
      "Epoch 18 Batch 3 Loss 0.0989\n",
      "(64, 21)\n",
      "Epoch 18 Batch 4 Loss 0.0793\n",
      "(64, 21)\n",
      "Epoch 18 Batch 5 Loss 0.0819\n",
      "(64, 21)\n",
      "Epoch 18 Batch 6 Loss 0.0894\n",
      "(64, 21)\n",
      "Epoch 18 Batch 7 Loss 0.0779\n",
      "(64, 21)\n",
      "Epoch 18 Batch 8 Loss 0.0996\n",
      "(64, 21)\n",
      "Epoch 18 Batch 9 Loss 0.0827\n",
      "(64, 21)\n",
      "Epoch 18 Batch 10 Loss 0.0903\n",
      "(64, 21)\n",
      "Epoch 18 Batch 11 Loss 0.1005\n",
      "(64, 21)\n",
      "Epoch 18 Batch 12 Loss 0.0810\n",
      "(64, 21)\n",
      "Epoch 18 Batch 13 Loss 0.0908\n",
      "(64, 21)\n",
      "Epoch 18 Batch 14 Loss 0.1120\n",
      "(64, 21)\n",
      "Epoch 18 Batch 15 Loss 0.1041\n",
      "(64, 21)\n",
      "Epoch 18 Batch 16 Loss 0.0870\n",
      "(64, 21)\n",
      "Epoch 18 Batch 17 Loss 0.0990\n",
      "(64, 21)\n",
      "Epoch 18 Batch 18 Loss 0.0908\n",
      "(64, 21)\n",
      "Epoch 18 Batch 19 Loss 0.0849\n",
      "(64, 21)\n",
      "Epoch 18 Batch 20 Loss 0.0782\n",
      "(64, 21)\n",
      "Epoch 18 Batch 21 Loss 0.0787\n",
      "(64, 21)\n",
      "Epoch 18 Batch 22 Loss 0.0895\n",
      "(64, 21)\n",
      "Epoch 18 Batch 23 Loss 0.0752\n",
      "(64, 21)\n",
      "Epoch 18 Batch 24 Loss 0.1031\n",
      "(64, 21)\n",
      "Epoch 18 Batch 25 Loss 0.1109\n",
      "(64, 21)\n",
      "Epoch 18 Batch 26 Loss 0.0846\n",
      "(64, 21)\n",
      "Epoch 18 Batch 27 Loss 0.1037\n",
      "(64, 21)\n",
      "Epoch 18 Batch 28 Loss 0.1028\n",
      "(64, 21)\n",
      "Epoch 18 Batch 29 Loss 0.1034\n",
      "(64, 21)\n",
      "Epoch 18 Batch 30 Loss 0.0928\n",
      "(64, 21)\n",
      "Epoch 18 Batch 31 Loss 0.1011\n",
      "(64, 21)\n",
      "Epoch 18 Batch 32 Loss 0.0999\n",
      "(64, 21)\n",
      "Epoch 18 Batch 33 Loss 0.1026\n",
      "(64, 21)\n",
      "Epoch 18 Batch 34 Loss 0.1057\n",
      "(64, 21)\n",
      "Epoch 18 Batch 35 Loss 0.0824\n",
      "(64, 21)\n",
      "Epoch 18 Batch 36 Loss 0.1066\n",
      "(64, 21)\n",
      "Epoch 18 Batch 37 Loss 0.0862\n",
      "(64, 21)\n",
      "Epoch 18 Batch 38 Loss 0.0998\n",
      "(64, 21)\n",
      "Epoch 18 Batch 39 Loss 0.0921\n",
      "(64, 21)\n",
      "Epoch 18 Batch 40 Loss 0.0911\n",
      "(64, 21)\n",
      "Epoch 18 Batch 41 Loss 0.1191\n",
      "(64, 21)\n",
      "Epoch 18 Batch 42 Loss 0.1068\n",
      "(64, 21)\n",
      "Epoch 18 Batch 43 Loss 0.1154\n",
      "(64, 21)\n",
      "Epoch 18 Batch 44 Loss 0.0920\n",
      "(64, 21)\n",
      "Epoch 18 Batch 45 Loss 0.1092\n",
      "(64, 21)\n",
      "Epoch 18 Batch 46 Loss 0.0897\n",
      "(64, 21)\n",
      "Epoch 18 Batch 47 Loss 0.0943\n",
      "(64, 21)\n",
      "Epoch 18 Batch 48 Loss 0.1167\n",
      "(64, 21)\n",
      "Epoch 18 Batch 49 Loss 0.0930\n",
      "(64, 21)\n",
      "Epoch 18 Batch 50 Loss 0.1044\n",
      "(64, 21)\n",
      "Epoch 18 Batch 51 Loss 0.1185\n",
      "(64, 21)\n",
      "Epoch 18 Batch 52 Loss 0.1101\n",
      "(64, 21)\n",
      "Epoch 18 Batch 53 Loss 0.1076\n",
      "(64, 21)\n",
      "Epoch 18 Batch 54 Loss 0.0825\n",
      "(64, 21)\n",
      "Epoch 18 Batch 55 Loss 0.0969\n",
      "(64, 21)\n",
      "Epoch 18 Batch 56 Loss 0.1200\n",
      "(64, 21)\n",
      "Epoch 18 Batch 57 Loss 0.1085\n",
      "(64, 21)\n",
      "Epoch 18 Batch 58 Loss 0.1122\n",
      "(64, 21)\n",
      "Epoch 18 Batch 59 Loss 0.1091\n",
      "(64, 21)\n",
      "Epoch 18 Batch 60 Loss 0.0969\n",
      "(64, 21)\n",
      "Epoch 18 Batch 61 Loss 0.1116\n",
      "(64, 21)\n",
      "Epoch 18 Batch 62 Loss 0.1019\n",
      "(64, 21)\n",
      "Epoch 18 Batch 63 Loss 0.0994\n",
      "(64, 21)\n",
      "Epoch 18 Batch 64 Loss 0.1185\n",
      "(64, 21)\n",
      "Epoch 18 Batch 65 Loss 0.0790\n",
      "(64, 21)\n",
      "Epoch 18 Batch 66 Loss 0.1027\n",
      "(64, 21)\n",
      "Epoch 18 Batch 67 Loss 0.1065\n",
      "(64, 21)\n",
      "Epoch 18 Batch 68 Loss 0.1488\n",
      "(64, 21)\n",
      "Epoch 18 Batch 69 Loss 0.1176\n",
      "(64, 21)\n",
      "Epoch 18 Batch 70 Loss 0.1100\n",
      "(64, 21)\n",
      "Epoch 18 Batch 71 Loss 0.0855\n",
      "(64, 21)\n",
      "Epoch 18 Batch 72 Loss 0.1015\n",
      "(64, 21)\n",
      "Epoch 18 Batch 73 Loss 0.1102\n",
      "(64, 21)\n",
      "Epoch 18 Batch 74 Loss 0.0856\n",
      "(64, 21)\n",
      "Epoch 18 Batch 75 Loss 0.0959\n",
      "(64, 21)\n",
      "Epoch 18 Batch 76 Loss 0.1030\n",
      "(64, 21)\n",
      "Epoch 18 Batch 77 Loss 0.1085\n",
      "(64, 21)\n",
      "Epoch 18 Batch 78 Loss 0.1051\n",
      "(64, 21)\n",
      "Epoch 18 Batch 79 Loss 0.0889\n",
      "(64, 21)\n",
      "Epoch 18 Batch 80 Loss 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:41:15.126861: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18 Loss 0.0981\n",
      "Time taken for 1 epoch 257.386803150177 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 19 Batch 0 Loss 0.0740\n",
      "(64, 21)\n",
      "Epoch 19 Batch 1 Loss 0.0654\n",
      "(64, 21)\n",
      "Epoch 19 Batch 2 Loss 0.0900\n",
      "(64, 21)\n",
      "Epoch 19 Batch 3 Loss 0.0838\n",
      "(64, 21)\n",
      "Epoch 19 Batch 4 Loss 0.0916\n",
      "(64, 21)\n",
      "Epoch 19 Batch 5 Loss 0.0408\n",
      "(64, 21)\n",
      "Epoch 19 Batch 6 Loss 0.0873\n",
      "(64, 21)\n",
      "Epoch 19 Batch 7 Loss 0.0522\n",
      "(64, 21)\n",
      "Epoch 19 Batch 8 Loss 0.0815\n",
      "(64, 21)\n",
      "Epoch 19 Batch 9 Loss 0.0944\n",
      "(64, 21)\n",
      "Epoch 19 Batch 10 Loss 0.0887\n",
      "(64, 21)\n",
      "Epoch 19 Batch 11 Loss 0.1147\n",
      "(64, 21)\n",
      "Epoch 19 Batch 12 Loss 0.0841\n",
      "(64, 21)\n",
      "Epoch 19 Batch 13 Loss 0.0589\n",
      "(64, 21)\n",
      "Epoch 19 Batch 14 Loss 0.0986\n",
      "(64, 21)\n",
      "Epoch 19 Batch 15 Loss 0.0802\n",
      "(64, 21)\n",
      "Epoch 19 Batch 16 Loss 0.0908\n",
      "(64, 21)\n",
      "Epoch 19 Batch 17 Loss 0.0715\n",
      "(64, 21)\n",
      "Epoch 19 Batch 18 Loss 0.0883\n",
      "(64, 21)\n",
      "Epoch 19 Batch 19 Loss 0.0604\n",
      "(64, 21)\n",
      "Epoch 19 Batch 20 Loss 0.0746\n",
      "(64, 21)\n",
      "Epoch 19 Batch 21 Loss 0.0947\n",
      "(64, 21)\n",
      "Epoch 19 Batch 22 Loss 0.0680\n",
      "(64, 21)\n",
      "Epoch 19 Batch 23 Loss 0.0807\n",
      "(64, 21)\n",
      "Epoch 19 Batch 24 Loss 0.0894\n",
      "(64, 21)\n",
      "Epoch 19 Batch 25 Loss 0.0777\n",
      "(64, 21)\n",
      "Epoch 19 Batch 26 Loss 0.1057\n",
      "(64, 21)\n",
      "Epoch 19 Batch 27 Loss 0.0797\n",
      "(64, 21)\n",
      "Epoch 19 Batch 28 Loss 0.0755\n",
      "(64, 21)\n",
      "Epoch 19 Batch 29 Loss 0.0788\n",
      "(64, 21)\n",
      "Epoch 19 Batch 30 Loss 0.0724\n",
      "(64, 21)\n",
      "Epoch 19 Batch 31 Loss 0.0858\n",
      "(64, 21)\n",
      "Epoch 19 Batch 32 Loss 0.0767\n",
      "(64, 21)\n",
      "Epoch 19 Batch 33 Loss 0.0923\n",
      "(64, 21)\n",
      "Epoch 19 Batch 34 Loss 0.0754\n",
      "(64, 21)\n",
      "Epoch 19 Batch 35 Loss 0.0972\n",
      "(64, 21)\n",
      "Epoch 19 Batch 36 Loss 0.0948\n",
      "(64, 21)\n",
      "Epoch 19 Batch 37 Loss 0.0731\n",
      "(64, 21)\n",
      "Epoch 19 Batch 38 Loss 0.0836\n",
      "(64, 21)\n",
      "Epoch 19 Batch 39 Loss 0.0898\n",
      "(64, 21)\n",
      "Epoch 19 Batch 40 Loss 0.0832\n",
      "(64, 21)\n",
      "Epoch 19 Batch 41 Loss 0.0943\n",
      "(64, 21)\n",
      "Epoch 19 Batch 42 Loss 0.0746\n",
      "(64, 21)\n",
      "Epoch 19 Batch 43 Loss 0.0823\n",
      "(64, 21)\n",
      "Epoch 19 Batch 44 Loss 0.0837\n",
      "(64, 21)\n",
      "Epoch 19 Batch 45 Loss 0.0741\n",
      "(64, 21)\n",
      "Epoch 19 Batch 46 Loss 0.0859\n",
      "(64, 21)\n",
      "Epoch 19 Batch 47 Loss 0.0933\n",
      "(64, 21)\n",
      "Epoch 19 Batch 48 Loss 0.0902\n",
      "(64, 21)\n",
      "Epoch 19 Batch 49 Loss 0.0637\n",
      "(64, 21)\n",
      "Epoch 19 Batch 50 Loss 0.0845\n",
      "(64, 21)\n",
      "Epoch 19 Batch 51 Loss 0.0845\n",
      "(64, 21)\n",
      "Epoch 19 Batch 52 Loss 0.1011\n",
      "(64, 21)\n",
      "Epoch 19 Batch 53 Loss 0.0933\n",
      "(64, 21)\n",
      "Epoch 19 Batch 54 Loss 0.0847\n",
      "(64, 21)\n",
      "Epoch 19 Batch 55 Loss 0.0869\n",
      "(64, 21)\n",
      "Epoch 19 Batch 56 Loss 0.1137\n",
      "(64, 21)\n",
      "Epoch 19 Batch 57 Loss 0.1064\n",
      "(64, 21)\n",
      "Epoch 19 Batch 58 Loss 0.1026\n",
      "(64, 21)\n",
      "Epoch 19 Batch 59 Loss 0.1099\n",
      "(64, 21)\n",
      "Epoch 19 Batch 60 Loss 0.0902\n",
      "(64, 21)\n",
      "Epoch 19 Batch 61 Loss 0.0833\n",
      "(64, 21)\n",
      "Epoch 19 Batch 62 Loss 0.0691\n",
      "(64, 21)\n",
      "Epoch 19 Batch 63 Loss 0.0949\n",
      "(64, 21)\n",
      "Epoch 19 Batch 64 Loss 0.0891\n",
      "(64, 21)\n",
      "Epoch 19 Batch 65 Loss 0.1101\n",
      "(64, 21)\n",
      "Epoch 19 Batch 66 Loss 0.0978\n",
      "(64, 21)\n",
      "Epoch 19 Batch 67 Loss 0.1156\n",
      "(64, 21)\n",
      "Epoch 19 Batch 68 Loss 0.0764\n",
      "(64, 21)\n",
      "Epoch 19 Batch 69 Loss 0.0720\n",
      "(64, 21)\n",
      "Epoch 19 Batch 70 Loss 0.1217\n",
      "(64, 21)\n",
      "Epoch 19 Batch 71 Loss 0.0890\n",
      "(64, 21)\n",
      "Epoch 19 Batch 72 Loss 0.0994\n",
      "(64, 21)\n",
      "Epoch 19 Batch 73 Loss 0.0928\n",
      "(64, 21)\n",
      "Epoch 19 Batch 74 Loss 0.1013\n",
      "(64, 21)\n",
      "Epoch 19 Batch 75 Loss 0.1072\n",
      "(64, 21)\n",
      "Epoch 19 Batch 76 Loss 0.1205\n",
      "(64, 21)\n",
      "Epoch 19 Batch 77 Loss 0.1040\n",
      "(64, 21)\n",
      "Epoch 19 Batch 78 Loss 0.0988\n",
      "(64, 21)\n",
      "Epoch 19 Batch 79 Loss 0.1173\n",
      "(64, 21)\n",
      "Epoch 19 Batch 80 Loss 0.0826\n",
      "Epoch 19 Loss 0.0875\n",
      "Time taken for 1 epoch 245.49917101860046 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:45:21.228318: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 20 Batch 0 Loss 0.0477\n",
      "(64, 21)\n",
      "Epoch 20 Batch 1 Loss 0.0525\n",
      "(64, 21)\n",
      "Epoch 20 Batch 2 Loss 0.0586\n",
      "(64, 21)\n",
      "Epoch 20 Batch 3 Loss 0.0577\n",
      "(64, 21)\n",
      "Epoch 20 Batch 4 Loss 0.0948\n",
      "(64, 21)\n",
      "Epoch 20 Batch 5 Loss 0.0799\n",
      "(64, 21)\n",
      "Epoch 20 Batch 6 Loss 0.0617\n",
      "(64, 21)\n",
      "Epoch 20 Batch 7 Loss 0.0527\n",
      "(64, 21)\n",
      "Epoch 20 Batch 8 Loss 0.0827\n",
      "(64, 21)\n",
      "Epoch 20 Batch 9 Loss 0.0781\n",
      "(64, 21)\n",
      "Epoch 20 Batch 10 Loss 0.0589\n",
      "(64, 21)\n",
      "Epoch 20 Batch 11 Loss 0.0978\n",
      "(64, 21)\n",
      "Epoch 20 Batch 12 Loss 0.0483\n",
      "(64, 21)\n",
      "Epoch 20 Batch 13 Loss 0.0684\n",
      "(64, 21)\n",
      "Epoch 20 Batch 14 Loss 0.0900\n",
      "(64, 21)\n",
      "Epoch 20 Batch 15 Loss 0.0760\n",
      "(64, 21)\n",
      "Epoch 20 Batch 16 Loss 0.0903\n",
      "(64, 21)\n",
      "Epoch 20 Batch 17 Loss 0.0760\n",
      "(64, 21)\n",
      "Epoch 20 Batch 18 Loss 0.0811\n",
      "(64, 21)\n",
      "Epoch 20 Batch 19 Loss 0.0745\n",
      "(64, 21)\n",
      "Epoch 20 Batch 20 Loss 0.0706\n",
      "(64, 21)\n",
      "Epoch 20 Batch 21 Loss 0.0753\n",
      "(64, 21)\n",
      "Epoch 20 Batch 22 Loss 0.0729\n",
      "(64, 21)\n",
      "Epoch 20 Batch 23 Loss 0.0947\n",
      "(64, 21)\n",
      "Epoch 20 Batch 24 Loss 0.0853\n",
      "(64, 21)\n",
      "Epoch 20 Batch 25 Loss 0.0853\n",
      "(64, 21)\n",
      "Epoch 20 Batch 26 Loss 0.0695\n",
      "(64, 21)\n",
      "Epoch 20 Batch 27 Loss 0.0653\n",
      "(64, 21)\n",
      "Epoch 20 Batch 28 Loss 0.0667\n",
      "(64, 21)\n",
      "Epoch 20 Batch 29 Loss 0.0647\n",
      "(64, 21)\n",
      "Epoch 20 Batch 30 Loss 0.0637\n",
      "(64, 21)\n",
      "Epoch 20 Batch 31 Loss 0.0685\n",
      "(64, 21)\n",
      "Epoch 20 Batch 32 Loss 0.1036\n",
      "(64, 21)\n",
      "Epoch 20 Batch 33 Loss 0.0828\n",
      "(64, 21)\n",
      "Epoch 20 Batch 34 Loss 0.0743\n",
      "(64, 21)\n",
      "Epoch 20 Batch 35 Loss 0.0630\n",
      "(64, 21)\n",
      "Epoch 20 Batch 36 Loss 0.1044\n",
      "(64, 21)\n",
      "Epoch 20 Batch 37 Loss 0.0797\n",
      "(64, 21)\n",
      "Epoch 20 Batch 38 Loss 0.0858\n",
      "(64, 21)\n",
      "Epoch 20 Batch 39 Loss 0.0881\n",
      "(64, 21)\n",
      "Epoch 20 Batch 40 Loss 0.0693\n",
      "(64, 21)\n",
      "Epoch 20 Batch 41 Loss 0.0776\n",
      "(64, 21)\n",
      "Epoch 20 Batch 42 Loss 0.0835\n",
      "(64, 21)\n",
      "Epoch 20 Batch 43 Loss 0.0851\n",
      "(64, 21)\n",
      "Epoch 20 Batch 44 Loss 0.0700\n",
      "(64, 21)\n",
      "Epoch 20 Batch 45 Loss 0.0886\n",
      "(64, 21)\n",
      "Epoch 20 Batch 46 Loss 0.0625\n",
      "(64, 21)\n",
      "Epoch 20 Batch 47 Loss 0.0868\n",
      "(64, 21)\n",
      "Epoch 20 Batch 48 Loss 0.0752\n",
      "(64, 21)\n",
      "Epoch 20 Batch 49 Loss 0.0793\n",
      "(64, 21)\n",
      "Epoch 20 Batch 50 Loss 0.0979\n",
      "(64, 21)\n",
      "Epoch 20 Batch 51 Loss 0.0720\n",
      "(64, 21)\n",
      "Epoch 20 Batch 52 Loss 0.0873\n",
      "(64, 21)\n",
      "Epoch 20 Batch 53 Loss 0.0915\n",
      "(64, 21)\n",
      "Epoch 20 Batch 54 Loss 0.0864\n",
      "(64, 21)\n",
      "Epoch 20 Batch 55 Loss 0.1076\n",
      "(64, 21)\n",
      "Epoch 20 Batch 56 Loss 0.0823\n",
      "(64, 21)\n",
      "Epoch 20 Batch 57 Loss 0.0762\n",
      "(64, 21)\n",
      "Epoch 20 Batch 58 Loss 0.0940\n",
      "(64, 21)\n",
      "Epoch 20 Batch 59 Loss 0.0727\n",
      "(64, 21)\n",
      "Epoch 20 Batch 60 Loss 0.0794\n",
      "(64, 21)\n",
      "Epoch 20 Batch 61 Loss 0.0757\n",
      "(64, 21)\n",
      "Epoch 20 Batch 62 Loss 0.1002\n",
      "(64, 21)\n",
      "Epoch 20 Batch 63 Loss 0.0986\n",
      "(64, 21)\n",
      "Epoch 20 Batch 64 Loss 0.1008\n",
      "(64, 21)\n",
      "Epoch 20 Batch 65 Loss 0.0828\n",
      "(64, 21)\n",
      "Epoch 20 Batch 66 Loss 0.0531\n",
      "(64, 21)\n",
      "Epoch 20 Batch 67 Loss 0.0842\n",
      "(64, 21)\n",
      "Epoch 20 Batch 68 Loss 0.0884\n",
      "(64, 21)\n",
      "Epoch 20 Batch 69 Loss 0.0951\n",
      "(64, 21)\n",
      "Epoch 20 Batch 70 Loss 0.0891\n",
      "(64, 21)\n",
      "Epoch 20 Batch 71 Loss 0.1012\n",
      "(64, 21)\n",
      "Epoch 20 Batch 72 Loss 0.0638\n",
      "(64, 21)\n",
      "Epoch 20 Batch 73 Loss 0.0734\n",
      "(64, 21)\n",
      "Epoch 20 Batch 74 Loss 0.0914\n",
      "(64, 21)\n",
      "Epoch 20 Batch 75 Loss 0.0758\n",
      "(64, 21)\n",
      "Epoch 20 Batch 76 Loss 0.0804\n",
      "(64, 21)\n",
      "Epoch 20 Batch 77 Loss 0.0835\n",
      "(64, 21)\n",
      "Epoch 20 Batch 78 Loss 0.1109\n",
      "(64, 21)\n",
      "Epoch 20 Batch 79 Loss 0.0825\n",
      "(64, 21)\n",
      "Epoch 20 Batch 80 Loss 0.1096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 16:49:23.290364: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 Loss 0.0797\n",
      "Time taken for 1 epoch 242.66775918006897 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=20\n",
    "import time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "        print(targ.shape)\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix+str(epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt-11'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> let's go home .  <end>\n",
      "[[1, 179, 33, 135, 3, 2]]\n",
      "(64, 1)\n",
      "(64, 1)\n",
      "(64, 1)\n",
      "(64, 1)\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = 20\n",
    "#Now we will generate the translation of the input sentence\n",
    "\n",
    "def evaluate(sentence):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    print(sentence)\n",
    "    sentence = inp_lang.texts_to_sequences([sentence])\n",
    "    print(sentence)\n",
    "    len_sent=len(sentence[0])\n",
    "    # Pad the sentence to the max_length_inp\n",
    "\n",
    "    sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence,\n",
    "                                                            maxlen=max_length_inp,\n",
    "                                                            padding='post')\n",
    "\n",
    "    hidden_initialize = encoder.initialize_hidden_state()\n",
    "    enc_output, enc_hidden = encoder(sentence, hidden_initialize)\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
    "\n",
    "    result = []\n",
    "    #Disable teacher forcing\n",
    "    attention_graph=[]\n",
    "    for i in range(max_seq_length):\n",
    "        predictions, dec_hidden, attention_weights = decoder(enc_output, dec_input)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "        attention_graph.append(attention_weights)\n",
    "        result.append(targ_lang.index_word[predicted_id])\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            returnd=(attention_graph,result,len_sent)\n",
    "            return returnd\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id] * BATCH_SIZE, 0)\n",
    "        dec_input = tf.transpose(dec_input)\n",
    "        print(dec_input.shape)\n",
    "    \n",
    "    returnd=(attention_graph,result,len_sent)\n",
    "    return returnd\n",
    "\n",
    "\n",
    "resultas = evaluate(\"Let's go home.\")\n",
    "\n",
    "attend, result, len_inp = resultas[0], resultas[1], resultas[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 23, 1)\n"
     ]
    }
   ],
   "source": [
    "print(attend[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_attend=[]\n",
    "for i in range(len(attend)):\n",
    "    new_attend.append(attend[i][0][:len_inp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "print(len_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(len(new_attend))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 1)\n"
     ]
    }
   ],
   "source": [
    "print(new_attend[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_attend=np.array(new_attend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0.02573219]\n",
      "  [0.02547985]\n",
      "  [0.02335659]\n",
      "  [0.02211129]\n",
      "  [0.0225455 ]\n",
      "  [0.0268532 ]]\n",
      "\n",
      " [[0.02573219]\n",
      "  [0.02547985]\n",
      "  [0.02335659]\n",
      "  [0.02211129]\n",
      "  [0.0225455 ]\n",
      "  [0.0268532 ]]\n",
      "\n",
      " [[0.02573219]\n",
      "  [0.02547985]\n",
      "  [0.02335659]\n",
      "  [0.02211129]\n",
      "  [0.0225455 ]\n",
      "  [0.0268532 ]]\n",
      "\n",
      " [[0.02573219]\n",
      "  [0.02547985]\n",
      "  [0.02335659]\n",
      "  [0.02211129]\n",
      "  [0.0225455 ]\n",
      "  [0.0268532 ]]\n",
      "\n",
      " [[0.02573219]\n",
      "  [0.02547985]\n",
      "  [0.02335659]\n",
      "  [0.02211129]\n",
      "  [0.0225455 ]\n",
      "  [0.0268532 ]]]\n"
     ]
    }
   ],
   "source": [
    "print(new_attend)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picaso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
