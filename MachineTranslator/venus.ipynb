{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "\n",
    "import re\n",
    "\n",
    "\n",
    "# Convert the unicode sequence to ascii\n",
    "def unicode_to_ascii(s):\n",
    "\n",
    "  # Normalize the unicode string and remove the non-spacking mark\n",
    "  return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
    "      if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "# Preprocess the sequence\n",
    "def preprocess_sentence(w):\n",
    "\n",
    "  # Clean the sequence\n",
    "  w = unicode_to_ascii(w.lower().strip())\n",
    "  \n",
    "  # Create a space between a word and the punctuation following it also place a space between the punctuation and the following word. Note that punctuation also includes | \n",
    "\n",
    "  w = re.sub(r\"([?.!।])\", r\" \\1 \", w)\n",
    "  \n",
    "  # Add a start and stop token to detect the start and end of the sequence\n",
    "  w = '<start> ' + w + ' <end>'\n",
    "  return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "# Create the Dataset\n",
    "def create_dataset(path):\n",
    "  lines = io.open(path, encoding='UTF-8').read().strip().split('\\n')\n",
    "\n",
    "  # Loop through lines (sequences) and extract the English and French sequences. Store them as a word-pair\n",
    "  word_pairs = [[preprocess_sentence(w) for w in l.split('\\t', 2)[:-1]]  for l in lines]\n",
    "  return zip(*word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_file='ben.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go.\tযাও।\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #5545004 (tanay)\n",
      "<start> go .  <end>\n",
      "<start> যাও ।  <end>\n"
     ]
    }
   ],
   "source": [
    "lines = io.open(path_to_file, encoding='UTF-8').read().strip().split('\\n')\n",
    "print(lines[0])\n",
    "print(preprocess_sentence(lines[0].split('\\t', 2)[0]))\n",
    "print(preprocess_sentence(lines[0].split('\\t', 2)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<start> go .  <end>\n",
      "<start> যাও ।  <end>\n"
     ]
    }
   ],
   "source": [
    "en, fra = create_dataset(path_to_file)\n",
    "print(en[0])\n",
    "print(fra[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 12:38:23.039117: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Convert sequences to tokenizers\n",
    "def tokenize(lang):\n",
    "  lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "      filters='')\n",
    "  \n",
    "  # Convert sequences into internal vocab\n",
    "  lang_tokenizer.fit_on_texts(lang)\n",
    "\n",
    "  # Convert internal vocab to numbers\n",
    "  tensor = lang_tokenizer.texts_to_sequences(lang)\n",
    "\n",
    "  # Pad the tensors to assign equal length to all the sequences\n",
    "  tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor,\n",
    "                                                         padding='post')\n",
    "\n",
    "  return tensor, lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "def load_dataset(path, num_examples=None):\n",
    " \n",
    "  # Create dataset (targ_lan = English, inp_lang = French)\n",
    "  inp_lang,targ_lang = create_dataset(path)\n",
    "\n",
    "  # Tokenize the sequences\n",
    "  input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
    "  target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
    "\n",
    "  return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider 50k examples\n",
    "num_examples = 50000\n",
    "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(path_to_file)\n",
    "\n",
    "# Calculate max_length of the target tensors\n",
    "max_length_targ, max_length_inp = target_tensor.shape[1], input_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21 23\n"
     ]
    }
   ],
   "source": [
    "print(max_length_targ, max_length_inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1 33  3  2  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0  0] (6509, 21)\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor[0], target_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5207 5207 1302 1302\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Create training and validation sets using an 80/20 split\n",
    "input_tensor_train, input_tensor_val, target_tensor_train, target_tensor_val = train_test_split(input_tensor, target_tensor, test_size=0.2)\n",
    "\n",
    "print(len(input_tensor_train), len(target_tensor_train), len(input_tensor_val), len(target_tensor_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   1   22   49 ...    0    0    0]\n",
      " [   1   84   10 ...    0    0    0]\n",
      " [   1    4  190 ...    0    0    0]\n",
      " ...\n",
      " [   1 1450 2625 ...    0    0    0]\n",
      " [   1    7   10 ...    0    0    0]\n",
      " [   1  165 1310 ...    0    0    0]]\n",
      "<keras.src.legacy.preprocessing.text.Tokenizer object at 0x165824890>\n"
     ]
    }
   ],
   "source": [
    "print(input_tensor_train)\n",
    "print(inp_lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Language; index to word mapping\n",
      "1 ----> <start>\n",
      "22 ----> what\n",
      "49 ----> time\n",
      "63 ----> does\n",
      "9 ----> the\n",
      "167 ----> next\n",
      "218 ----> train\n",
      "99 ----> leave\n",
      "38 ----> for\n",
      "894 ----> tokyo\n",
      "5 ----> ?\n",
      "2 ----> <end>\n",
      "\n",
      "Target Language; index to word mapping\n",
      "1 ----> <start>\n",
      "1984 ----> টোকিওর\n",
      "34 ----> জনয\n",
      "258 ----> পরের\n",
      "391 ----> টরেনটা\n",
      "153 ----> কটার\n",
      "43 ----> সময\n",
      "450 ----> ছাডে\n",
      "5 ----> ?\n",
      "2 ----> <end>\n"
     ]
    }
   ],
   "source": [
    "# Show the mapping b/w word index and language tokenizer\n",
    "def convert(lang, tensor):\n",
    "  for t in tensor:\n",
    "    if t != 0:\n",
    "      print (\"%d ----> %s\" % (t, lang.index_word[t]))\n",
    "      \n",
    "print (\"Input Language; index to word mapping\")\n",
    "convert(inp_lang, input_tensor_train[0])\n",
    "print ()\n",
    "print (\"Target Language; index to word mapping\")\n",
    "convert(targ_lang, target_tensor_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(input_tensor_train)\n",
    "BATCH_SIZE = 64\n",
    "steps_per_epoch = len(input_tensor_train)//BATCH_SIZE\n",
    "embedding_dim = 256\n",
    "units = 1024\n",
    "vocab_inp_size = len(inp_lang.word_index) + 1\n",
    "vocab_tar_size = len(targ_lang.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_tensor_slices((input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
    "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([64, 23]), TensorShape([64, 21]))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Size of input and target batches\n",
    "example_input_batch, example_target_batch = next(iter(dataset))\n",
    "example_input_batch.shape, example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units, batch_sz):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.batch_sz = batch_sz\n",
    "        self.enc_units = enc_units\n",
    "\n",
    "        # Embedding layer\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = tf.keras.layers.LSTM(self.enc_units,\n",
    "                                         return_sequences=True,\n",
    "                                         return_state=True,\n",
    "                                         recurrent_initializer='glorot_uniform')\n",
    "\n",
    "    # Encoder network comprises an Embedding layer followed by an LSTM layer\n",
    "    def call(self, x, hidden):\n",
    "        x = self.embedding(x)\n",
    "        output, state_h, state_c = self.lstm(x, initial_state=hidden)\n",
    "        state = [state_h, state_c]\n",
    "        return output, state\n",
    "\n",
    "    # To initialize the hidden state\n",
    "    def initialize_hidden_state(self):\n",
    "        return [tf.zeros((self.batch_sz, self.enc_units)), tf.zeros((self.batch_sz, self.enc_units))]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: (batch size, sequence length, units) (64, 23, 1024)\n",
      "Encoder Hidden state shape: (batch size, units) (64, 1024)\n",
      "Encoder Cell state shape: (batch size, units) (64, 1024)\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "\n",
    "sample_hidden = encoder.initialize_hidden_state()\n",
    "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
    "\n",
    "print ('Encoder output shape: (batch size, sequence length, units) {}'.format(sample_output.shape))\n",
    "print ('Encoder Hidden state shape: (batch size, units) {}'.format(sample_hidden[0].shape))\n",
    "print ('Encoder Cell state shape: (batch size, units) {}'.format(sample_hidden[1].shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "class PayAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units, length):\n",
    "        self.units=units\n",
    "        self.length=length\n",
    "        super(PayAttention, self).__init__() #Call initializer of the superclass\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(self.units,1), initializer='normal')\n",
    "        self.b = self.add_weight(shape=(self.length,1), initializer='zeros')\n",
    "        super(PayAttention, self).build(input_shape)\n",
    "\n",
    "    def call(self, sentences):\n",
    "        E = tf.nn.tanh(tf.keras.backend.dot(sentences,self.w)+self.b)\n",
    "        A = tf.nn.softmax(E, axis=1)\n",
    "        out= A*sentences\n",
    "        return tf.keras.backend.sum(out, axis=1), A\n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units, batch_sz,inp_units, enc_length):\n",
    "        super().__init__()\n",
    "        self.batch_sz=batch_sz\n",
    "        self.dec_units=dec_units\n",
    "        self.embedding_dim=embedding_dim\n",
    "        self.vocab_size=vocab_size \n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim) \n",
    "\n",
    "        self.gru = tf.keras.layers.GRU(self.dec_units,\n",
    "                                   return_sequences=True,\n",
    "                                   return_state=True,\n",
    "                                   recurrent_initializer='glorot_uniform')\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)        \n",
    "\n",
    "    def call(self, context_vector, dec_input):\n",
    "        # hidden is the hidden states of all the units in the encoder\n",
    "        # context_vector is the context vector from the attention layer\n",
    "        # dec_input is the input to the decoder\n",
    "\n",
    "        # Now, first embed the decoder input\n",
    "        x=self.embedding(dec_input)\n",
    "        \n",
    "        # Now we will concat the encoder input and the context vector\n",
    "            #first expand the context vector\n",
    "        context_vector=tf.expand_dims(context_vector,1)\n",
    "        x = tf.concat([context_vector, x], axis=-1)\n",
    "\n",
    "        # Pass through a GRU layer\n",
    "        output,state = self.gru(x)\n",
    "\n",
    "        # Pass through a dense layer to get the probabilities distribution over the target vocabulary\n",
    "        output = tf.reshape(output, (-1, output.shape[2]))\n",
    "        x = self.fc(output)\n",
    "\n",
    "        return x, state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we define the loss function. The loss function is the cross entropy loss function. The cross entropy loss function is defined as follows:\n",
    "#-sum(y_true * log(y_pred), axis=-1)\n",
    "import numpy as np\n",
    "\n",
    "optimizer = tf.optimizers.Adam()\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = 1 - np.equal(real, 0)\n",
    "    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=real, logits=pred) * mask\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Now we run the training loop\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "\n",
    "decoder=DecoderLayer(vocab_tar_size, embedding_dim, units, BATCH_SIZE, units, max_length_inp)\n",
    "encoder=Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
    "attention=PayAttention(units, max_length_inp)\n",
    "\n",
    "import os\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
    "                                 encoder=encoder,\n",
    "                                 decoder=decoder,\n",
    "                                 attention=attention)\n",
    "\n",
    "def train_step(inp, targ):\n",
    "    loss=0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        \n",
    "        # Initialize the hidden state of the encoder and pass the input to the encoder\n",
    "        hidden_initialize = encoder.initialize_hidden_state()\n",
    "        enc_output, enc_hidden = encoder(inp, hidden_initialize)\n",
    "\n",
    "        #Now run the attention layer\n",
    "        context_vector, attention_weights = attention(enc_output)\n",
    "        dec_input = tf.expand_dims([targ_lang.word_index['<start>']] * BATCH_SIZE, 1)   \n",
    "\n",
    "        # Teacher forcing - feeding the target as the next input\n",
    "        for t in range(1, targ.shape[1]):\n",
    "            # passing enc_output to the decoder\n",
    "            predictions, dec_hidden = decoder(context_vector, dec_input)\n",
    "            loss += loss_function(targ[:, t], predictions)\n",
    "            # using teacher forcing\n",
    "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
    "        \n",
    "\n",
    "    batch_loss = (loss / int(targ.shape[1]))\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    return batch_loss\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81\n"
     ]
    }
   ],
   "source": [
    "print(steps_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 1 Batch 0 Loss 0.3545\n",
      "(64, 21)\n",
      "Epoch 1 Batch 1 Loss 0.3561\n",
      "(64, 21)\n",
      "Epoch 1 Batch 2 Loss 0.3595\n",
      "(64, 21)\n",
      "Epoch 1 Batch 3 Loss 0.3273\n",
      "(64, 21)\n",
      "Epoch 1 Batch 4 Loss 0.3805\n",
      "(64, 21)\n",
      "Epoch 1 Batch 5 Loss 0.3417\n",
      "(64, 21)\n",
      "Epoch 1 Batch 6 Loss 0.3837\n",
      "(64, 21)\n",
      "Epoch 1 Batch 7 Loss 0.3203\n",
      "(64, 21)\n",
      "Epoch 1 Batch 8 Loss 0.3395\n",
      "(64, 21)\n",
      "Epoch 1 Batch 9 Loss 0.4266\n",
      "(64, 21)\n",
      "Epoch 1 Batch 10 Loss 0.3914\n",
      "(64, 21)\n",
      "Epoch 1 Batch 11 Loss 0.3714\n",
      "(64, 21)\n",
      "Epoch 1 Batch 12 Loss 0.4053\n",
      "(64, 21)\n",
      "Epoch 1 Batch 13 Loss 0.4689\n",
      "(64, 21)\n",
      "Epoch 1 Batch 14 Loss 0.3660\n",
      "(64, 21)\n",
      "Epoch 1 Batch 15 Loss 0.3301\n",
      "(64, 21)\n",
      "Epoch 1 Batch 16 Loss 0.3784\n",
      "(64, 21)\n",
      "Epoch 1 Batch 17 Loss 0.4065\n",
      "(64, 21)\n",
      "Epoch 1 Batch 18 Loss 0.4588\n",
      "(64, 21)\n",
      "Epoch 1 Batch 19 Loss 0.3710\n",
      "(64, 21)\n",
      "Epoch 1 Batch 20 Loss 0.3208\n",
      "(64, 21)\n",
      "Epoch 1 Batch 21 Loss 0.3602\n",
      "(64, 21)\n",
      "Epoch 1 Batch 22 Loss 0.3885\n",
      "(64, 21)\n",
      "Epoch 1 Batch 23 Loss 0.4281\n",
      "(64, 21)\n",
      "Epoch 1 Batch 24 Loss 0.5082\n",
      "(64, 21)\n",
      "Epoch 1 Batch 25 Loss 0.3652\n",
      "(64, 21)\n",
      "Epoch 1 Batch 26 Loss 0.3123\n",
      "(64, 21)\n",
      "Epoch 1 Batch 27 Loss 0.3822\n",
      "(64, 21)\n",
      "Epoch 1 Batch 28 Loss 0.3782\n",
      "(64, 21)\n",
      "Epoch 1 Batch 29 Loss 0.3333\n",
      "(64, 21)\n",
      "Epoch 1 Batch 30 Loss 0.3687\n",
      "(64, 21)\n",
      "Epoch 1 Batch 31 Loss 0.3551\n",
      "(64, 21)\n",
      "Epoch 1 Batch 32 Loss 0.4162\n",
      "(64, 21)\n",
      "Epoch 1 Batch 33 Loss 0.3762\n",
      "(64, 21)\n",
      "Epoch 1 Batch 34 Loss 0.4277\n",
      "(64, 21)\n",
      "Epoch 1 Batch 35 Loss 0.3052\n",
      "(64, 21)\n",
      "Epoch 1 Batch 36 Loss 0.3547\n",
      "(64, 21)\n",
      "Epoch 1 Batch 37 Loss 0.4572\n",
      "(64, 21)\n",
      "Epoch 1 Batch 38 Loss 0.4699\n",
      "(64, 21)\n",
      "Epoch 1 Batch 39 Loss 0.3841\n",
      "(64, 21)\n",
      "Epoch 1 Batch 40 Loss 0.4118\n",
      "(64, 21)\n",
      "Epoch 1 Batch 41 Loss 0.4283\n",
      "(64, 21)\n",
      "Epoch 1 Batch 42 Loss 0.4135\n",
      "(64, 21)\n",
      "Epoch 1 Batch 43 Loss 0.3715\n",
      "(64, 21)\n",
      "Epoch 1 Batch 44 Loss 0.3727\n",
      "(64, 21)\n",
      "Epoch 1 Batch 45 Loss 0.3784\n",
      "(64, 21)\n",
      "Epoch 1 Batch 46 Loss 0.3770\n",
      "(64, 21)\n",
      "Epoch 1 Batch 47 Loss 0.4102\n",
      "(64, 21)\n",
      "Epoch 1 Batch 48 Loss 0.3675\n",
      "(64, 21)\n",
      "Epoch 1 Batch 49 Loss 0.3830\n",
      "(64, 21)\n",
      "Epoch 1 Batch 50 Loss 0.3823\n",
      "(64, 21)\n",
      "Epoch 1 Batch 51 Loss 0.3393\n",
      "(64, 21)\n",
      "Epoch 1 Batch 52 Loss 0.4123\n",
      "(64, 21)\n",
      "Epoch 1 Batch 53 Loss 0.3959\n",
      "(64, 21)\n",
      "Epoch 1 Batch 54 Loss 0.3956\n",
      "(64, 21)\n",
      "Epoch 1 Batch 55 Loss 0.4571\n",
      "(64, 21)\n",
      "Epoch 1 Batch 56 Loss 0.4497\n",
      "(64, 21)\n",
      "Epoch 1 Batch 57 Loss 0.3329\n",
      "(64, 21)\n",
      "Epoch 1 Batch 58 Loss 0.4921\n",
      "(64, 21)\n",
      "Epoch 1 Batch 59 Loss 0.3660\n",
      "(64, 21)\n",
      "Epoch 1 Batch 60 Loss 0.4231\n",
      "(64, 21)\n",
      "Epoch 1 Batch 61 Loss 0.4422\n",
      "(64, 21)\n",
      "Epoch 1 Batch 62 Loss 0.3608\n",
      "(64, 21)\n",
      "Epoch 1 Batch 63 Loss 0.4515\n",
      "(64, 21)\n",
      "Epoch 1 Batch 64 Loss 0.3748\n",
      "(64, 21)\n",
      "Epoch 1 Batch 65 Loss 0.3930\n",
      "(64, 21)\n",
      "Epoch 1 Batch 66 Loss 0.3641\n",
      "(64, 21)\n",
      "Epoch 1 Batch 67 Loss 0.3381\n",
      "(64, 21)\n",
      "Epoch 1 Batch 68 Loss 0.3570\n",
      "(64, 21)\n",
      "Epoch 1 Batch 69 Loss 0.4010\n",
      "(64, 21)\n",
      "Epoch 1 Batch 70 Loss 0.3994\n",
      "(64, 21)\n",
      "Epoch 1 Batch 71 Loss 0.4087\n",
      "(64, 21)\n",
      "Epoch 1 Batch 72 Loss 0.4422\n",
      "(64, 21)\n",
      "Epoch 1 Batch 73 Loss 0.3904\n",
      "(64, 21)\n",
      "Epoch 1 Batch 74 Loss 0.4507\n",
      "(64, 21)\n",
      "Epoch 1 Batch 75 Loss 0.4190\n",
      "(64, 21)\n",
      "Epoch 1 Batch 76 Loss 0.3715\n",
      "(64, 21)\n",
      "Epoch 1 Batch 77 Loss 0.4110\n",
      "(64, 21)\n",
      "Epoch 1 Batch 78 Loss 0.4177\n",
      "(64, 21)\n",
      "Epoch 1 Batch 79 Loss 0.4315\n",
      "(64, 21)\n",
      "Epoch 1 Batch 80 Loss 0.3647\n",
      "Epoch 1 Loss 0.3899\n",
      "Time taken for 1 epoch 202.97050189971924 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:51:14.726170: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 2 Batch 0 Loss 0.2604\n",
      "(64, 21)\n",
      "Epoch 2 Batch 1 Loss 0.2697\n",
      "(64, 21)\n",
      "Epoch 2 Batch 2 Loss 0.3053\n",
      "(64, 21)\n",
      "Epoch 2 Batch 3 Loss 0.2343\n",
      "(64, 21)\n",
      "Epoch 2 Batch 4 Loss 0.2526\n",
      "(64, 21)\n",
      "Epoch 2 Batch 5 Loss 0.3107\n",
      "(64, 21)\n",
      "Epoch 2 Batch 6 Loss 0.3914\n",
      "(64, 21)\n",
      "Epoch 2 Batch 7 Loss 0.3136\n",
      "(64, 21)\n",
      "Epoch 2 Batch 8 Loss 0.3126\n",
      "(64, 21)\n",
      "Epoch 2 Batch 9 Loss 0.3154\n",
      "(64, 21)\n",
      "Epoch 2 Batch 10 Loss 0.2479\n",
      "(64, 21)\n",
      "Epoch 2 Batch 11 Loss 0.3021\n",
      "(64, 21)\n",
      "Epoch 2 Batch 12 Loss 0.3467\n",
      "(64, 21)\n",
      "Epoch 2 Batch 13 Loss 0.2448\n",
      "(64, 21)\n",
      "Epoch 2 Batch 14 Loss 0.2674\n",
      "(64, 21)\n",
      "Epoch 2 Batch 15 Loss 0.2741\n",
      "(64, 21)\n",
      "Epoch 2 Batch 16 Loss 0.2777\n",
      "(64, 21)\n",
      "Epoch 2 Batch 17 Loss 0.2748\n",
      "(64, 21)\n",
      "Epoch 2 Batch 18 Loss 0.2902\n",
      "(64, 21)\n",
      "Epoch 2 Batch 19 Loss 0.3032\n",
      "(64, 21)\n",
      "Epoch 2 Batch 20 Loss 0.3355\n",
      "(64, 21)\n",
      "Epoch 2 Batch 21 Loss 0.3702\n",
      "(64, 21)\n",
      "Epoch 2 Batch 22 Loss 0.2954\n",
      "(64, 21)\n",
      "Epoch 2 Batch 23 Loss 0.2933\n",
      "(64, 21)\n",
      "Epoch 2 Batch 24 Loss 0.3080\n",
      "(64, 21)\n",
      "Epoch 2 Batch 25 Loss 0.3197\n",
      "(64, 21)\n",
      "Epoch 2 Batch 26 Loss 0.2920\n",
      "(64, 21)\n",
      "Epoch 2 Batch 27 Loss 0.3514\n",
      "(64, 21)\n",
      "Epoch 2 Batch 28 Loss 0.3165\n",
      "(64, 21)\n",
      "Epoch 2 Batch 29 Loss 0.3153\n",
      "(64, 21)\n",
      "Epoch 2 Batch 30 Loss 0.3369\n",
      "(64, 21)\n",
      "Epoch 2 Batch 31 Loss 0.2813\n",
      "(64, 21)\n",
      "Epoch 2 Batch 32 Loss 0.3433\n",
      "(64, 21)\n",
      "Epoch 2 Batch 33 Loss 0.3186\n",
      "(64, 21)\n",
      "Epoch 2 Batch 34 Loss 0.2732\n",
      "(64, 21)\n",
      "Epoch 2 Batch 35 Loss 0.3224\n",
      "(64, 21)\n",
      "Epoch 2 Batch 36 Loss 0.2933\n",
      "(64, 21)\n",
      "Epoch 2 Batch 37 Loss 0.3139\n",
      "(64, 21)\n",
      "Epoch 2 Batch 38 Loss 0.2755\n",
      "(64, 21)\n",
      "Epoch 2 Batch 39 Loss 0.2760\n",
      "(64, 21)\n",
      "Epoch 2 Batch 40 Loss 0.3153\n",
      "(64, 21)\n",
      "Epoch 2 Batch 41 Loss 0.3002\n",
      "(64, 21)\n",
      "Epoch 2 Batch 42 Loss 0.3121\n",
      "(64, 21)\n",
      "Epoch 2 Batch 43 Loss 0.3312\n",
      "(64, 21)\n",
      "Epoch 2 Batch 44 Loss 0.3106\n",
      "(64, 21)\n",
      "Epoch 2 Batch 45 Loss 0.3100\n",
      "(64, 21)\n",
      "Epoch 2 Batch 46 Loss 0.3056\n",
      "(64, 21)\n",
      "Epoch 2 Batch 47 Loss 0.3439\n",
      "(64, 21)\n",
      "Epoch 2 Batch 48 Loss 0.3272\n",
      "(64, 21)\n",
      "Epoch 2 Batch 49 Loss 0.2918\n",
      "(64, 21)\n",
      "Epoch 2 Batch 50 Loss 0.3295\n",
      "(64, 21)\n",
      "Epoch 2 Batch 51 Loss 0.3479\n",
      "(64, 21)\n",
      "Epoch 2 Batch 52 Loss 0.3069\n",
      "(64, 21)\n",
      "Epoch 2 Batch 53 Loss 0.3133\n",
      "(64, 21)\n",
      "Epoch 2 Batch 54 Loss 0.3003\n",
      "(64, 21)\n",
      "Epoch 2 Batch 55 Loss 0.3120\n",
      "(64, 21)\n",
      "Epoch 2 Batch 56 Loss 0.3042\n",
      "(64, 21)\n",
      "Epoch 2 Batch 57 Loss 0.2809\n",
      "(64, 21)\n",
      "Epoch 2 Batch 58 Loss 0.3391\n",
      "(64, 21)\n",
      "Epoch 2 Batch 59 Loss 0.3843\n",
      "(64, 21)\n",
      "Epoch 2 Batch 60 Loss 0.3369\n",
      "(64, 21)\n",
      "Epoch 2 Batch 61 Loss 0.3324\n",
      "(64, 21)\n",
      "Epoch 2 Batch 62 Loss 0.3442\n",
      "(64, 21)\n",
      "Epoch 2 Batch 63 Loss 0.2974\n",
      "(64, 21)\n",
      "Epoch 2 Batch 64 Loss 0.3032\n",
      "(64, 21)\n",
      "Epoch 2 Batch 65 Loss 0.3077\n",
      "(64, 21)\n",
      "Epoch 2 Batch 66 Loss 0.3552\n",
      "(64, 21)\n",
      "Epoch 2 Batch 67 Loss 0.3117\n",
      "(64, 21)\n",
      "Epoch 2 Batch 68 Loss 0.2830\n",
      "(64, 21)\n",
      "Epoch 2 Batch 69 Loss 0.3322\n",
      "(64, 21)\n",
      "Epoch 2 Batch 70 Loss 0.3387\n",
      "(64, 21)\n",
      "Epoch 2 Batch 71 Loss 0.2769\n",
      "(64, 21)\n",
      "Epoch 2 Batch 72 Loss 0.4278\n",
      "(64, 21)\n",
      "Epoch 2 Batch 73 Loss 0.3415\n",
      "(64, 21)\n",
      "Epoch 2 Batch 74 Loss 0.3538\n",
      "(64, 21)\n",
      "Epoch 2 Batch 75 Loss 0.3157\n",
      "(64, 21)\n",
      "Epoch 2 Batch 76 Loss 0.3376\n",
      "(64, 21)\n",
      "Epoch 2 Batch 77 Loss 0.3728\n",
      "(64, 21)\n",
      "Epoch 2 Batch 78 Loss 0.2782\n",
      "(64, 21)\n",
      "Epoch 2 Batch 79 Loss 0.3934\n",
      "(64, 21)\n",
      "Epoch 2 Batch 80 Loss 0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:54:37.368806: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss 0.3138\n",
      "Time taken for 1 epoch 203.19978213310242 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 3 Batch 0 Loss 0.2391\n",
      "(64, 21)\n",
      "Epoch 3 Batch 1 Loss 0.1855\n",
      "(64, 21)\n",
      "Epoch 3 Batch 2 Loss 0.1952\n",
      "(64, 21)\n",
      "Epoch 3 Batch 3 Loss 0.2495\n",
      "(64, 21)\n",
      "Epoch 3 Batch 4 Loss 0.2102\n",
      "(64, 21)\n",
      "Epoch 3 Batch 5 Loss 0.2573\n",
      "(64, 21)\n",
      "Epoch 3 Batch 6 Loss 0.2098\n",
      "(64, 21)\n",
      "Epoch 3 Batch 7 Loss 0.1900\n",
      "(64, 21)\n",
      "Epoch 3 Batch 8 Loss 0.1946\n",
      "(64, 21)\n",
      "Epoch 3 Batch 9 Loss 0.2215\n",
      "(64, 21)\n",
      "Epoch 3 Batch 10 Loss 0.2541\n",
      "(64, 21)\n",
      "Epoch 3 Batch 11 Loss 0.2101\n",
      "(64, 21)\n",
      "Epoch 3 Batch 12 Loss 0.2926\n",
      "(64, 21)\n",
      "Epoch 3 Batch 13 Loss 0.2614\n",
      "(64, 21)\n",
      "Epoch 3 Batch 14 Loss 0.2603\n",
      "(64, 21)\n",
      "Epoch 3 Batch 15 Loss 0.2173\n",
      "(64, 21)\n",
      "Epoch 3 Batch 16 Loss 0.2570\n",
      "(64, 21)\n",
      "Epoch 3 Batch 17 Loss 0.2495\n",
      "(64, 21)\n",
      "Epoch 3 Batch 18 Loss 0.2070\n",
      "(64, 21)\n",
      "Epoch 3 Batch 19 Loss 0.2830\n",
      "(64, 21)\n",
      "Epoch 3 Batch 20 Loss 0.2480\n",
      "(64, 21)\n",
      "Epoch 3 Batch 21 Loss 0.2313\n",
      "(64, 21)\n",
      "Epoch 3 Batch 22 Loss 0.2517\n",
      "(64, 21)\n",
      "Epoch 3 Batch 23 Loss 0.2632\n",
      "(64, 21)\n",
      "Epoch 3 Batch 24 Loss 0.2525\n",
      "(64, 21)\n",
      "Epoch 3 Batch 25 Loss 0.2369\n",
      "(64, 21)\n",
      "Epoch 3 Batch 26 Loss 0.2064\n",
      "(64, 21)\n",
      "Epoch 3 Batch 27 Loss 0.2415\n",
      "(64, 21)\n",
      "Epoch 3 Batch 28 Loss 0.2367\n",
      "(64, 21)\n",
      "Epoch 3 Batch 29 Loss 0.2494\n",
      "(64, 21)\n",
      "Epoch 3 Batch 30 Loss 0.2750\n",
      "(64, 21)\n",
      "Epoch 3 Batch 31 Loss 0.2495\n",
      "(64, 21)\n",
      "Epoch 3 Batch 32 Loss 0.2458\n",
      "(64, 21)\n",
      "Epoch 3 Batch 33 Loss 0.2495\n",
      "(64, 21)\n",
      "Epoch 3 Batch 34 Loss 0.2544\n",
      "(64, 21)\n",
      "Epoch 3 Batch 35 Loss 0.2555\n",
      "(64, 21)\n",
      "Epoch 3 Batch 36 Loss 0.2027\n",
      "(64, 21)\n",
      "Epoch 3 Batch 37 Loss 0.2580\n",
      "(64, 21)\n",
      "Epoch 3 Batch 38 Loss 0.2118\n",
      "(64, 21)\n",
      "Epoch 3 Batch 39 Loss 0.2874\n",
      "(64, 21)\n",
      "Epoch 3 Batch 40 Loss 0.2798\n",
      "(64, 21)\n",
      "Epoch 3 Batch 41 Loss 0.2341\n",
      "(64, 21)\n",
      "Epoch 3 Batch 42 Loss 0.2561\n",
      "(64, 21)\n",
      "Epoch 3 Batch 43 Loss 0.2996\n",
      "(64, 21)\n",
      "Epoch 3 Batch 44 Loss 0.2547\n",
      "(64, 21)\n",
      "Epoch 3 Batch 45 Loss 0.2657\n",
      "(64, 21)\n",
      "Epoch 3 Batch 46 Loss 0.2395\n",
      "(64, 21)\n",
      "Epoch 3 Batch 47 Loss 0.2577\n",
      "(64, 21)\n",
      "Epoch 3 Batch 48 Loss 0.2418\n",
      "(64, 21)\n",
      "Epoch 3 Batch 49 Loss 0.3121\n",
      "(64, 21)\n",
      "Epoch 3 Batch 50 Loss 0.2579\n",
      "(64, 21)\n",
      "Epoch 3 Batch 51 Loss 0.2253\n",
      "(64, 21)\n",
      "Epoch 3 Batch 52 Loss 0.2817\n",
      "(64, 21)\n",
      "Epoch 3 Batch 53 Loss 0.2662\n",
      "(64, 21)\n",
      "Epoch 3 Batch 54 Loss 0.2391\n",
      "(64, 21)\n",
      "Epoch 3 Batch 55 Loss 0.2352\n",
      "(64, 21)\n",
      "Epoch 3 Batch 56 Loss 0.2546\n",
      "(64, 21)\n",
      "Epoch 3 Batch 57 Loss 0.2084\n",
      "(64, 21)\n",
      "Epoch 3 Batch 58 Loss 0.2134\n",
      "(64, 21)\n",
      "Epoch 3 Batch 59 Loss 0.2891\n",
      "(64, 21)\n",
      "Epoch 3 Batch 60 Loss 0.2505\n",
      "(64, 21)\n",
      "Epoch 3 Batch 61 Loss 0.2156\n",
      "(64, 21)\n",
      "Epoch 3 Batch 62 Loss 0.2802\n",
      "(64, 21)\n",
      "Epoch 3 Batch 63 Loss 0.3442\n",
      "(64, 21)\n",
      "Epoch 3 Batch 64 Loss 0.2396\n",
      "(64, 21)\n",
      "Epoch 3 Batch 65 Loss 0.2912\n",
      "(64, 21)\n",
      "Epoch 3 Batch 66 Loss 0.2876\n",
      "(64, 21)\n",
      "Epoch 3 Batch 67 Loss 0.2232\n",
      "(64, 21)\n",
      "Epoch 3 Batch 68 Loss 0.2761\n",
      "(64, 21)\n",
      "Epoch 3 Batch 69 Loss 0.1907\n",
      "(64, 21)\n",
      "Epoch 3 Batch 70 Loss 0.2817\n",
      "(64, 21)\n",
      "Epoch 3 Batch 71 Loss 0.2822\n",
      "(64, 21)\n",
      "Epoch 3 Batch 72 Loss 0.2505\n",
      "(64, 21)\n",
      "Epoch 3 Batch 73 Loss 0.3014\n",
      "(64, 21)\n",
      "Epoch 3 Batch 74 Loss 0.2541\n",
      "(64, 21)\n",
      "Epoch 3 Batch 75 Loss 0.3190\n",
      "(64, 21)\n",
      "Epoch 3 Batch 76 Loss 0.2448\n",
      "(64, 21)\n",
      "Epoch 3 Batch 77 Loss 0.3073\n",
      "(64, 21)\n",
      "Epoch 3 Batch 78 Loss 0.2900\n",
      "(64, 21)\n",
      "Epoch 3 Batch 79 Loss 0.2501\n",
      "(64, 21)\n",
      "Epoch 3 Batch 80 Loss 0.3365\n",
      "Epoch 3 Loss 0.2516\n",
      "Time taken for 1 epoch 201.1464822292328 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 13:57:59.072532: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 4 Batch 0 Loss 0.1658\n",
      "(64, 21)\n",
      "Epoch 4 Batch 1 Loss 0.1740\n",
      "(64, 21)\n",
      "Epoch 4 Batch 2 Loss 0.2049\n",
      "(64, 21)\n",
      "Epoch 4 Batch 3 Loss 0.1425\n",
      "(64, 21)\n",
      "Epoch 4 Batch 4 Loss 0.1619\n",
      "(64, 21)\n",
      "Epoch 4 Batch 5 Loss 0.2137\n",
      "(64, 21)\n",
      "Epoch 4 Batch 6 Loss 0.1842\n",
      "(64, 21)\n",
      "Epoch 4 Batch 7 Loss 0.1942\n",
      "(64, 21)\n",
      "Epoch 4 Batch 8 Loss 0.2065\n",
      "(64, 21)\n",
      "Epoch 4 Batch 9 Loss 0.2384\n",
      "(64, 21)\n",
      "Epoch 4 Batch 10 Loss 0.1683\n",
      "(64, 21)\n",
      "Epoch 4 Batch 11 Loss 0.1924\n",
      "(64, 21)\n",
      "Epoch 4 Batch 12 Loss 0.1948\n",
      "(64, 21)\n",
      "Epoch 4 Batch 13 Loss 0.2152\n",
      "(64, 21)\n",
      "Epoch 4 Batch 14 Loss 0.1941\n",
      "(64, 21)\n",
      "Epoch 4 Batch 15 Loss 0.1654\n",
      "(64, 21)\n",
      "Epoch 4 Batch 16 Loss 0.2231\n",
      "(64, 21)\n",
      "Epoch 4 Batch 17 Loss 0.1933\n",
      "(64, 21)\n",
      "Epoch 4 Batch 18 Loss 0.1815\n",
      "(64, 21)\n",
      "Epoch 4 Batch 19 Loss 0.2096\n",
      "(64, 21)\n",
      "Epoch 4 Batch 20 Loss 0.1994\n",
      "(64, 21)\n",
      "Epoch 4 Batch 21 Loss 0.1693\n",
      "(64, 21)\n",
      "Epoch 4 Batch 22 Loss 0.1785\n",
      "(64, 21)\n",
      "Epoch 4 Batch 23 Loss 0.1797\n",
      "(64, 21)\n",
      "Epoch 4 Batch 24 Loss 0.2202\n",
      "(64, 21)\n",
      "Epoch 4 Batch 25 Loss 0.1770\n",
      "(64, 21)\n",
      "Epoch 4 Batch 26 Loss 0.1666\n",
      "(64, 21)\n",
      "Epoch 4 Batch 27 Loss 0.1733\n",
      "(64, 21)\n",
      "Epoch 4 Batch 28 Loss 0.1618\n",
      "(64, 21)\n",
      "Epoch 4 Batch 29 Loss 0.1648\n",
      "(64, 21)\n",
      "Epoch 4 Batch 30 Loss 0.2428\n",
      "(64, 21)\n",
      "Epoch 4 Batch 31 Loss 0.1853\n",
      "(64, 21)\n",
      "Epoch 4 Batch 32 Loss 0.2073\n",
      "(64, 21)\n",
      "Epoch 4 Batch 33 Loss 0.1992\n",
      "(64, 21)\n",
      "Epoch 4 Batch 34 Loss 0.2265\n",
      "(64, 21)\n",
      "Epoch 4 Batch 35 Loss 0.1857\n",
      "(64, 21)\n",
      "Epoch 4 Batch 36 Loss 0.2021\n",
      "(64, 21)\n",
      "Epoch 4 Batch 37 Loss 0.1983\n",
      "(64, 21)\n",
      "Epoch 4 Batch 38 Loss 0.2508\n",
      "(64, 21)\n",
      "Epoch 4 Batch 39 Loss 0.1757\n",
      "(64, 21)\n",
      "Epoch 4 Batch 40 Loss 0.1906\n",
      "(64, 21)\n",
      "Epoch 4 Batch 41 Loss 0.2004\n",
      "(64, 21)\n",
      "Epoch 4 Batch 42 Loss 0.2002\n",
      "(64, 21)\n",
      "Epoch 4 Batch 43 Loss 0.2454\n",
      "(64, 21)\n",
      "Epoch 4 Batch 44 Loss 0.1715\n",
      "(64, 21)\n",
      "Epoch 4 Batch 45 Loss 0.1766\n",
      "(64, 21)\n",
      "Epoch 4 Batch 46 Loss 0.2371\n",
      "(64, 21)\n",
      "Epoch 4 Batch 47 Loss 0.2326\n",
      "(64, 21)\n",
      "Epoch 4 Batch 48 Loss 0.1992\n",
      "(64, 21)\n",
      "Epoch 4 Batch 49 Loss 0.2046\n",
      "(64, 21)\n",
      "Epoch 4 Batch 50 Loss 0.2823\n",
      "(64, 21)\n",
      "Epoch 4 Batch 51 Loss 0.2154\n",
      "(64, 21)\n",
      "Epoch 4 Batch 52 Loss 0.1804\n",
      "(64, 21)\n",
      "Epoch 4 Batch 53 Loss 0.2076\n",
      "(64, 21)\n",
      "Epoch 4 Batch 54 Loss 0.2253\n",
      "(64, 21)\n",
      "Epoch 4 Batch 55 Loss 0.2089\n",
      "(64, 21)\n",
      "Epoch 4 Batch 56 Loss 0.2017\n",
      "(64, 21)\n",
      "Epoch 4 Batch 57 Loss 0.2038\n",
      "(64, 21)\n",
      "Epoch 4 Batch 58 Loss 0.1951\n",
      "(64, 21)\n",
      "Epoch 4 Batch 59 Loss 0.2365\n",
      "(64, 21)\n",
      "Epoch 4 Batch 60 Loss 0.2532\n",
      "(64, 21)\n",
      "Epoch 4 Batch 61 Loss 0.2059\n",
      "(64, 21)\n",
      "Epoch 4 Batch 62 Loss 0.2512\n",
      "(64, 21)\n",
      "Epoch 4 Batch 63 Loss 0.1953\n",
      "(64, 21)\n",
      "Epoch 4 Batch 64 Loss 0.2538\n",
      "(64, 21)\n",
      "Epoch 4 Batch 65 Loss 0.2271\n",
      "(64, 21)\n",
      "Epoch 4 Batch 66 Loss 0.2539\n",
      "(64, 21)\n",
      "Epoch 4 Batch 67 Loss 0.2173\n",
      "(64, 21)\n",
      "Epoch 4 Batch 68 Loss 0.1544\n",
      "(64, 21)\n",
      "Epoch 4 Batch 69 Loss 0.2305\n",
      "(64, 21)\n",
      "Epoch 4 Batch 70 Loss 0.1859\n",
      "(64, 21)\n",
      "Epoch 4 Batch 71 Loss 0.2274\n",
      "(64, 21)\n",
      "Epoch 4 Batch 72 Loss 0.2260\n",
      "(64, 21)\n",
      "Epoch 4 Batch 73 Loss 0.2141\n",
      "(64, 21)\n",
      "Epoch 4 Batch 74 Loss 0.2676\n",
      "(64, 21)\n",
      "Epoch 4 Batch 75 Loss 0.2121\n",
      "(64, 21)\n",
      "Epoch 4 Batch 76 Loss 0.2219\n",
      "(64, 21)\n",
      "Epoch 4 Batch 77 Loss 0.1885\n",
      "(64, 21)\n",
      "Epoch 4 Batch 78 Loss 0.2404\n",
      "(64, 21)\n",
      "Epoch 4 Batch 79 Loss 0.2034\n",
      "(64, 21)\n",
      "Epoch 4 Batch 80 Loss 0.2085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:01:11.200386: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 Loss 0.2042\n",
      "Time taken for 1 epoch 192.72276401519775 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 5 Batch 0 Loss 0.1468\n",
      "(64, 21)\n",
      "Epoch 5 Batch 1 Loss 0.1446\n",
      "(64, 21)\n",
      "Epoch 5 Batch 2 Loss 0.1378\n",
      "(64, 21)\n",
      "Epoch 5 Batch 3 Loss 0.1198\n",
      "(64, 21)\n",
      "Epoch 5 Batch 4 Loss 0.1464\n",
      "(64, 21)\n",
      "Epoch 5 Batch 5 Loss 0.1521\n",
      "(64, 21)\n",
      "Epoch 5 Batch 6 Loss 0.1272\n",
      "(64, 21)\n",
      "Epoch 5 Batch 7 Loss 0.1381\n",
      "(64, 21)\n",
      "Epoch 5 Batch 8 Loss 0.1620\n",
      "(64, 21)\n",
      "Epoch 5 Batch 9 Loss 0.1264\n",
      "(64, 21)\n",
      "Epoch 5 Batch 10 Loss 0.1352\n",
      "(64, 21)\n",
      "Epoch 5 Batch 11 Loss 0.1310\n",
      "(64, 21)\n",
      "Epoch 5 Batch 12 Loss 0.1751\n",
      "(64, 21)\n",
      "Epoch 5 Batch 13 Loss 0.1602\n",
      "(64, 21)\n",
      "Epoch 5 Batch 14 Loss 0.1212\n",
      "(64, 21)\n",
      "Epoch 5 Batch 15 Loss 0.1463\n",
      "(64, 21)\n",
      "Epoch 5 Batch 16 Loss 0.1413\n",
      "(64, 21)\n",
      "Epoch 5 Batch 17 Loss 0.1827\n",
      "(64, 21)\n",
      "Epoch 5 Batch 18 Loss 0.2075\n",
      "(64, 21)\n",
      "Epoch 5 Batch 19 Loss 0.1204\n",
      "(64, 21)\n",
      "Epoch 5 Batch 20 Loss 0.1317\n",
      "(64, 21)\n",
      "Epoch 5 Batch 21 Loss 0.1302\n",
      "(64, 21)\n",
      "Epoch 5 Batch 22 Loss 0.1428\n",
      "(64, 21)\n",
      "Epoch 5 Batch 23 Loss 0.1545\n",
      "(64, 21)\n",
      "Epoch 5 Batch 24 Loss 0.1652\n",
      "(64, 21)\n",
      "Epoch 5 Batch 25 Loss 0.1410\n",
      "(64, 21)\n",
      "Epoch 5 Batch 26 Loss 0.1900\n",
      "(64, 21)\n",
      "Epoch 5 Batch 27 Loss 0.1378\n",
      "(64, 21)\n",
      "Epoch 5 Batch 28 Loss 0.1549\n",
      "(64, 21)\n",
      "Epoch 5 Batch 29 Loss 0.1728\n",
      "(64, 21)\n",
      "Epoch 5 Batch 30 Loss 0.1411\n",
      "(64, 21)\n",
      "Epoch 5 Batch 31 Loss 0.1802\n",
      "(64, 21)\n",
      "Epoch 5 Batch 32 Loss 0.2090\n",
      "(64, 21)\n",
      "Epoch 5 Batch 33 Loss 0.1651\n",
      "(64, 21)\n",
      "Epoch 5 Batch 34 Loss 0.2115\n",
      "(64, 21)\n",
      "Epoch 5 Batch 35 Loss 0.1567\n",
      "(64, 21)\n",
      "Epoch 5 Batch 36 Loss 0.1717\n",
      "(64, 21)\n",
      "Epoch 5 Batch 37 Loss 0.1821\n",
      "(64, 21)\n",
      "Epoch 5 Batch 38 Loss 0.1293\n",
      "(64, 21)\n",
      "Epoch 5 Batch 39 Loss 0.1708\n",
      "(64, 21)\n",
      "Epoch 5 Batch 40 Loss 0.1455\n",
      "(64, 21)\n",
      "Epoch 5 Batch 41 Loss 0.1897\n",
      "(64, 21)\n",
      "Epoch 5 Batch 42 Loss 0.1716\n",
      "(64, 21)\n",
      "Epoch 5 Batch 43 Loss 0.1771\n",
      "(64, 21)\n",
      "Epoch 5 Batch 44 Loss 0.1622\n",
      "(64, 21)\n",
      "Epoch 5 Batch 45 Loss 0.1277\n",
      "(64, 21)\n",
      "Epoch 5 Batch 46 Loss 0.1753\n",
      "(64, 21)\n",
      "Epoch 5 Batch 47 Loss 0.1585\n",
      "(64, 21)\n",
      "Epoch 5 Batch 48 Loss 0.1763\n",
      "(64, 21)\n",
      "Epoch 5 Batch 49 Loss 0.1733\n",
      "(64, 21)\n",
      "Epoch 5 Batch 50 Loss 0.1662\n",
      "(64, 21)\n",
      "Epoch 5 Batch 51 Loss 0.1653\n",
      "(64, 21)\n",
      "Epoch 5 Batch 52 Loss 0.2136\n",
      "(64, 21)\n",
      "Epoch 5 Batch 53 Loss 0.1897\n",
      "(64, 21)\n",
      "Epoch 5 Batch 54 Loss 0.1696\n",
      "(64, 21)\n",
      "Epoch 5 Batch 55 Loss 0.1508\n",
      "(64, 21)\n",
      "Epoch 5 Batch 56 Loss 0.1944\n",
      "(64, 21)\n",
      "Epoch 5 Batch 57 Loss 0.1339\n",
      "(64, 21)\n",
      "Epoch 5 Batch 58 Loss 0.1809\n",
      "(64, 21)\n",
      "Epoch 5 Batch 59 Loss 0.1816\n",
      "(64, 21)\n",
      "Epoch 5 Batch 60 Loss 0.1866\n",
      "(64, 21)\n",
      "Epoch 5 Batch 61 Loss 0.1682\n",
      "(64, 21)\n",
      "Epoch 5 Batch 62 Loss 0.1431\n",
      "(64, 21)\n",
      "Epoch 5 Batch 63 Loss 0.1813\n",
      "(64, 21)\n",
      "Epoch 5 Batch 64 Loss 0.2016\n",
      "(64, 21)\n",
      "Epoch 5 Batch 65 Loss 0.1591\n",
      "(64, 21)\n",
      "Epoch 5 Batch 66 Loss 0.1798\n",
      "(64, 21)\n",
      "Epoch 5 Batch 67 Loss 0.1374\n",
      "(64, 21)\n",
      "Epoch 5 Batch 68 Loss 0.1776\n",
      "(64, 21)\n",
      "Epoch 5 Batch 69 Loss 0.1605\n",
      "(64, 21)\n",
      "Epoch 5 Batch 70 Loss 0.1956\n",
      "(64, 21)\n",
      "Epoch 5 Batch 71 Loss 0.1632\n",
      "(64, 21)\n",
      "Epoch 5 Batch 72 Loss 0.1534\n",
      "(64, 21)\n",
      "Epoch 5 Batch 73 Loss 0.2234\n",
      "(64, 21)\n",
      "Epoch 5 Batch 74 Loss 0.2050\n",
      "(64, 21)\n",
      "Epoch 5 Batch 75 Loss 0.2410\n",
      "(64, 21)\n",
      "Epoch 5 Batch 76 Loss 0.1432\n",
      "(64, 21)\n",
      "Epoch 5 Batch 77 Loss 0.1771\n",
      "(64, 21)\n",
      "Epoch 5 Batch 78 Loss 0.1776\n",
      "(64, 21)\n",
      "Epoch 5 Batch 79 Loss 0.2003\n",
      "(64, 21)\n",
      "Epoch 5 Batch 80 Loss 0.1720\n",
      "Epoch 5 Loss 0.1643\n",
      "Time taken for 1 epoch 189.42516899108887 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:04:21.220694: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 6 Batch 0 Loss 0.1090\n",
      "(64, 21)\n",
      "Epoch 6 Batch 1 Loss 0.0923\n",
      "(64, 21)\n",
      "Epoch 6 Batch 2 Loss 0.1112\n",
      "(64, 21)\n",
      "Epoch 6 Batch 3 Loss 0.1216\n",
      "(64, 21)\n",
      "Epoch 6 Batch 4 Loss 0.1268\n",
      "(64, 21)\n",
      "Epoch 6 Batch 5 Loss 0.1268\n",
      "(64, 21)\n",
      "Epoch 6 Batch 6 Loss 0.1183\n",
      "(64, 21)\n",
      "Epoch 6 Batch 7 Loss 0.1249\n",
      "(64, 21)\n",
      "Epoch 6 Batch 8 Loss 0.1244\n",
      "(64, 21)\n",
      "Epoch 6 Batch 9 Loss 0.0962\n",
      "(64, 21)\n",
      "Epoch 6 Batch 10 Loss 0.1034\n",
      "(64, 21)\n",
      "Epoch 6 Batch 11 Loss 0.1055\n",
      "(64, 21)\n",
      "Epoch 6 Batch 12 Loss 0.1124\n",
      "(64, 21)\n",
      "Epoch 6 Batch 13 Loss 0.1545\n",
      "(64, 21)\n",
      "Epoch 6 Batch 14 Loss 0.1282\n",
      "(64, 21)\n",
      "Epoch 6 Batch 15 Loss 0.1166\n",
      "(64, 21)\n",
      "Epoch 6 Batch 16 Loss 0.1242\n",
      "(64, 21)\n",
      "Epoch 6 Batch 17 Loss 0.1103\n",
      "(64, 21)\n",
      "Epoch 6 Batch 18 Loss 0.0986\n",
      "(64, 21)\n",
      "Epoch 6 Batch 19 Loss 0.1459\n",
      "(64, 21)\n",
      "Epoch 6 Batch 20 Loss 0.1147\n",
      "(64, 21)\n",
      "Epoch 6 Batch 21 Loss 0.1169\n",
      "(64, 21)\n",
      "Epoch 6 Batch 22 Loss 0.1513\n",
      "(64, 21)\n",
      "Epoch 6 Batch 23 Loss 0.1237\n",
      "(64, 21)\n",
      "Epoch 6 Batch 24 Loss 0.0847\n",
      "(64, 21)\n",
      "Epoch 6 Batch 25 Loss 0.1109\n",
      "(64, 21)\n",
      "Epoch 6 Batch 26 Loss 0.0935\n",
      "(64, 21)\n",
      "Epoch 6 Batch 27 Loss 0.1462\n",
      "(64, 21)\n",
      "Epoch 6 Batch 28 Loss 0.1643\n",
      "(64, 21)\n",
      "Epoch 6 Batch 29 Loss 0.1397\n",
      "(64, 21)\n",
      "Epoch 6 Batch 30 Loss 0.1225\n",
      "(64, 21)\n",
      "Epoch 6 Batch 31 Loss 0.1592\n",
      "(64, 21)\n",
      "Epoch 6 Batch 32 Loss 0.1263\n",
      "(64, 21)\n",
      "Epoch 6 Batch 33 Loss 0.1272\n",
      "(64, 21)\n",
      "Epoch 6 Batch 34 Loss 0.1266\n",
      "(64, 21)\n",
      "Epoch 6 Batch 35 Loss 0.1311\n",
      "(64, 21)\n",
      "Epoch 6 Batch 36 Loss 0.1851\n",
      "(64, 21)\n",
      "Epoch 6 Batch 37 Loss 0.1728\n",
      "(64, 21)\n",
      "Epoch 6 Batch 38 Loss 0.1340\n",
      "(64, 21)\n",
      "Epoch 6 Batch 39 Loss 0.1479\n",
      "(64, 21)\n",
      "Epoch 6 Batch 40 Loss 0.1606\n",
      "(64, 21)\n",
      "Epoch 6 Batch 41 Loss 0.1151\n",
      "(64, 21)\n",
      "Epoch 6 Batch 42 Loss 0.1356\n",
      "(64, 21)\n",
      "Epoch 6 Batch 43 Loss 0.1386\n",
      "(64, 21)\n",
      "Epoch 6 Batch 44 Loss 0.1214\n",
      "(64, 21)\n",
      "Epoch 6 Batch 45 Loss 0.1116\n",
      "(64, 21)\n",
      "Epoch 6 Batch 46 Loss 0.1346\n",
      "(64, 21)\n",
      "Epoch 6 Batch 47 Loss 0.1439\n",
      "(64, 21)\n",
      "Epoch 6 Batch 48 Loss 0.1444\n",
      "(64, 21)\n",
      "Epoch 6 Batch 49 Loss 0.1450\n",
      "(64, 21)\n",
      "Epoch 6 Batch 50 Loss 0.1508\n",
      "(64, 21)\n",
      "Epoch 6 Batch 51 Loss 0.1360\n",
      "(64, 21)\n",
      "Epoch 6 Batch 52 Loss 0.1357\n",
      "(64, 21)\n",
      "Epoch 6 Batch 53 Loss 0.1338\n",
      "(64, 21)\n",
      "Epoch 6 Batch 54 Loss 0.1493\n",
      "(64, 21)\n",
      "Epoch 6 Batch 55 Loss 0.1300\n",
      "(64, 21)\n",
      "Epoch 6 Batch 56 Loss 0.1271\n",
      "(64, 21)\n",
      "Epoch 6 Batch 57 Loss 0.1527\n",
      "(64, 21)\n",
      "Epoch 6 Batch 58 Loss 0.1465\n",
      "(64, 21)\n",
      "Epoch 6 Batch 59 Loss 0.1428\n",
      "(64, 21)\n",
      "Epoch 6 Batch 60 Loss 0.1651\n",
      "(64, 21)\n",
      "Epoch 6 Batch 61 Loss 0.1747\n",
      "(64, 21)\n",
      "Epoch 6 Batch 62 Loss 0.1387\n",
      "(64, 21)\n",
      "Epoch 6 Batch 63 Loss 0.1453\n",
      "(64, 21)\n",
      "Epoch 6 Batch 64 Loss 0.1428\n",
      "(64, 21)\n",
      "Epoch 6 Batch 65 Loss 0.1240\n",
      "(64, 21)\n",
      "Epoch 6 Batch 66 Loss 0.1286\n",
      "(64, 21)\n",
      "Epoch 6 Batch 67 Loss 0.1279\n",
      "(64, 21)\n",
      "Epoch 6 Batch 68 Loss 0.1411\n",
      "(64, 21)\n",
      "Epoch 6 Batch 69 Loss 0.2053\n",
      "(64, 21)\n",
      "Epoch 6 Batch 70 Loss 0.1269\n",
      "(64, 21)\n",
      "Epoch 6 Batch 71 Loss 0.1625\n",
      "(64, 21)\n",
      "Epoch 6 Batch 72 Loss 0.1663\n",
      "(64, 21)\n",
      "Epoch 6 Batch 73 Loss 0.1504\n",
      "(64, 21)\n",
      "Epoch 6 Batch 74 Loss 0.1559\n",
      "(64, 21)\n",
      "Epoch 6 Batch 75 Loss 0.1710\n",
      "(64, 21)\n",
      "Epoch 6 Batch 76 Loss 0.1350\n",
      "(64, 21)\n",
      "Epoch 6 Batch 77 Loss 0.1253\n",
      "(64, 21)\n",
      "Epoch 6 Batch 78 Loss 0.1657\n",
      "(64, 21)\n",
      "Epoch 6 Batch 79 Loss 0.1616\n",
      "(64, 21)\n",
      "Epoch 6 Batch 80 Loss 0.1403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:07:30.688292: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Loss 0.1351\n",
      "Time taken for 1 epoch 190.0282850265503 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 7 Batch 0 Loss 0.1012\n",
      "(64, 21)\n",
      "Epoch 7 Batch 1 Loss 0.0961\n",
      "(64, 21)\n",
      "Epoch 7 Batch 2 Loss 0.1118\n",
      "(64, 21)\n",
      "Epoch 7 Batch 3 Loss 0.1145\n",
      "(64, 21)\n",
      "Epoch 7 Batch 4 Loss 0.1123\n",
      "(64, 21)\n",
      "Epoch 7 Batch 5 Loss 0.1033\n",
      "(64, 21)\n",
      "Epoch 7 Batch 6 Loss 0.1264\n",
      "(64, 21)\n",
      "Epoch 7 Batch 7 Loss 0.1079\n",
      "(64, 21)\n",
      "Epoch 7 Batch 8 Loss 0.0930\n",
      "(64, 21)\n",
      "Epoch 7 Batch 9 Loss 0.1118\n",
      "(64, 21)\n",
      "Epoch 7 Batch 10 Loss 0.0960\n",
      "(64, 21)\n",
      "Epoch 7 Batch 11 Loss 0.1146\n",
      "(64, 21)\n",
      "Epoch 7 Batch 12 Loss 0.0974\n",
      "(64, 21)\n",
      "Epoch 7 Batch 13 Loss 0.0890\n",
      "(64, 21)\n",
      "Epoch 7 Batch 14 Loss 0.0931\n",
      "(64, 21)\n",
      "Epoch 7 Batch 15 Loss 0.0998\n",
      "(64, 21)\n",
      "Epoch 7 Batch 16 Loss 0.0926\n",
      "(64, 21)\n",
      "Epoch 7 Batch 17 Loss 0.1162\n",
      "(64, 21)\n",
      "Epoch 7 Batch 18 Loss 0.1061\n",
      "(64, 21)\n",
      "Epoch 7 Batch 19 Loss 0.0956\n",
      "(64, 21)\n",
      "Epoch 7 Batch 20 Loss 0.1080\n",
      "(64, 21)\n",
      "Epoch 7 Batch 21 Loss 0.1144\n",
      "(64, 21)\n",
      "Epoch 7 Batch 22 Loss 0.1150\n",
      "(64, 21)\n",
      "Epoch 7 Batch 23 Loss 0.1135\n",
      "(64, 21)\n",
      "Epoch 7 Batch 24 Loss 0.0958\n",
      "(64, 21)\n",
      "Epoch 7 Batch 25 Loss 0.1195\n",
      "(64, 21)\n",
      "Epoch 7 Batch 26 Loss 0.1227\n",
      "(64, 21)\n",
      "Epoch 7 Batch 27 Loss 0.1470\n",
      "(64, 21)\n",
      "Epoch 7 Batch 28 Loss 0.0799\n",
      "(64, 21)\n",
      "Epoch 7 Batch 29 Loss 0.1028\n",
      "(64, 21)\n",
      "Epoch 7 Batch 30 Loss 0.1220\n",
      "(64, 21)\n",
      "Epoch 7 Batch 31 Loss 0.1408\n",
      "(64, 21)\n",
      "Epoch 7 Batch 32 Loss 0.0937\n",
      "(64, 21)\n",
      "Epoch 7 Batch 33 Loss 0.1048\n",
      "(64, 21)\n",
      "Epoch 7 Batch 34 Loss 0.1088\n",
      "(64, 21)\n",
      "Epoch 7 Batch 35 Loss 0.1287\n",
      "(64, 21)\n",
      "Epoch 7 Batch 36 Loss 0.1096\n",
      "(64, 21)\n",
      "Epoch 7 Batch 37 Loss 0.1419\n",
      "(64, 21)\n",
      "Epoch 7 Batch 38 Loss 0.1219\n",
      "(64, 21)\n",
      "Epoch 7 Batch 39 Loss 0.1003\n",
      "(64, 21)\n",
      "Epoch 7 Batch 40 Loss 0.1151\n",
      "(64, 21)\n",
      "Epoch 7 Batch 41 Loss 0.0907\n",
      "(64, 21)\n",
      "Epoch 7 Batch 42 Loss 0.1224\n",
      "(64, 21)\n",
      "Epoch 7 Batch 43 Loss 0.1434\n",
      "(64, 21)\n",
      "Epoch 7 Batch 44 Loss 0.1271\n",
      "(64, 21)\n",
      "Epoch 7 Batch 45 Loss 0.1214\n",
      "(64, 21)\n",
      "Epoch 7 Batch 46 Loss 0.1147\n",
      "(64, 21)\n",
      "Epoch 7 Batch 47 Loss 0.1095\n",
      "(64, 21)\n",
      "Epoch 7 Batch 48 Loss 0.1158\n",
      "(64, 21)\n",
      "Epoch 7 Batch 49 Loss 0.1104\n",
      "(64, 21)\n",
      "Epoch 7 Batch 50 Loss 0.1157\n",
      "(64, 21)\n",
      "Epoch 7 Batch 51 Loss 0.0828\n",
      "(64, 21)\n",
      "Epoch 7 Batch 52 Loss 0.1217\n",
      "(64, 21)\n",
      "Epoch 7 Batch 53 Loss 0.1114\n",
      "(64, 21)\n",
      "Epoch 7 Batch 54 Loss 0.0893\n",
      "(64, 21)\n",
      "Epoch 7 Batch 55 Loss 0.1227\n",
      "(64, 21)\n",
      "Epoch 7 Batch 56 Loss 0.1139\n",
      "(64, 21)\n",
      "Epoch 7 Batch 57 Loss 0.1251\n",
      "(64, 21)\n",
      "Epoch 7 Batch 58 Loss 0.1086\n",
      "(64, 21)\n",
      "Epoch 7 Batch 59 Loss 0.1121\n",
      "(64, 21)\n",
      "Epoch 7 Batch 60 Loss 0.1312\n",
      "(64, 21)\n",
      "Epoch 7 Batch 61 Loss 0.1410\n",
      "(64, 21)\n",
      "Epoch 7 Batch 62 Loss 0.1156\n",
      "(64, 21)\n",
      "Epoch 7 Batch 63 Loss 0.1498\n",
      "(64, 21)\n",
      "Epoch 7 Batch 64 Loss 0.1272\n",
      "(64, 21)\n",
      "Epoch 7 Batch 65 Loss 0.1195\n",
      "(64, 21)\n",
      "Epoch 7 Batch 66 Loss 0.1204\n",
      "(64, 21)\n",
      "Epoch 7 Batch 67 Loss 0.1233\n",
      "(64, 21)\n",
      "Epoch 7 Batch 68 Loss 0.1121\n",
      "(64, 21)\n",
      "Epoch 7 Batch 69 Loss 0.1304\n",
      "(64, 21)\n",
      "Epoch 7 Batch 70 Loss 0.1334\n",
      "(64, 21)\n",
      "Epoch 7 Batch 71 Loss 0.1427\n",
      "(64, 21)\n",
      "Epoch 7 Batch 72 Loss 0.1267\n",
      "(64, 21)\n",
      "Epoch 7 Batch 73 Loss 0.1087\n",
      "(64, 21)\n",
      "Epoch 7 Batch 74 Loss 0.1287\n",
      "(64, 21)\n",
      "Epoch 7 Batch 75 Loss 0.1310\n",
      "(64, 21)\n",
      "Epoch 7 Batch 76 Loss 0.1210\n",
      "(64, 21)\n",
      "Epoch 7 Batch 77 Loss 0.1235\n",
      "(64, 21)\n",
      "Epoch 7 Batch 78 Loss 0.1035\n",
      "(64, 21)\n",
      "Epoch 7 Batch 79 Loss 0.1325\n",
      "(64, 21)\n",
      "Epoch 7 Batch 80 Loss 0.1332\n",
      "Epoch 7 Loss 0.1148\n",
      "Time taken for 1 epoch 191.73433899879456 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:10:42.983198: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 8 Batch 0 Loss 0.0837\n",
      "(64, 21)\n",
      "Epoch 8 Batch 1 Loss 0.0858\n",
      "(64, 21)\n",
      "Epoch 8 Batch 2 Loss 0.0922\n",
      "(64, 21)\n",
      "Epoch 8 Batch 3 Loss 0.0715\n",
      "(64, 21)\n",
      "Epoch 8 Batch 4 Loss 0.0800\n",
      "(64, 21)\n",
      "Epoch 8 Batch 5 Loss 0.0706\n",
      "(64, 21)\n",
      "Epoch 8 Batch 6 Loss 0.0914\n",
      "(64, 21)\n",
      "Epoch 8 Batch 7 Loss 0.0805\n",
      "(64, 21)\n",
      "Epoch 8 Batch 8 Loss 0.0647\n",
      "(64, 21)\n",
      "Epoch 8 Batch 9 Loss 0.0939\n",
      "(64, 21)\n",
      "Epoch 8 Batch 10 Loss 0.1142\n",
      "(64, 21)\n",
      "Epoch 8 Batch 11 Loss 0.1093\n",
      "(64, 21)\n",
      "Epoch 8 Batch 12 Loss 0.0917\n",
      "(64, 21)\n",
      "Epoch 8 Batch 13 Loss 0.0984\n",
      "(64, 21)\n",
      "Epoch 8 Batch 14 Loss 0.0749\n",
      "(64, 21)\n",
      "Epoch 8 Batch 15 Loss 0.0746\n",
      "(64, 21)\n",
      "Epoch 8 Batch 16 Loss 0.0871\n",
      "(64, 21)\n",
      "Epoch 8 Batch 17 Loss 0.0889\n",
      "(64, 21)\n",
      "Epoch 8 Batch 18 Loss 0.0999\n",
      "(64, 21)\n",
      "Epoch 8 Batch 19 Loss 0.1094\n",
      "(64, 21)\n",
      "Epoch 8 Batch 20 Loss 0.0912\n",
      "(64, 21)\n",
      "Epoch 8 Batch 21 Loss 0.0828\n",
      "(64, 21)\n",
      "Epoch 8 Batch 22 Loss 0.0844\n",
      "(64, 21)\n",
      "Epoch 8 Batch 23 Loss 0.0828\n",
      "(64, 21)\n",
      "Epoch 8 Batch 24 Loss 0.1141\n",
      "(64, 21)\n",
      "Epoch 8 Batch 25 Loss 0.0726\n",
      "(64, 21)\n",
      "Epoch 8 Batch 26 Loss 0.0907\n",
      "(64, 21)\n",
      "Epoch 8 Batch 27 Loss 0.1041\n",
      "(64, 21)\n",
      "Epoch 8 Batch 28 Loss 0.0992\n",
      "(64, 21)\n",
      "Epoch 8 Batch 29 Loss 0.0830\n",
      "(64, 21)\n",
      "Epoch 8 Batch 30 Loss 0.1136\n",
      "(64, 21)\n",
      "Epoch 8 Batch 31 Loss 0.1109\n",
      "(64, 21)\n",
      "Epoch 8 Batch 32 Loss 0.0803\n",
      "(64, 21)\n",
      "Epoch 8 Batch 33 Loss 0.1370\n",
      "(64, 21)\n",
      "Epoch 8 Batch 34 Loss 0.0968\n",
      "(64, 21)\n",
      "Epoch 8 Batch 35 Loss 0.1047\n",
      "(64, 21)\n",
      "Epoch 8 Batch 36 Loss 0.1362\n",
      "(64, 21)\n",
      "Epoch 8 Batch 37 Loss 0.0864\n",
      "(64, 21)\n",
      "Epoch 8 Batch 38 Loss 0.1356\n",
      "(64, 21)\n",
      "Epoch 8 Batch 39 Loss 0.1023\n",
      "(64, 21)\n",
      "Epoch 8 Batch 40 Loss 0.0901\n",
      "(64, 21)\n",
      "Epoch 8 Batch 41 Loss 0.0827\n",
      "(64, 21)\n",
      "Epoch 8 Batch 42 Loss 0.0861\n",
      "(64, 21)\n",
      "Epoch 8 Batch 43 Loss 0.0913\n",
      "(64, 21)\n",
      "Epoch 8 Batch 44 Loss 0.1305\n",
      "(64, 21)\n",
      "Epoch 8 Batch 45 Loss 0.0796\n",
      "(64, 21)\n",
      "Epoch 8 Batch 46 Loss 0.0995\n",
      "(64, 21)\n",
      "Epoch 8 Batch 47 Loss 0.0936\n",
      "(64, 21)\n",
      "Epoch 8 Batch 48 Loss 0.1006\n",
      "(64, 21)\n",
      "Epoch 8 Batch 49 Loss 0.0768\n",
      "(64, 21)\n",
      "Epoch 8 Batch 50 Loss 0.0806\n",
      "(64, 21)\n",
      "Epoch 8 Batch 51 Loss 0.1098\n",
      "(64, 21)\n",
      "Epoch 8 Batch 52 Loss 0.1010\n",
      "(64, 21)\n",
      "Epoch 8 Batch 53 Loss 0.1079\n",
      "(64, 21)\n",
      "Epoch 8 Batch 54 Loss 0.0920\n",
      "(64, 21)\n",
      "Epoch 8 Batch 55 Loss 0.1135\n",
      "(64, 21)\n",
      "Epoch 8 Batch 56 Loss 0.1126\n",
      "(64, 21)\n",
      "Epoch 8 Batch 57 Loss 0.0941\n",
      "(64, 21)\n",
      "Epoch 8 Batch 58 Loss 0.0998\n",
      "(64, 21)\n",
      "Epoch 8 Batch 59 Loss 0.1193\n",
      "(64, 21)\n",
      "Epoch 8 Batch 60 Loss 0.0902\n",
      "(64, 21)\n",
      "Epoch 8 Batch 61 Loss 0.1313\n",
      "(64, 21)\n",
      "Epoch 8 Batch 62 Loss 0.1031\n",
      "(64, 21)\n",
      "Epoch 8 Batch 63 Loss 0.1311\n",
      "(64, 21)\n",
      "Epoch 8 Batch 64 Loss 0.0906\n",
      "(64, 21)\n",
      "Epoch 8 Batch 65 Loss 0.0886\n",
      "(64, 21)\n",
      "Epoch 8 Batch 66 Loss 0.0923\n",
      "(64, 21)\n",
      "Epoch 8 Batch 67 Loss 0.0918\n",
      "(64, 21)\n",
      "Epoch 8 Batch 68 Loss 0.1182\n",
      "(64, 21)\n",
      "Epoch 8 Batch 69 Loss 0.1105\n",
      "(64, 21)\n",
      "Epoch 8 Batch 70 Loss 0.1040\n",
      "(64, 21)\n",
      "Epoch 8 Batch 71 Loss 0.0992\n",
      "(64, 21)\n",
      "Epoch 8 Batch 72 Loss 0.1023\n",
      "(64, 21)\n",
      "Epoch 8 Batch 73 Loss 0.1015\n",
      "(64, 21)\n",
      "Epoch 8 Batch 74 Loss 0.0956\n",
      "(64, 21)\n",
      "Epoch 8 Batch 75 Loss 0.1069\n",
      "(64, 21)\n",
      "Epoch 8 Batch 76 Loss 0.0997\n",
      "(64, 21)\n",
      "Epoch 8 Batch 77 Loss 0.1221\n",
      "(64, 21)\n",
      "Epoch 8 Batch 78 Loss 0.0978\n",
      "(64, 21)\n",
      "Epoch 8 Batch 79 Loss 0.1156\n",
      "(64, 21)\n",
      "Epoch 8 Batch 80 Loss 0.1038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:13:54.931194: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Loss 0.0979\n",
      "Time taken for 1 epoch 192.54406118392944 sec\n",
      "\n",
      "(64, 21)\n",
      "Epoch 9 Batch 0 Loss 0.0722\n",
      "(64, 21)\n",
      "Epoch 9 Batch 1 Loss 0.0600\n",
      "(64, 21)\n",
      "Epoch 9 Batch 2 Loss 0.0578\n",
      "(64, 21)\n",
      "Epoch 9 Batch 3 Loss 0.0900\n",
      "(64, 21)\n",
      "Epoch 9 Batch 4 Loss 0.0737\n",
      "(64, 21)\n",
      "Epoch 9 Batch 5 Loss 0.0656\n",
      "(64, 21)\n",
      "Epoch 9 Batch 6 Loss 0.0988\n",
      "(64, 21)\n",
      "Epoch 9 Batch 7 Loss 0.0675\n",
      "(64, 21)\n",
      "Epoch 9 Batch 8 Loss 0.0688\n",
      "(64, 21)\n",
      "Epoch 9 Batch 9 Loss 0.0756\n",
      "(64, 21)\n",
      "Epoch 9 Batch 10 Loss 0.0810\n",
      "(64, 21)\n",
      "Epoch 9 Batch 11 Loss 0.0837\n",
      "(64, 21)\n",
      "Epoch 9 Batch 12 Loss 0.0761\n",
      "(64, 21)\n",
      "Epoch 9 Batch 13 Loss 0.0756\n",
      "(64, 21)\n",
      "Epoch 9 Batch 14 Loss 0.0619\n",
      "(64, 21)\n",
      "Epoch 9 Batch 15 Loss 0.0979\n",
      "(64, 21)\n",
      "Epoch 9 Batch 16 Loss 0.0854\n",
      "(64, 21)\n",
      "Epoch 9 Batch 17 Loss 0.0760\n",
      "(64, 21)\n",
      "Epoch 9 Batch 18 Loss 0.0692\n",
      "(64, 21)\n",
      "Epoch 9 Batch 19 Loss 0.0833\n",
      "(64, 21)\n",
      "Epoch 9 Batch 20 Loss 0.0779\n",
      "(64, 21)\n",
      "Epoch 9 Batch 21 Loss 0.0866\n",
      "(64, 21)\n",
      "Epoch 9 Batch 22 Loss 0.0711\n",
      "(64, 21)\n",
      "Epoch 9 Batch 23 Loss 0.0897\n",
      "(64, 21)\n",
      "Epoch 9 Batch 24 Loss 0.0739\n",
      "(64, 21)\n",
      "Epoch 9 Batch 25 Loss 0.0785\n",
      "(64, 21)\n",
      "Epoch 9 Batch 26 Loss 0.0990\n",
      "(64, 21)\n",
      "Epoch 9 Batch 27 Loss 0.1131\n",
      "(64, 21)\n",
      "Epoch 9 Batch 28 Loss 0.1015\n",
      "(64, 21)\n",
      "Epoch 9 Batch 29 Loss 0.0912\n",
      "(64, 21)\n",
      "Epoch 9 Batch 30 Loss 0.0934\n",
      "(64, 21)\n",
      "Epoch 9 Batch 31 Loss 0.1002\n",
      "(64, 21)\n",
      "Epoch 9 Batch 32 Loss 0.0938\n",
      "(64, 21)\n",
      "Epoch 9 Batch 33 Loss 0.0939\n",
      "(64, 21)\n",
      "Epoch 9 Batch 34 Loss 0.0817\n",
      "(64, 21)\n",
      "Epoch 9 Batch 35 Loss 0.0778\n",
      "(64, 21)\n",
      "Epoch 9 Batch 36 Loss 0.0712\n",
      "(64, 21)\n",
      "Epoch 9 Batch 37 Loss 0.0916\n",
      "(64, 21)\n",
      "Epoch 9 Batch 38 Loss 0.0849\n",
      "(64, 21)\n",
      "Epoch 9 Batch 39 Loss 0.0862\n",
      "(64, 21)\n",
      "Epoch 9 Batch 40 Loss 0.0811\n",
      "(64, 21)\n",
      "Epoch 9 Batch 41 Loss 0.0847\n",
      "(64, 21)\n",
      "Epoch 9 Batch 42 Loss 0.0832\n",
      "(64, 21)\n",
      "Epoch 9 Batch 43 Loss 0.1086\n",
      "(64, 21)\n",
      "Epoch 9 Batch 44 Loss 0.0859\n",
      "(64, 21)\n",
      "Epoch 9 Batch 45 Loss 0.0975\n",
      "(64, 21)\n",
      "Epoch 9 Batch 46 Loss 0.0996\n",
      "(64, 21)\n",
      "Epoch 9 Batch 47 Loss 0.0816\n",
      "(64, 21)\n",
      "Epoch 9 Batch 48 Loss 0.0944\n",
      "(64, 21)\n",
      "Epoch 9 Batch 49 Loss 0.0856\n",
      "(64, 21)\n",
      "Epoch 9 Batch 50 Loss 0.0795\n",
      "(64, 21)\n",
      "Epoch 9 Batch 51 Loss 0.1184\n",
      "(64, 21)\n",
      "Epoch 9 Batch 52 Loss 0.0990\n",
      "(64, 21)\n",
      "Epoch 9 Batch 53 Loss 0.1063\n",
      "(64, 21)\n",
      "Epoch 9 Batch 54 Loss 0.0879\n",
      "(64, 21)\n",
      "Epoch 9 Batch 55 Loss 0.0838\n",
      "(64, 21)\n",
      "Epoch 9 Batch 56 Loss 0.0766\n",
      "(64, 21)\n",
      "Epoch 9 Batch 57 Loss 0.0869\n",
      "(64, 21)\n",
      "Epoch 9 Batch 58 Loss 0.0870\n",
      "(64, 21)\n",
      "Epoch 9 Batch 59 Loss 0.0943\n",
      "(64, 21)\n",
      "Epoch 9 Batch 60 Loss 0.0875\n",
      "(64, 21)\n",
      "Epoch 9 Batch 61 Loss 0.1115\n",
      "(64, 21)\n",
      "Epoch 9 Batch 62 Loss 0.1078\n",
      "(64, 21)\n",
      "Epoch 9 Batch 63 Loss 0.1043\n",
      "(64, 21)\n",
      "Epoch 9 Batch 64 Loss 0.0904\n",
      "(64, 21)\n",
      "Epoch 9 Batch 65 Loss 0.0829\n",
      "(64, 21)\n",
      "Epoch 9 Batch 66 Loss 0.0848\n",
      "(64, 21)\n",
      "Epoch 9 Batch 67 Loss 0.0850\n",
      "(64, 21)\n",
      "Epoch 9 Batch 68 Loss 0.0824\n",
      "(64, 21)\n",
      "Epoch 9 Batch 69 Loss 0.1065\n",
      "(64, 21)\n",
      "Epoch 9 Batch 70 Loss 0.0928\n",
      "(64, 21)\n",
      "Epoch 9 Batch 71 Loss 0.0836\n",
      "(64, 21)\n",
      "Epoch 9 Batch 72 Loss 0.0766\n",
      "(64, 21)\n",
      "Epoch 9 Batch 73 Loss 0.0905\n",
      "(64, 21)\n",
      "Epoch 9 Batch 74 Loss 0.0942\n",
      "(64, 21)\n",
      "Epoch 9 Batch 75 Loss 0.1115\n",
      "(64, 21)\n",
      "Epoch 9 Batch 76 Loss 0.0838\n",
      "(64, 21)\n",
      "Epoch 9 Batch 77 Loss 0.1032\n",
      "(64, 21)\n",
      "Epoch 9 Batch 78 Loss 0.0847\n",
      "(64, 21)\n",
      "Epoch 9 Batch 79 Loss 0.0878\n",
      "(64, 21)\n",
      "Epoch 9 Batch 80 Loss 0.1076\n",
      "Epoch 9 Loss 0.0871\n",
      "Time taken for 1 epoch 192.39004015922546 sec\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:17:07.917332: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 21)\n",
      "Epoch 10 Batch 0 Loss 0.0730\n",
      "(64, 21)\n",
      "Epoch 10 Batch 1 Loss 0.0765\n",
      "(64, 21)\n",
      "Epoch 10 Batch 2 Loss 0.0546\n",
      "(64, 21)\n",
      "Epoch 10 Batch 3 Loss 0.0623\n",
      "(64, 21)\n",
      "Epoch 10 Batch 4 Loss 0.0483\n",
      "(64, 21)\n",
      "Epoch 10 Batch 5 Loss 0.0548\n",
      "(64, 21)\n",
      "Epoch 10 Batch 6 Loss 0.0641\n",
      "(64, 21)\n",
      "Epoch 10 Batch 7 Loss 0.0663\n",
      "(64, 21)\n",
      "Epoch 10 Batch 8 Loss 0.0852\n",
      "(64, 21)\n",
      "Epoch 10 Batch 9 Loss 0.0675\n",
      "(64, 21)\n",
      "Epoch 10 Batch 10 Loss 0.0396\n",
      "(64, 21)\n",
      "Epoch 10 Batch 11 Loss 0.0684\n",
      "(64, 21)\n",
      "Epoch 10 Batch 12 Loss 0.0569\n",
      "(64, 21)\n",
      "Epoch 10 Batch 13 Loss 0.0755\n",
      "(64, 21)\n",
      "Epoch 10 Batch 14 Loss 0.0674\n",
      "(64, 21)\n",
      "Epoch 10 Batch 15 Loss 0.0695\n",
      "(64, 21)\n",
      "Epoch 10 Batch 16 Loss 0.0782\n",
      "(64, 21)\n",
      "Epoch 10 Batch 17 Loss 0.0626\n",
      "(64, 21)\n",
      "Epoch 10 Batch 18 Loss 0.0674\n",
      "(64, 21)\n",
      "Epoch 10 Batch 19 Loss 0.0725\n",
      "(64, 21)\n",
      "Epoch 10 Batch 20 Loss 0.0690\n",
      "(64, 21)\n",
      "Epoch 10 Batch 21 Loss 0.0741\n",
      "(64, 21)\n",
      "Epoch 10 Batch 22 Loss 0.0897\n",
      "(64, 21)\n",
      "Epoch 10 Batch 23 Loss 0.0749\n",
      "(64, 21)\n",
      "Epoch 10 Batch 24 Loss 0.0732\n",
      "(64, 21)\n",
      "Epoch 10 Batch 25 Loss 0.0637\n",
      "(64, 21)\n",
      "Epoch 10 Batch 26 Loss 0.0832\n",
      "(64, 21)\n",
      "Epoch 10 Batch 27 Loss 0.0805\n",
      "(64, 21)\n",
      "Epoch 10 Batch 28 Loss 0.0844\n",
      "(64, 21)\n",
      "Epoch 10 Batch 29 Loss 0.0666\n",
      "(64, 21)\n",
      "Epoch 10 Batch 30 Loss 0.0997\n",
      "(64, 21)\n",
      "Epoch 10 Batch 31 Loss 0.0901\n",
      "(64, 21)\n",
      "Epoch 10 Batch 32 Loss 0.0833\n",
      "(64, 21)\n",
      "Epoch 10 Batch 33 Loss 0.0773\n",
      "(64, 21)\n",
      "Epoch 10 Batch 34 Loss 0.0710\n",
      "(64, 21)\n",
      "Epoch 10 Batch 35 Loss 0.0714\n",
      "(64, 21)\n",
      "Epoch 10 Batch 36 Loss 0.0736\n",
      "(64, 21)\n",
      "Epoch 10 Batch 37 Loss 0.0643\n",
      "(64, 21)\n",
      "Epoch 10 Batch 38 Loss 0.0705\n",
      "(64, 21)\n",
      "Epoch 10 Batch 39 Loss 0.1183\n",
      "(64, 21)\n",
      "Epoch 10 Batch 40 Loss 0.0702\n",
      "(64, 21)\n",
      "Epoch 10 Batch 41 Loss 0.0913\n",
      "(64, 21)\n",
      "Epoch 10 Batch 42 Loss 0.0643\n",
      "(64, 21)\n",
      "Epoch 10 Batch 43 Loss 0.0695\n",
      "(64, 21)\n",
      "Epoch 10 Batch 44 Loss 0.0855\n",
      "(64, 21)\n",
      "Epoch 10 Batch 45 Loss 0.0841\n",
      "(64, 21)\n",
      "Epoch 10 Batch 46 Loss 0.0740\n",
      "(64, 21)\n",
      "Epoch 10 Batch 47 Loss 0.0870\n",
      "(64, 21)\n",
      "Epoch 10 Batch 48 Loss 0.0889\n",
      "(64, 21)\n",
      "Epoch 10 Batch 49 Loss 0.0859\n",
      "(64, 21)\n",
      "Epoch 10 Batch 50 Loss 0.0705\n",
      "(64, 21)\n",
      "Epoch 10 Batch 51 Loss 0.0728\n",
      "(64, 21)\n",
      "Epoch 10 Batch 52 Loss 0.0981\n",
      "(64, 21)\n",
      "Epoch 10 Batch 53 Loss 0.0796\n",
      "(64, 21)\n",
      "Epoch 10 Batch 54 Loss 0.0897\n",
      "(64, 21)\n",
      "Epoch 10 Batch 55 Loss 0.1037\n",
      "(64, 21)\n",
      "Epoch 10 Batch 56 Loss 0.0872\n",
      "(64, 21)\n",
      "Epoch 10 Batch 57 Loss 0.1025\n",
      "(64, 21)\n",
      "Epoch 10 Batch 58 Loss 0.0842\n",
      "(64, 21)\n",
      "Epoch 10 Batch 59 Loss 0.0710\n",
      "(64, 21)\n",
      "Epoch 10 Batch 60 Loss 0.0830\n",
      "(64, 21)\n",
      "Epoch 10 Batch 61 Loss 0.0687\n",
      "(64, 21)\n",
      "Epoch 10 Batch 62 Loss 0.0786\n",
      "(64, 21)\n",
      "Epoch 10 Batch 63 Loss 0.0944\n",
      "(64, 21)\n",
      "Epoch 10 Batch 64 Loss 0.0761\n",
      "(64, 21)\n",
      "Epoch 10 Batch 65 Loss 0.0791\n",
      "(64, 21)\n",
      "Epoch 10 Batch 66 Loss 0.0731\n",
      "(64, 21)\n",
      "Epoch 10 Batch 67 Loss 0.0814\n",
      "(64, 21)\n",
      "Epoch 10 Batch 68 Loss 0.0862\n",
      "(64, 21)\n",
      "Epoch 10 Batch 69 Loss 0.0948\n",
      "(64, 21)\n",
      "Epoch 10 Batch 70 Loss 0.0748\n",
      "(64, 21)\n",
      "Epoch 10 Batch 71 Loss 0.0703\n",
      "(64, 21)\n",
      "Epoch 10 Batch 72 Loss 0.0711\n",
      "(64, 21)\n",
      "Epoch 10 Batch 73 Loss 0.1016\n",
      "(64, 21)\n",
      "Epoch 10 Batch 74 Loss 0.0635\n",
      "(64, 21)\n",
      "Epoch 10 Batch 75 Loss 0.0931\n",
      "(64, 21)\n",
      "Epoch 10 Batch 76 Loss 0.0924\n",
      "(64, 21)\n",
      "Epoch 10 Batch 77 Loss 0.0946\n",
      "(64, 21)\n",
      "Epoch 10 Batch 78 Loss 0.0925\n",
      "(64, 21)\n",
      "Epoch 10 Batch 79 Loss 0.0870\n",
      "(64, 21)\n",
      "Epoch 10 Batch 80 Loss 0.1029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-16 14:20:20.789721: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 Loss 0.0779\n",
      "Time taken for 1 epoch 193.48266696929932 sec\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS=10\n",
    "import time\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
    "        batch_loss = train_step(inp, targ)\n",
    "        total_loss += batch_loss\n",
    "        print(targ.shape)\n",
    "        print('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                                         batch,\n",
    "                                                         batch_loss.numpy()))\n",
    "    # saving (checkpoint) the model every 2 epochs\n",
    "    if (epoch + 1) % 2 == 0:\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix+str(epoch))\n",
    "\n",
    "    print('Epoch {} Loss {:.4f}'.format(epoch + 1,\n",
    "                                      total_loss / steps_per_epoch))\n",
    "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt-12'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 1024)\n",
      "(1, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['তোর', 'নাম', 'কি', '?', '<end>']"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_length = 20\n",
    "\n",
    "#Now we will generate the translation of the input sentence\n",
    "\n",
    "def evaluate(sentence):\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    sentence = inp_lang.texts_to_sequences([sentence])\n",
    "\n",
    "    # Pad the sentence to the max_length_inp\n",
    "\n",
    "    sentence = tf.keras.preprocessing.sequence.pad_sequences(sentence,\n",
    "                                                            maxlen=max_length_inp,\n",
    "                                                            padding='post')\n",
    "\n",
    "    hidden_initialize = encoder.initialize_hidden_state()\n",
    "    enc_output, enc_hidden = encoder(sentence, hidden_initialize)\n",
    "\n",
    "    context_vector, attention_weights = attention(enc_output)\n",
    "\n",
    "    context_vector=context_vector[0]\n",
    "    context_vector=tf.expand_dims(context_vector,0) \n",
    "    print(context_vector.shape)\n",
    "\n",
    "\n",
    "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 1) \n",
    "    print(dec_input.shape)\n",
    "    result = []\n",
    "    #Disable teacher forcing\n",
    "\n",
    "    for i in range(max_seq_length):\n",
    "        predictions, dec_hidden = decoder(context_vector, dec_input)\n",
    "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
    "\n",
    "        result.append(targ_lang.index_word[predicted_id])\n",
    "\n",
    "        if targ_lang.index_word[predicted_id] == '<end>':\n",
    "            return result\n",
    "\n",
    "        # the predicted ID is fed back into the model\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "evaluate(\"What's your name?\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "picaso",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
